{"url": "https://github.com/bihealth/vcfpy/blob/99e2165df30f11e0c95f3170f31bc5191d9e9e15/vcfpy/header.py#L227-L253", "repo": "vcfpy", "func_name": "header_without_lines", "original_string": ["def header_without_lines(header, remove):\n", "    \"\"\"Return :py:class:`Header` without lines given in ``remove``\n", "\n", "    ``remove`` is an iterable of pairs ``key``/``ID`` with the VCF header key\n", "    and ``ID`` of entry to remove.  In the case that a line does not have\n", "    a ``mapping`` entry, you can give the full value to remove.\n", "\n", "    .. code-block:: python\n", "\n", "        # header is a vcfpy.Header, e.g., as read earlier from file\n", "        new_header = vcfpy.without_header_lines(\n", "            header, [('assembly', None), ('FILTER', 'PASS')])\n", "        # now, the header lines starting with \"##assembly=\" and the \"PASS\"\n", "        # filter line will be missing from new_header\n", "    \"\"\"\n", "    remove = set(remove)\n", "    # Copy over lines that are not removed\n", "    lines = []\n", "    for line in header.lines:\n", "        if hasattr(line, \"mapping\"):\n", "            if (line.key, line.mapping.get(\"ID\", None)) in remove:\n", "                continue  # filter out\n", "        else:\n", "            if (line.key, line.value) in remove:\n", "                continue  # filter out\n", "        lines.append(line)\n", "    return Header(lines, header.samples)\n"], "language": "python", "code": "def header_without_lines(header, remove):\n    \"\"\"\"\"\"\n    remove = set(remove)\n    lines = []\n    for line in header.lines:\n        if hasattr(line, 'mapping'):\n            if (line.key, line.mapping.get('ID', None)) in remove:\n                continue\n        elif (line.key, line.value) in remove:\n            continue\n        lines.append(line)\n    return Header(lines, header.samples)\n", "code_tokens": ["header", "without", "lines", "header", "remove", "remove", "set", "remove", "lines", "for", "line", "in", "header", "lines", "if", "hasattr", "line", "mapping", "if", "line", "key", "line", "mapping", "get", "id", "none", "in", "remove", "continue", "elif", "line", "key", "line", "value", "in", "remove", "continue", "lines", "append", "line", "return", "header", "lines", "header", "samples"], "docstring": "concatenate several file remove header lines", "docstring_tokens": ["concatenate", "several", "file", "remove", "header", "lines"], "idx": 0}
{"url": "https://github.com/utiasSTARS/pykitti/blob/d3e1bb81676e831886726cc5ed79ce1f049aef2c/pykitti/downloader/tracking.py#L26-L38", "repo": "pykitti", "func_name": "clean_file", "original_string": ["def clean_file(filename):\n", "    f = open(filename, 'r')\n", "    new_lines = []\n", "    for line in f.readlines():\n", "        new_lines.append(line.rstrip())\n", "    f.close()\n", "\n", "    f = open(filename, 'w')\n", "    for line in new_lines:\n", "        f.write(line + '\\n')\n", "\n", "\n", "    f.close()\n"], "language": "python", "code": "def clean_file(filename):\n    f = open(filename, 'r')\n    new_lines = []\n    for line in f.readlines():\n        new_lines.append(line.rstrip())\n    f.close()\n    f = open(filename, 'w')\n    for line in new_lines:\n        f.write(line + '\\n')\n    f.close()\n", "code_tokens": ["clean", "file", "filename", "open", "filename", "new", "lines", "for", "line", "in", "readlines", "new", "lines", "append", "line", "rstrip", "close", "open", "filename", "for", "line", "in", "new", "lines", "write", "line", "close"], "docstring": "concatenate several file remove header lines", "docstring_tokens": ["concatenate", "several", "file", "remove", "header", "lines"], "idx": 1}
{"url": "https://github.com/LuminosoInsight/langcodes/blob/0cedf9ca257ebf7250de5d3a63ec33a7d198db58/langcodes/registry_parser.py#L6-L25", "repo": "langcodes", "func_name": "parse_file", "original_string": ["def parse_file(file):\n", "    \"\"\"\n", "    Take an open file containing the IANA subtag registry, and yield a\n", "    dictionary of information for each subtag it describes.\n", "    \"\"\"\n", "    lines = []\n", "    for line in file:\n", "        line = line.rstrip('\\n')\n", "        if line == '%%':\n", "            # This is a separator between items. Parse the data we've\n", "            # collected and yield the result.\n", "            yield from parse_item(lines)\n", "            lines.clear()\n", "        elif line.startswith('  '):\n", "            # This is a continuation line. Concatenate it to the previous\n", "            # line, including one of the spaces.\n", "            lines[-1] += line[1:]\n", "        else:\n", "            lines.append(line)\n", "    yield from parse_item(lines)\n"], "language": "python", "code": "def parse_file(file):\n    \"\"\"\"\"\"\n    lines = []\n    for line in file:\n        line = line.rstrip('\\n')\n        if line == '%%':\n            yield from parse_item(lines)\n            lines.clear()\n        elif line.startswith('  '):\n            lines[-1] += line[1:]\n        else:\n            lines.append(line)\n    yield from parse_item(lines)\n", "code_tokens": ["parse", "file", "file", "lines", "for", "line", "in", "file", "line", "line", "rstrip", "if", "line", "yield", "from", "parse", "item", "lines", "lines", "clear", "elif", "line", "startswith", "lines", "1", "line", "1", "else", "lines", "append", "line", "yield", "from", "parse", "item", "lines"], "docstring": "concatenate several file remove header lines", "docstring_tokens": ["concatenate", "several", "file", "remove", "header", "lines"], "idx": 2}
{"url": "https://github.com/cdeboever3/cdpybio/blob/38efdf0e11d01bc00a135921cb91a19c03db5d5c/cdpybio/variants.py#L83-L110", "repo": "cdpybio", "func_name": "vcf_as_df", "original_string": ["def vcf_as_df(fn):\n", "    \"\"\"\n", "    Read VCF file into pandas DataFrame.\n", "\n", "    Parameters:\n", "    -----------\n", "    fn : str\n", "        Path to VCF file.\n", "\n", "    Returns\n", "    -------\n", "    df : pandas.DataFrame\n", "        The VCF file as a data frame. Note that all header information is thrown\n", "        away.\n", "\n", "    \"\"\"\n", "    header_lines = 0\n", "    with open(fn, 'r') as f:\n", "        line = f.readline().strip()\n", "        header_lines += 1\n", "        while line[0] == '#':\n", "            line = f.readline().strip()\n", "            header_lines += 1\n", "    \n", "    header_lines -= 2\n", "    df = pd.read_table(fn, skiprows=header_lines, header=0)\n", "    df.columns = ['CHROM'] + list(df.columns[1:])\n", "    return df\n"], "language": "python", "code": "def vcf_as_df(fn):\n    \"\"\"\"\"\"\n    header_lines = 0\n    with open(fn, 'r') as f:\n        line = f.readline().strip()\n        header_lines += 1\n        while line[0] == '#':\n            line = f.readline().strip()\n            header_lines += 1\n    header_lines -= 2\n    df = pd.read_table(fn, skiprows=header_lines, header=0)\n    df.columns = ['CHROM'] + list(df.columns[1:])\n    return df\n", "code_tokens": ["vcf", "as", "df", "fn", "header", "lines", "0", "with", "open", "fn", "as", "line", "readline", "strip", "header", "lines", "1", "while", "line", "0", "line", "readline", "strip", "header", "lines", "1", "header", "lines", "2", "df", "pd", "read", "table", "fn", "skiprows", "header", "lines", "header", "0", "df", "columns", "chrom", "list", "df", "columns", "1", "return", "df"], "docstring": "concatenate several file remove header lines", "docstring_tokens": ["concatenate", "several", "file", "remove", "header", "lines"], "idx": 3}
{"url": "https://github.com/wummel/linkchecker/blob/c2ce810c3fb00b895a841a7be6b2e78c64e7b042/linkcheck/cookies.py#L27-L45", "repo": "linkchecker", "func_name": "from_file", "original_string": ["def from_file (filename):\n", "    \"\"\"Parse cookie data from a text file in HTTP header format.\n", "\n", "    @return: list of tuples (headers, scheme, host, path)\n", "    \"\"\"\n", "    entries = []\n", "    with open(filename) as fd:\n", "        lines = []\n", "        for line in fd.readlines():\n", "            line = line.rstrip()\n", "            if not line:\n", "                if lines:\n", "                    entries.append(from_headers(\"\\r\\n\".join(lines)))\n", "                lines = []\n", "            else:\n", "                lines.append(line)\n", "        if lines:\n", "            entries.append(from_headers(\"\\r\\n\".join(lines)))\n", "        return entries\n"], "language": "python", "code": "def from_file(filename):\n    \"\"\"\"\"\"\n    entries = []\n    with open(filename) as fd:\n        lines = []\n        for line in fd.readlines():\n            line = line.rstrip()\n            if not line:\n                if lines:\n                    entries.append(from_headers('\\r\\n'.join(lines)))\n                lines = []\n            else:\n                lines.append(line)\n        if lines:\n            entries.append(from_headers('\\r\\n'.join(lines)))\n        return entries\n", "code_tokens": ["from", "file", "filename", "entries", "with", "open", "filename", "as", "fd", "lines", "for", "line", "in", "fd", "readlines", "line", "line", "rstrip", "if", "not", "line", "if", "lines", "entries", "append", "from", "headers", "join", "lines", "lines", "else", "lines", "append", "line", "if", "lines", "entries", "append", "from", "headers", "join", "lines", "return", "entries"], "docstring": "concatenate several file remove header lines", "docstring_tokens": ["concatenate", "several", "file", "remove", "header", "lines"], "idx": 4}
{"url": "https://github.com/skoczen/inkblock/blob/099f834c1e9fc0938abaa8824725eeac57603f6c/inkblock/main.py#L151-L172", "repo": "inkblock", "func_name": "filename_generator", "original_string": ["def filename_generator(file_parts, new_m_time=None):\n", "    # print \"filename_generator\"\n", "    # print file_parts\n", "    concat = \"\".join(file_parts)\n", "    if concat in FILENAMES_GENERATED:\n", "        # print FILENAMES_GENERATED[concat]\n", "        return FILENAMES_GENERATED[concat]\n", "    \n", "    sha = \"\"\n", "    if \"-inkmd\" not in file_parts[0]:\n", "        for base in MEDIA_ROOTS:\n", "            try:\n", "                sha = \"%s-inkmd\" % md5(os.path.join(base, concat))\n", "                break\n", "            except IOError:\n", "                pass\n", "\n", "\n", "    new_name = ''.join([file_parts[0], sha, file_parts[1]])\n", "    FILENAMES_GENERATED[concat] = new_name\n", "    # print new_name\n", "    return new_name\n"], "language": "python", "code": "def filename_generator(file_parts, new_m_time=None):\n    concat = ''.join(file_parts)\n    if concat in FILENAMES_GENERATED:\n        return FILENAMES_GENERATED[concat]\n    sha = ''\n    if '-inkmd' not in file_parts[0]:\n        for base in MEDIA_ROOTS:\n            try:\n                sha = '%s-inkmd' % md5(os.path.join(base, concat))\n                break\n            except IOError:\n                pass\n    new_name = ''.join([file_parts[0], sha, file_parts[1]])\n    FILENAMES_GENERATED[concat] = new_name\n    return new_name\n", "code_tokens": ["filename", "generator", "file", "parts", "new", "time", "none", "concat", "join", "file", "parts", "if", "concat", "in", "filenames", "generated", "return", "filenames", "generated", "concat", "sha", "if", "inkmd", "not", "in", "file", "parts", "0", "for", "base", "in", "media", "roots", "try", "sha", "inkmd", "os", "path", "join", "base", "concat", "break", "except", "ioerror", "pass", "new", "name", "join", "file", "parts", "0", "sha", "file", "parts", "1", "filenames", "generated", "concat", "new", "name", "return", "new", "name"], "docstring": "concatenate several file remove header lines", "docstring_tokens": ["concatenate", "several", "file", "remove", "header", "lines"], "idx": 5}
{"url": "https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/queues/priority_queue.py#L38-L49", "repo": "algorithms", "func_name": "push", "original_string": ["    def push(self, item, priority=None):\n", "        \"\"\"Push the item in the priority queue.\n", "        if priority is not given, priority is set to the value of item.\n", "        \"\"\"\n", "        priority = item if priority is None else priority\n", "        node = PriorityQueueNode(item, priority)\n", "        for index, current in enumerate(self.priority_queue_list):\n", "            if current.priority < node.priority:\n", "                self.priority_queue_list.insert(index, node)\n", "                return\n", "        # when traversed complete queue\n", "        self.priority_queue_list.append(node)\n"], "language": "python", "code": "def push(self, item, priority=None):\n    \"\"\"\"\"\"\n    priority = item if priority is None else priority\n    node = PriorityQueueNode(item, priority)\n    for index, current in enumerate(self.priority_queue_list):\n        if current.priority < node.priority:\n            self.priority_queue_list.insert(index, node)\n            return\n    self.priority_queue_list.append(node)\n", "code_tokens": ["push", "self", "item", "priority", "none", "priority", "item", "if", "priority", "is", "none", "else", "priority", "node", "priorityqueuenode", "item", "priority", "for", "index", "current", "in", "enumerate", "self", "priority", "queue", "list", "if", "current", "priority", "node", "priority", "self", "priority", "queue", "list", "insert", "index", "node", "return", "self", "priority", "queue", "list", "append", "node"], "docstring": "priority queue", "docstring_tokens": ["priority", "queue"], "idx": 6}
{"url": "https://github.com/FreshXOpenSource/wallaby-base/blob/5c75b1729e99af905c44b1293e67dbd5527d022d/wallaby/pf/room.py#L366-L370", "repo": "wallaby-base", "func_name": "__enqueue", "original_string": ["    def __enqueue(self, catcher, pillow, feathers, passThrower=False, **ka):\n", "        if pillow.endswith(\"!\"):\n", "            self.__enqueueStatefull(catcher, pillow, passThrower)\n", "        else:\n", "            self._normalQueue.append( (catcher, (pillow, feathers), ka) ) \n"], "language": "python", "code": "def __enqueue(self, catcher, pillow, feathers, passThrower=False, **ka):\n    if pillow.endswith('!'):\n        self.__enqueueStatefull(catcher, pillow, passThrower)\n    else:\n        self._normalQueue.append((catcher, (pillow, feathers), ka))\n", "code_tokens": ["enqueue", "self", "catcher", "pillow", "feathers", "passthrower", "false", "ka", "if", "pillow", "endswith", "self", "enqueuestatefull", "catcher", "pillow", "passthrower", "else", "self", "normalqueue", "append", "catcher", "pillow", "feathers", "ka"], "docstring": "priority queue", "docstring_tokens": ["priority", "queue"], "idx": 7}
{"url": "https://github.com/limpyd/redis-limpyd-jobs/blob/264c71029bad4377d6132bf8bb9c55c44f3b03a2/limpyd_jobs/workers.py#L492-L504", "repo": "redis-limpyd-jobs", "func_name": "requeue_job", "original_string": ["    def requeue_job(self, job, queue, priority, delayed_for=None):\n", "        \"\"\"\n", "        Requeue a job in a queue with the given priority, possibly delayed\n", "        \"\"\"\n", "        job.requeue(queue_name=queue._cached_name,\n", "                    priority=priority,\n", "                    delayed_for=delayed_for,\n", "                    queue_model=self.queue_model)\n", "\n", "        if hasattr(job, 'on_requeued'):\n", "            job.on_requeued(queue)\n", "\n", "        self.log(self.job_requeue_message(job, queue))\n"], "language": "python", "code": "def requeue_job(self, job, queue, priority, delayed_for=None):\n    \"\"\"\"\"\"\n    job.requeue(queue_name=queue._cached_name, priority=priority,\n        delayed_for=delayed_for, queue_model=self.queue_model)\n    if hasattr(job, 'on_requeued'):\n        job.on_requeued(queue)\n    self.log(self.job_requeue_message(job, queue))\n", "code_tokens": ["requeue", "job", "self", "job", "queue", "priority", "delayed", "for", "none", "job", "requeue", "queue", "name", "queue", "cached", "name", "priority", "priority", "delayed", "for", "delayed", "for", "queue", "model", "self", "queue", "model", "if", "hasattr", "job", "on", "requeued", "job", "on", "requeued", "queue", "self", "log", "self", "job", "requeue", "message", "job", "queue"], "docstring": "priority queue", "docstring_tokens": ["priority", "queue"], "idx": 8}
{"url": "https://github.com/ask/ghettoq/blob/22a0fcd865b618cbbbfd102efd88a7983507c24e/ghettoq/backends/beanstalk.py#L32-L34", "repo": "ghettoq", "func_name": "put", "original_string": ["    def put(self, queue, message, priority=0, **kwargs):\n", "        self.client.use(queue)\n", "        self.client.put(message, priority=priority)\n"], "language": "python", "code": "def put(self, queue, message, priority=0, **kwargs):\n    self.client.use(queue)\n    self.client.put(message, priority=priority)\n", "code_tokens": ["put", "self", "queue", "message", "priority", "0", "kwargs", "self", "client", "use", "queue", "self", "client", "put", "message", "priority", "priority"], "docstring": "priority queue", "docstring_tokens": ["priority", "queue"], "idx": 9}
{"url": "https://github.com/hubo1016/vlcp/blob/239055229ec93a99cc7e15208075724ccf543bd1/vlcp/event/pqueue.py#L1032-L1040", "repo": "vlcp", "func_name": "setPriority", "original_string": ["    def setPriority(self, queue, priority):\n", "        '''\n", "        Set priority of a sub-queue\n", "        '''\n", "        q = self.queueindex[queue]\n", "        self.queues[q[0]].removeSubQueue(q[1])\n", "        newPriority = self.queues.setdefault(priority, CBQueue.MultiQueue(self, priority))\n", "        q[0] = priority\n", "        newPriority.addSubQueue(q[1])\n"], "language": "python", "code": "def setPriority(self, queue, priority):\n    \"\"\"\"\"\"\n    q = self.queueindex[queue]\n    self.queues[q[0]].removeSubQueue(q[1])\n    newPriority = self.queues.setdefault(priority, CBQueue.MultiQueue(self,\n        priority))\n    q[0] = priority\n    newPriority.addSubQueue(q[1])\n", "code_tokens": ["setpriority", "self", "queue", "priority", "self", "queueindex", "queue", "self", "queues", "0", "removesubqueue", "1", "newpriority", "self", "queues", "setdefault", "priority", "cbqueue", "multiqueue", "self", "priority", "0", "priority", "newpriority", "addsubqueue", "1"], "docstring": "priority queue", "docstring_tokens": ["priority", "queue"], "idx": 10}
{"url": "https://github.com/rfk/threading2/blob/7ec234ddd8c0d7e683b1a5c4a79a3d001143189f/threading2/t2_base.py#L372-L378", "repo": "threading2", "func_name": "_set_priority", "original_string": ["    def _set_priority(self,priority):\n", "        if not 0 <= priority <= 1:\n", "            raise ValueError(\"priority must be between 0 and 1\")\n", "        self.__priority = priority\n", "        if self.is_alive():\n", "            self.priority = priority\n", "        return priority\n"], "language": "python", "code": "def _set_priority(self, priority):\n    if not 0 <= priority <= 1:\n        raise ValueError('priority must be between 0 and 1')\n    self.__priority = priority\n    if self.is_alive():\n        self.priority = priority\n    return priority\n", "code_tokens": ["set", "priority", "self", "priority", "if", "not", "0", "priority", "1", "raise", "valueerror", "priority", "must", "be", "between", "0", "and", "1", "self", "priority", "priority", "if", "self", "is", "alive", "self", "priority", "priority", "return", "priority"], "docstring": "priority queue", "docstring_tokens": ["priority", "queue"], "idx": 11}
{"url": "https://github.com/open-mmlab/mmcv/blob/0d77f61450aab4dde8b8585a577cc496acb95d7f/mmcv/runner/priority.py#L43-L61", "repo": "mmcv", "func_name": "get_priority", "original_string": ["def get_priority(priority: Union[int, str, Priority]) -> int:\n", "    \"\"\"Get priority value.\n", "\n", "    Args:\n", "        priority (int or str or :obj:`Priority`): Priority.\n", "\n", "    Returns:\n", "        int: The priority value.\n", "    \"\"\"\n", "    if isinstance(priority, int):\n", "        if priority < 0 or priority > 100:\n", "            raise ValueError('priority must be between 0 and 100')\n", "        return priority\n", "    elif isinstance(priority, Priority):\n", "        return priority.value\n", "    elif isinstance(priority, str):\n", "        return Priority[priority.upper()].value\n", "    else:\n", "        raise TypeError('priority must be an integer or Priority enum value')\n"], "language": "python", "code": "def get_priority(priority: Union[int, str, Priority]) ->int:\n    \"\"\"\"\"\"\n    if isinstance(priority, int):\n        if priority < 0 or priority > 100:\n            raise ValueError('priority must be between 0 and 100')\n        return priority\n    elif isinstance(priority, Priority):\n        return priority.value\n    elif isinstance(priority, str):\n        return Priority[priority.upper()].value\n    else:\n        raise TypeError('priority must be an integer or Priority enum value')\n", "code_tokens": ["get", "priority", "priority", "union", "int", "str", "priority", "int", "if", "isinstance", "priority", "int", "if", "priority", "0", "or", "priority", "100", "raise", "valueerror", "priority", "must", "be", "between", "0", "and", "100", "return", "priority", "elif", "isinstance", "priority", "priority", "return", "priority", "value", "elif", "isinstance", "priority", "str", "return", "priority", "priority", "upper", "value", "else", "raise", "typeerror", "priority", "must", "be", "an", "integer", "or", "priority", "enum", "value"], "docstring": "priority queue", "docstring_tokens": ["priority", "queue"], "idx": 12}
{"url": "https://github.com/LasLabs/python-helpscout/blob/84bf669417d72ca19641a02c9a660e1ae4271de4/helpscout/request_paginator/__init__.py#L114-L123", "repo": "python-helpscout", "func_name": "post", "original_string": ["    def post(self, json=None):\n", "        \"\"\"Send a POST request and return the JSON decoded result.\n", "\n", "        Args:\n", "            json (dict, optional): Object to encode and send in request.\n", "\n", "        Returns:\n", "            mixed: JSON decoded response data.\n", "        \"\"\"\n", "        return self._call('post', url=self.endpoint, json=json)\n"], "language": "python", "code": "def post(self, json=None):\n    \"\"\"\"\"\"\n    return self._call('post', url=self.endpoint, json=json)\n", "code_tokens": ["post", "self", "json", "none", "return", "self", "call", "post", "url", "self", "endpoint", "json", "json"], "docstring": "httpclient post json", "docstring_tokens": ["httpclient", "post", "json"], "idx": 13}
{"url": "https://github.com/fvalverd/AutoApi-client-Python/blob/a6b04947f80bf988c1515d622c78c587b341b3f4/auto_api_client/__init__.py#L91-L94", "repo": "AutoApi-client-Python", "func_name": "post", "original_string": ["    def post(self, json=None):\n", "        response = self._http(requests.post, json=json)\n", "        if response.status_code == 201:\n", "            return response.json()\n"], "language": "python", "code": "def post(self, json=None):\n    response = self._http(requests.post, json=json)\n    if response.status_code == 201:\n        return response.json()\n", "code_tokens": ["post", "self", "json", "none", "response", "self", "http", "requests", "post", "json", "json", "if", "response", "status", "code", "201", "return", "response", "json"], "docstring": "httpclient post json", "docstring_tokens": ["httpclient", "post", "json"], "idx": 14}
{"url": "https://github.com/vladcalin/gemstone/blob/325a49d17621b9d45ffd2b5eca6f0de284de8ba4/gemstone/discovery/default.py#L19-L31", "repo": "gemstone", "func_name": "make_jsonrpc_call", "original_string": ["    def make_jsonrpc_call(self, url, method, params):\n", "        client = HTTPClient()\n", "        body = json.dumps({\n", "            \"jsonrpc\": \"2.0\",\n", "            \"method\": method,\n", "            \"params\": params,\n", "            \"id\": \"\".join([random.choice(string.ascii_letters) for _ in range(10)])\n", "        })\n", "\n", "        request = HTTPRequest(url, method=\"POST\", headers={\"content-type\": \"application/json\"},\n", "                              body=body)\n", "        result = client.fetch(request)\n", "        return result\n"], "language": "python", "code": "def make_jsonrpc_call(self, url, method, params):\n    client = HTTPClient()\n    body = json.dumps({'jsonrpc': '2.0', 'method': method, 'params': params,\n        'id': ''.join([random.choice(string.ascii_letters) for _ in range(\n        10)])})\n    request = HTTPRequest(url, method='POST', headers={'content-type':\n        'application/json'}, body=body)\n    result = client.fetch(request)\n    return result\n", "code_tokens": ["make", "jsonrpc", "call", "self", "url", "method", "params", "client", "httpclient", "body", "json", "dumps", "jsonrpc", "2", "0", "method", "method", "params", "params", "id", "join", "random", "choice", "string", "ascii", "letters", "for", "in", "range", "10", "request", "httprequest", "url", "method", "post", "headers", "content", "type", "application", "json", "body", "body", "result", "client", "fetch", "request", "return", "result"], "docstring": "httpclient post json", "docstring_tokens": ["httpclient", "post", "json"], "idx": 15}
{"url": "https://github.com/maxweisspoker/simplebitcoinfuncs/blob/ad332433dfcc067e86d2e77fa0c8f1a27daffb63/simplebitcoinfuncs/miscbitcoinfuncs.py#L190-L199", "repo": "simplebitcoinfuncs", "func_name": "inttoDER", "original_string": ["def inttoDER(a):\n", "    '''\n", "    Format an int/long to DER hex format\n", "    '''\n", "\n", "    o = dechex(a,1)\n", "    if int(o[:2],16) > 127:\n", "        o = '00' + o\n", "    olen = dechex(len(o)//2,1)\n", "    return '02' + olen + o\n"], "language": "python", "code": "def inttoDER(a):\n    \"\"\"\"\"\"\n    o = dechex(a, 1)\n    if int(o[:2], 16) > 127:\n        o = '00' + o\n    olen = dechex(len(o) // 2, 1)\n    return '02' + olen + o\n", "code_tokens": ["inttoder", "dechex", "1", "if", "int", "2", "16", "127", "00", "olen", "dechex", "len", "2", "1", "return", "02", "olen"], "docstring": "convert decimal to hex", "docstring_tokens": ["convert", "decimal", "to", "hex"], "idx": 16}
{"url": "https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/textio.py#L121-L140", "repo": "PyDev.Debugger", "func_name": "hexadecimal", "original_string": ["    def hexadecimal(token):\n", "        \"\"\"\n", "        Convert a strip of hexadecimal numbers into binary data.\n", "\n", "        @type  token: str\n", "        @param token: String to parse.\n", "\n", "        @rtype:  str\n", "        @return: Parsed string value.\n", "        \"\"\"\n", "        token = ''.join([ c for c in token if c.isalnum() ])\n", "        if len(token) % 2 != 0:\n", "            raise ValueError(\"Missing characters in hex data\")\n", "        data = ''\n", "        for i in compat.xrange(0, len(token), 2):\n", "            x = token[i:i+2]\n", "            d = int(x, 16)\n", "            s = struct.pack('<B', d)\n", "            data += s\n", "        return data\n"], "language": "python", "code": "def hexadecimal(token):\n    \"\"\"\"\"\"\n    token = ''.join([c for c in token if c.isalnum()])\n    if len(token) % 2 != 0:\n        raise ValueError('Missing characters in hex data')\n    data = ''\n    for i in compat.xrange(0, len(token), 2):\n        x = token[i:i + 2]\n        d = int(x, 16)\n        s = struct.pack('<B', d)\n        data += s\n    return data\n", "code_tokens": ["hexadecimal", "token", "token", "join", "for", "in", "token", "if", "isalnum", "if", "len", "token", "2", "0", "raise", "valueerror", "missing", "characters", "in", "hex", "data", "data", "for", "in", "compat", "xrange", "0", "len", "token", "2", "token", "2", "int", "16", "struct", "pack", "data", "return", "data"], "docstring": "convert decimal to hex", "docstring_tokens": ["convert", "decimal", "to", "hex"], "idx": 17}
{"url": "https://github.com/restran/mountains/blob/a97fee568b112f4e10d878f815d0db3dd0a98d74/mountains/encoding/converter.py#L84-L93", "repo": "mountains", "func_name": "hex2dec", "original_string": ["\n", "def hex2dec(s):\n", "    \"\"\"\n", "    hex2dec\n", "    \u5341\u516d\u8fdb\u5236 to \u5341\u8fdb\u5236\n", "    :param s:\n", "    :return:\n", "    \"\"\"\n", "    if not isinstance(s, str):\n", "        s = str(s)\n"], "language": "python", "code": "def hex2dec(s):\n    \"\"\"\"\"\"\n    if not isinstance(s, str):\n        s = str(s)\n", "code_tokens": ["if", "not", "isinstance", "str", "str"], "docstring": "convert decimal to hex", "docstring_tokens": ["convert", "decimal", "to", "hex"], "idx": 18}
{"url": "https://github.com/mathiasertl/django-ca/blob/976d7ea05276320f20daed2a6d59c8f5660fe976/ca/django_ca/utils.py#L211-L222", "repo": "django-ca", "func_name": "is_power2", "original_string": ["\n", "\n", "def is_power2(num: int) -> bool:\n", "    \"\"\"Return True if `num` is a power of 2.\n", "\n", "    >>> is_power2(4)\n", "    True\n", "    >>> is_power2(3)\n", "    False\n", "    \"\"\"\n", "    return num != 0 and ((num & (num - 1)) == 0)\n", "\n"], "language": "python", "code": "def is_power2(num: int) ->bool:\n    \"\"\"\"\"\"\n    return num != 0 and num & num - 1 == 0\n", "code_tokens": ["is", "num", "int", "bool", "return", "num", "0", "and", "num", "num", "1", "0"], "docstring": "convert decimal to hex", "docstring_tokens": ["convert", "decimal", "to", "hex"], "idx": 19}
{"url": "https://github.com/talkincode/txradius/blob/b86fdbc9be41183680b82b07d3a8e8ea10926e01/txradius/mschap/mschap.py#L94-L101", "repo": "txradius", "func_name": "convert_to_hex_string", "original_string": ["def convert_to_hex_string(string):\n", "    hex_str = \"\"\n", "    for c in string:\n", "        hex_tmp = hex(ord(c))[2:]\n", "        if len(hex_tmp) == 1:\n", "            hex_tmp = \"0\" + hex_tmp\n", "        hex_str += hex_tmp\n", "    return hex_str.upper()\n"], "language": "python", "code": "def convert_to_hex_string(string):\n    hex_str = ''\n    for c in string:\n        hex_tmp = hex(ord(c))[2:]\n        if len(hex_tmp) == 1:\n            hex_tmp = '0' + hex_tmp\n        hex_str += hex_tmp\n    return hex_str.upper()\n", "code_tokens": ["convert", "to", "hex", "string", "string", "hex", "str", "for", "in", "string", "hex", "tmp", "hex", "ord", "2", "if", "len", "hex", "tmp", "1", "hex", "tmp", "0", "hex", "tmp", "hex", "str", "hex", "tmp", "return", "hex", "str", "upper"], "docstring": "convert decimal to hex", "docstring_tokens": ["convert", "decimal", "to", "hex"], "idx": 20}
{"url": "https://github.com/sparklingpandas/sparklingpandas/blob/7d549df4348c979042b683c355aa778fc6d3a768/sparklingpandas/pcontext.py#L68-L155", "repo": "sparklingpandas", "func_name": "read_csv", "original_string": ["    def read_csv(self, file_path, use_whole_file=False, names=None, skiprows=0,\n", "                 *args, **kwargs):\n", "        \"\"\"Read a CSV file in and parse it into Pandas DataFrames. By default,\n", "        the first row from the first partition of that data is parsed and used\n", "        as the column names for the data from. If no 'names' param is\n", "        provided we parse the first row of the first partition of data and\n", "        use it for column names.\n", "\n", "        Parameters\n", "        ----------\n", "        file_path: string\n", "            Path to input. Any valid file path in Spark works here, eg:\n", "            'file:///my/path/in/local/file/system' or 'hdfs:/user/juliet/'\n", "        use_whole_file: boolean\n", "            Whether of not to use the whole file.\n", "        names: list of strings, optional\n", "        skiprows: integer, optional\n", "            indicates how many rows of input to skip. This will\n", "            only be applied to the first partition of the data (so if\n", "            #skiprows > #row in first partition this will not work). Generally\n", "            this shouldn't be an issue for small values of skiprows.\n", "        No other value of header is supported.\n", "        All additional parameters available in pandas.read_csv() are usable\n", "        here.\n", "\n", "        Returns\n", "        -------\n", "        A SparklingPandas DataFrame that contains the data from the\n", "        specified file.\n", "        \"\"\"\n", "        def csv_file(partition_number, files):\n", "            # pylint: disable=unexpected-keyword-arg\n", "            file_count = 0\n", "            for _, contents in files:\n", "                # Only skip lines on the first file\n", "                if partition_number == 0 and file_count == 0 and _skiprows > 0:\n", "                    yield pandas.read_csv(\n", "                        sio(contents), *args,\n", "                        header=None,\n", "                        names=mynames,\n", "                        skiprows=_skiprows,\n", "                        **kwargs)\n", "                else:\n", "                    file_count += 1\n", "                    yield pandas.read_csv(\n", "                        sio(contents), *args,\n", "                        header=None,\n", "                        names=mynames,\n", "                        **kwargs)\n", "\n", "        def csv_rows(partition_number, rows):\n", "            # pylint: disable=unexpected-keyword-arg\n", "            in_str = \"\\n\".join(rows)\n", "            if partition_number == 0:\n", "                return iter([\n", "                    pandas.read_csv(\n", "                        sio(in_str), *args, header=None,\n", "                        names=mynames,\n", "                        skiprows=_skiprows,\n", "                        **kwargs)])\n", "            else:\n", "                # could use .iterows instead?\n", "                return iter([pandas.read_csv(sio(in_str), *args, header=None,\n", "                                             names=mynames, **kwargs)])\n", "\n", "        # If we need to peak at the first partition and determine the column\n", "        # names\n", "        mynames = None\n", "        _skiprows = skiprows\n", "        if names:\n", "            mynames = names\n", "        else:\n", "            # In the future we could avoid this expensive call.\n", "            first_line = self.spark_ctx.textFile(file_path).first()\n", "            frame = pandas.read_csv(sio(first_line), **kwargs)\n", "            # pylint sees frame as a tuple despite it being a DataFrame\n", "            mynames = list(frame.columns)\n", "            _skiprows += 1\n", "\n", "        # Do the actual load\n", "        if use_whole_file:\n", "            return self.from_pandas_rdd(\n", "                self.spark_ctx.wholeTextFiles(file_path)\n", "                .mapPartitionsWithIndex(csv_file))\n", "        else:\n", "            return self.from_pandas_rdd(\n", "                self.spark_ctx.textFile(file_path)\n", "                    .mapPartitionsWithIndex(csv_rows))\n"], "language": "python", "code": "def read_csv(self, file_path, use_whole_file=False, names=None, skiprows=0,\n    *args, **kwargs):\n    \"\"\"\"\"\"\n\n    def csv_file(partition_number, files):\n        file_count = 0\n        for _, contents in files:\n            if partition_number == 0 and file_count == 0 and _skiprows > 0:\n                yield pandas.read_csv(sio(contents), *args, header=None,\n                    names=mynames, skiprows=_skiprows, **kwargs)\n            else:\n                file_count += 1\n                yield pandas.read_csv(sio(contents), *args, header=None,\n                    names=mynames, **kwargs)\n\n    def csv_rows(partition_number, rows):\n        in_str = '\\n'.join(rows)\n        if partition_number == 0:\n            return iter([pandas.read_csv(sio(in_str), *args, header=None,\n                names=mynames, skiprows=_skiprows, **kwargs)])\n        else:\n            return iter([pandas.read_csv(sio(in_str), *args, header=None,\n                names=mynames, **kwargs)])\n    mynames = None\n    _skiprows = skiprows\n    if names:\n        mynames = names\n    else:\n        first_line = self.spark_ctx.textFile(file_path).first()\n        frame = pandas.read_csv(sio(first_line), **kwargs)\n        mynames = list(frame.columns)\n        _skiprows += 1\n    if use_whole_file:\n        return self.from_pandas_rdd(self.spark_ctx.wholeTextFiles(file_path\n            ).mapPartitionsWithIndex(csv_file))\n    else:\n        return self.from_pandas_rdd(self.spark_ctx.textFile(file_path).\n            mapPartitionsWithIndex(csv_rows))\n", "code_tokens": ["read", "csv", "self", "file", "path", "use", "whole", "file", "false", "names", "none", "skiprows", "0", "args", "kwargs", "def", "csv", "file", "partition", "number", "files", "file", "count", "0", "for", "contents", "in", "files", "if", "partition", "number", "0", "and", "file", "count", "0", "and", "skiprows", "0", "yield", "pandas", "read", "csv", "sio", "contents", "args", "header", "none", "names", "mynames", "skiprows", "skiprows", "kwargs", "else", "file", "count", "1", "yield", "pandas", "read", "csv", "sio", "contents", "args", "header", "none", "names", "mynames", "kwargs", "def", "csv", "rows", "partition", "number", "rows", "in", "str", "join", "rows", "if", "partition", "number", "0", "return", "iter", "pandas", "read", "csv", "sio", "in", "str", "args", "header", "none", "names", "mynames", "skiprows", "skiprows", "kwargs", "else", "return", "iter", "pandas", "read", "csv", "sio", "in", "str", "args", "header", "none", "names", "mynames", "kwargs", "mynames", "none", "skiprows", "skiprows", "if", "names", "mynames", "names", "else", "first", "line", "self", "spark", "ctx", "textfile", "file", "path", "first", "frame", "pandas", "read", "csv", "sio", "first", "line", "kwargs", "mynames", "list", "frame", "columns", "skiprows", "1", "if", "use", "whole", "file", "return", "self", "from", "pandas", "rdd", "self", "spark", "ctx", "wholetextfiles", "file", "path", "mappartitionswithindex", "csv", "file", "else", "return", "self", "from", "pandas", "rdd", "self", "spark", "ctx", "textfile", "file", "path", "mappartitionswithindex", "csv", "rows"], "docstring": "how to read .csv file in an efficient way?", "docstring_tokens": ["how", "to", "read", "csv", "file", "in", "an", "efficient", "way"], "idx": 21}
{"url": "https://github.com/probcomp/crosscat/blob/4a05bddb06a45f3b7b3e05e095720f16257d1535/src/utils/data_utils.py#L297-L304", "repo": "crosscat", "func_name": "read_csv", "original_string": ["def read_csv(filename, has_header=True):\n", "    with open(filename) as fh:\n", "        csv_reader = csv.reader(fh)\n", "        header = None\n", "        if has_header:\n", "            header = csv_reader.next()\n", "        rows = [row for row in csv_reader]\n", "    return header, rows\n"], "language": "python", "code": "def read_csv(filename, has_header=True):\n    with open(filename) as fh:\n        csv_reader = csv.reader(fh)\n        header = None\n        if has_header:\n            header = csv_reader.next()\n        rows = [row for row in csv_reader]\n    return header, rows\n", "code_tokens": ["read", "csv", "filename", "has", "header", "true", "with", "open", "filename", "as", "fh", "csv", "reader", "csv", "reader", "fh", "header", "none", "if", "has", "header", "header", "csv", "reader", "next", "rows", "row", "for", "row", "in", "csv", "reader", "return", "header", "rows"], "docstring": "how to read .csv file in an efficient way?", "docstring_tokens": ["how", "to", "read", "csv", "file", "in", "an", "efficient", "way"], "idx": 22}
{"url": "https://github.com/dshean/pygeotools/blob/5ac745717c0098d01eb293ff1fe32fd7358c76ab/pygeotools/lib/iolib.py#L604-L626", "repo": "pygeotools", "func_name": "readcsv", "original_string": ["def readcsv(fn):\n", "    \"\"\"\n", "    Wrapper to read arbitrary csv, check for header\n", "\n", "    Needs some work to be more robust, quickly added for demcoreg sampling\n", "    \"\"\"\n", "    import csv\n", "    #Check first line for header\n", "    with open(fn, 'r') as f:\n", "        reader = csv.DictReader(f)\n", "        hdr = reader.fieldnames\n", "\n", "    #Assume there is a header on first line, check \n", "    skiprows = 1\n", "    if np.all(f.isdigit() for f in hdr):\n", "        hdr = None\n", "        skiprows = 0\n", "\n", "    #Check header for lat/lon/z or x/y/z tags\n", "\n", "    #Should probably do genfromtxt here if header exists and dtype of cols is variable\n", "    pts = np.loadtxt(fn, delimiter=',', skiprows=skiprows, dtype=None)\n", "    return pts\n"], "language": "python", "code": "def readcsv(fn):\n    \"\"\"\"\"\"\n    import csv\n    with open(fn, 'r') as f:\n        reader = csv.DictReader(f)\n        hdr = reader.fieldnames\n    skiprows = 1\n    if np.all(f.isdigit() for f in hdr):\n        hdr = None\n        skiprows = 0\n    pts = np.loadtxt(fn, delimiter=',', skiprows=skiprows, dtype=None)\n    return pts\n", "code_tokens": ["readcsv", "fn", "import", "csv", "with", "open", "fn", "as", "reader", "csv", "dictreader", "hdr", "reader", "fieldnames", "skiprows", "1", "if", "np", "all", "isdigit", "for", "in", "hdr", "hdr", "none", "skiprows", "0", "pts", "np", "loadtxt", "fn", "delimiter", "skiprows", "skiprows", "dtype", "none", "return", "pts"], "docstring": "how to read .csv file in an efficient way?", "docstring_tokens": ["how", "to", "read", "csv", "file", "in", "an", "efficient", "way"], "idx": 23}
{"url": "https://github.com/PeerAssets/pypeerassets/blob/8927b4a686887f44fe2cd9de777e2c827c948987/pypeerassets/transactions.py#L99-L114", "repo": "pypeerassets", "func_name": "serialize", "original_string": ["    def serialize(self):\n", "        from itertools import chain\n", "        result = Stream()\n", "        result << self.version.to_bytes(4, 'little')\n", "\n", "        if self.network.tx_timestamp:\n", "            result << self.timestamp.to_bytes(4, 'little')\n", "\n", "        result << Parser.to_varint(len(self.ins))\n", "        # the most efficient way to flatten a list in python\n", "        result << bytearray(chain.from_iterable(txin.serialize() for txin in self.ins))\n", "        result << Parser.to_varint(len(self.outs))\n", "        # the most efficient way to flatten a list in python\n", "        result << bytearray(chain.from_iterable(txout.serialize() for txout in self.outs))\n", "        result << self.locktime\n", "        return result.serialize()\n"], "language": "python", "code": "def serialize(self):\n    from itertools import chain\n    result = Stream()\n    result << self.version.to_bytes(4, 'little')\n    if self.network.tx_timestamp:\n        result << self.timestamp.to_bytes(4, 'little')\n    result << Parser.to_varint(len(self.ins))\n    result << bytearray(chain.from_iterable(txin.serialize() for txin in\n        self.ins))\n    result << Parser.to_varint(len(self.outs))\n    result << bytearray(chain.from_iterable(txout.serialize() for txout in\n        self.outs))\n    result << self.locktime\n    return result.serialize()\n", "code_tokens": ["serialize", "self", "from", "itertools", "import", "chain", "result", "stream", "result", "self", "version", "to", "bytes", "4", "little", "if", "self", "network", "tx", "timestamp", "result", "self", "timestamp", "to", "bytes", "4", "little", "result", "parser", "to", "varint", "len", "self", "ins", "result", "bytearray", "chain", "from", "iterable", "txin", "serialize", "for", "txin", "in", "self", "ins", "result", "parser", "to", "varint", "len", "self", "outs", "result", "bytearray", "chain", "from", "iterable", "txout", "serialize", "for", "txout", "in", "self", "outs", "result", "self", "locktime", "return", "result", "serialize"], "docstring": "how to read .csv file in an efficient way?", "docstring_tokens": ["how", "to", "read", "csv", "file", "in", "an", "efficient", "way"], "idx": 24}
{"url": "https://github.com/lappis-unb/salic-ml/blob/1b3ebc4f8067740999897ccffd9892dc94482a93/src/salicml/utils/read_csv.py#L10-L15", "repo": "salic-ml", "func_name": "read_csv", "original_string": ["def read_csv(csv_name, usecols=None):\n", "    \"\"\"Returns a DataFrame from a .csv file stored in /data/raw/\"\"\"\n", "    csv_path = os.path.join(DATA_FOLDER, csv_name)\n", "    csv = pd.read_csv(csv_path, low_memory=False,\n", "                      usecols=usecols, encoding=\"utf-8\")\n", "    return csv\n"], "language": "python", "code": "def read_csv(csv_name, usecols=None):\n    \"\"\"\"\"\"\n    csv_path = os.path.join(DATA_FOLDER, csv_name)\n    csv = pd.read_csv(csv_path, low_memory=False, usecols=usecols, encoding\n        ='utf-8')\n    return csv\n", "code_tokens": ["read", "csv", "csv", "name", "usecols", "none", "csv", "path", "os", "path", "join", "data", "folder", "csv", "name", "csv", "pd", "read", "csv", "csv", "path", "low", "memory", "false", "usecols", "usecols", "encoding", "utf", "8", "return", "csv"], "docstring": "how to read .csv file in an efficient way?", "docstring_tokens": ["how", "to", "read", "csv", "file", "in", "an", "efficient", "way"], "idx": 25}
{"url": "https://github.com/thespacedoctor/sherlock/blob/2c80fb6fa31b04e7820e6928e3d437a21e692dd3/sherlock/imports/ned_d.py#L177-L274", "repo": "sherlock", "func_name": "_create_dictionary_of_ned_d", "original_string": ["\n", "    def _create_dictionary_of_ned_d(\n", "            self):\n", "        \"\"\"create a list of dictionaries containing all the rows in the ned_d catalogue\n", "\n", "        **Return**\n", "\n", "        - ``dictList`` - a list of dictionaries containing all the rows in the ned_d catalogue\n", "        \n", "\n", "        .. todo ::\n", "\n", "            - update key arguments values and definitions with defaults\n", "            - update return values and definitions\n", "            - update usage examples and text\n", "            - update docstring text\n", "            - check sublime snippet exists\n", "            - clip any useful text to docs mindmap\n", "            - regenerate the docs and check redendering of this docstring\n", "        \"\"\"\n", "        self.log.debug(\n", "            'starting the ``_create_dictionary_of_ned_d`` method')\n", "\n", "        count = 0\n", "        with open(self.pathToDataFile, 'r') as csvFile:\n", "            csvReader = csv.reader(\n", "                csvFile, dialect='excel', delimiter=',', quotechar='\"')\n", "            totalRows = sum(1 for row in csvReader)\n", "        csvFile.close()\n", "        totalCount = totalRows\n", "\n", "        with open(self.pathToDataFile, 'r') as csvFile:\n", "            csvReader = csv.reader(\n", "                csvFile, dialect='excel', delimiter=',', quotechar='\"')\n", "            theseKeys = []\n", "            dictList = []\n", "            for row in csvReader:\n", "                if len(theseKeys) == 0:\n", "                    totalRows -= 1\n", "                if \"Exclusion Code\" in row and \"Hubble const.\" in row:\n", "                    for i in row:\n", "                        if i == \"redshift (z)\":\n", "                            theseKeys.append(\"redshift\")\n", "                        elif i == \"Hubble const.\":\n", "                            theseKeys.append(\"hubble_const\")\n", "                        elif i == \"G\":\n", "                            theseKeys.append(\"galaxy_index_id\")\n", "                        elif i == \"err\":\n", "                            theseKeys.append(\"dist_mod_err\")\n", "                        elif i == \"D (Mpc)\":\n", "                            theseKeys.append(\"dist_mpc\")\n", "                        elif i == \"Date (Yr. - 1980)\":\n", "                            theseKeys.append(\"ref_date\")\n", "                        elif i == \"REFCODE\":\n", "                            theseKeys.append(\"ref\")\n", "                        elif i == \"Exclusion Code\":\n", "                            theseKeys.append(\"dist_in_ned_flag\")\n", "                        elif i == \"Adopted LMC modulus\":\n", "                            theseKeys.append(\"lmc_mod\")\n", "                        elif i == \"m-M\":\n", "                            theseKeys.append(\"dist_mod\")\n", "                        elif i == \"Notes\":\n", "                            theseKeys.append(\"notes\")\n", "                        elif i == \"SN ID\":\n", "                            theseKeys.append(\"dist_derived_from_sn\")\n", "                        elif i == \"method\":\n", "                            theseKeys.append(\"dist_method\")\n", "                        elif i == \"Galaxy ID\":\n", "                            theseKeys.append(\"primary_ned_id\")\n", "                        elif i == \"D\":\n", "                            theseKeys.append(\"dist_index_id\")\n", "                        else:\n", "                            theseKeys.append(i)\n", "                    continue\n", "\n", "                if len(theseKeys):\n", "                    count += 1\n", "                    if count > 1:\n", "                        # Cursor up one line and clear line\n", "                        sys.stdout.write(\"\\x1b[1A\\x1b[2K\")\n", "                    if count > totalCount:\n", "                        count = totalCount\n", "                    percent = (old_div(float(count), float(totalCount))) * 100.\n", "                    print(\n", "                        \"%(count)s / %(totalCount)s (%(percent)1.1f%%) rows added to memory\" % locals())\n", "                    rowDict = {}\n", "                    for t, r in zip(theseKeys, row):\n", "                        rowDict[t] = r\n", "                        if t == \"ref_date\":\n", "                            try:\n", "                                rowDict[t] = int(r) + 1980\n", "                            except:\n", "                                rowDict[t] = None\n", "\n", "                    if rowDict[\"dist_index_id\"] != \"999999\":\n", "                        dictList.append(rowDict)\n", "\n", "        csvFile.close()\n"], "language": "python", "code": "def _create_dictionary_of_ned_d(self):\n    \"\"\"\"\"\"\n    self.log.debug('starting the ``_create_dictionary_of_ned_d`` method')\n    count = 0\n    with open(self.pathToDataFile, 'r') as csvFile:\n        csvReader = csv.reader(csvFile, dialect='excel', delimiter=',',\n            quotechar='\"')\n        totalRows = sum(1 for row in csvReader)\n    csvFile.close()\n    totalCount = totalRows\n    with open(self.pathToDataFile, 'r') as csvFile:\n        csvReader = csv.reader(csvFile, dialect='excel', delimiter=',',\n            quotechar='\"')\n        theseKeys = []\n        dictList = []\n        for row in csvReader:\n            if len(theseKeys) == 0:\n                totalRows -= 1\n            if 'Exclusion Code' in row and 'Hubble const.' in row:\n                for i in row:\n                    if i == 'redshift (z)':\n                        theseKeys.append('redshift')\n                    elif i == 'Hubble const.':\n                        theseKeys.append('hubble_const')\n                    elif i == 'G':\n                        theseKeys.append('galaxy_index_id')\n                    elif i == 'err':\n                        theseKeys.append('dist_mod_err')\n                    elif i == 'D (Mpc)':\n                        theseKeys.append('dist_mpc')\n                    elif i == 'Date (Yr. - 1980)':\n                        theseKeys.append('ref_date')\n                    elif i == 'REFCODE':\n                        theseKeys.append('ref')\n                    elif i == 'Exclusion Code':\n                        theseKeys.append('dist_in_ned_flag')\n                    elif i == 'Adopted LMC modulus':\n                        theseKeys.append('lmc_mod')\n                    elif i == 'm-M':\n                        theseKeys.append('dist_mod')\n                    elif i == 'Notes':\n                        theseKeys.append('notes')\n                    elif i == 'SN ID':\n                        theseKeys.append('dist_derived_from_sn')\n                    elif i == 'method':\n                        theseKeys.append('dist_method')\n                    elif i == 'Galaxy ID':\n                        theseKeys.append('primary_ned_id')\n                    elif i == 'D':\n                        theseKeys.append('dist_index_id')\n                    else:\n                        theseKeys.append(i)\n                continue\n            if len(theseKeys):\n                count += 1\n                if count > 1:\n                    sys.stdout.write('\\x1b[1A\\x1b[2K')\n                if count > totalCount:\n                    count = totalCount\n                percent = old_div(float(count), float(totalCount)) * 100.0\n                print(\n                    '%(count)s / %(totalCount)s (%(percent)1.1f%%) rows added to memory'\n                     % locals())\n                rowDict = {}\n                for t, r in zip(theseKeys, row):\n                    rowDict[t] = r\n                    if t == 'ref_date':\n                        try:\n                            rowDict[t] = int(r) + 1980\n                        except:\n                            rowDict[t] = None\n                if rowDict['dist_index_id'] != '999999':\n                    dictList.append(rowDict)\n    csvFile.close()\n", "code_tokens": ["create", "dictionary", "of", "ned", "self", "self", "log", "debug", "starting", "the", "create", "dictionary", "of", "ned", "method", "count", "0", "with", "open", "self", "pathtodatafile", "as", "csvfile", "csvreader", "csv", "reader", "csvfile", "dialect", "excel", "delimiter", "quotechar", "totalrows", "sum", "1", "for", "row", "in", "csvreader", "csvfile", "close", "totalcount", "totalrows", "with", "open", "self", "pathtodatafile", "as", "csvfile", "csvreader", "csv", "reader", "csvfile", "dialect", "excel", "delimiter", "quotechar", "thesekeys", "dictlist", "for", "row", "in", "csvreader", "if", "len", "thesekeys", "0", "totalrows", "1", "if", "exclusion", "code", "in", "row", "and", "hubble", "const", "in", "row", "for", "in", "row", "if", "redshift", "thesekeys", "append", "redshift", "elif", "hubble", "const", "thesekeys", "append", "hubble", "const", "elif", "thesekeys", "append", "galaxy", "index", "id", "elif", "err", "thesekeys", "append", "dist", "mod", "err", "elif", "mpc", "thesekeys", "append", "dist", "mpc", "elif", "date", "yr", "1980", "thesekeys", "append", "ref", "date", "elif", "refcode", "thesekeys", "append", "ref", "elif", "exclusion", "code", "thesekeys", "append", "dist", "in", "ned", "flag", "elif", "adopted", "lmc", "modulus", "thesekeys", "append", "lmc", "mod", "elif", "thesekeys", "append", "dist", "mod", "elif", "notes", "thesekeys", "append", "notes", "elif", "sn", "id", "thesekeys", "append", "dist", "derived", "from", "sn", "elif", "method", "thesekeys", "append", "dist", "method", "elif", "galaxy", "id", "thesekeys", "append", "primary", "ned", "id", "elif", "thesekeys", "append", "dist", "index", "id", "else", "thesekeys", "append", "continue", "if", "len", "thesekeys", "count", "1", "if", "count", "1", "sys", "stdout", "write", "if", "count", "totalcount", "count", "totalcount", "percent", "old", "div", "float", "count", "float", "totalcount", "100", "0", "print", "count", "totalcount", "percent", "1", "rows", "added", "to", "memory", "locals", "rowdict", "for", "in", "zip", "thesekeys", "row", "rowdict", "if", "ref", "date", "try", "rowdict", "int", "1980", "except", "rowdict", "none", "if", "rowdict", "dist", "index", "id", "999999", "dictlist", "append", "rowdict", "csvfile", "close"], "docstring": "how to read .csv file in an efficient way?", "docstring_tokens": ["how", "to", "read", "csv", "file", "in", "an", "efficient", "way"], "idx": 26}
{"url": "https://github.com/remix/partridge/blob/0ba80fa30035e5e09fd8d7a7bdf1f28b93d53d03/partridge/gtfs.py#L89-L112", "repo": "partridge", "func_name": "_read_csv", "original_string": ["    def _read_csv(self, filename: str) -> pd.DataFrame:\n", "        path = self._pathmap.get(filename)\n", "        columns = self._config.nodes.get(filename, {}).get(\"required_columns\", [])\n", "\n", "        if path is None or os.path.getsize(path) == 0:\n", "            # The file is missing or empty. Return an empty\n", "            # DataFrame containing any required columns.\n", "            return empty_df(columns)\n", "\n", "        # If the file isn't in the zip, return an empty DataFrame.\n", "        with open(path, \"rb\") as f:\n", "            encoding = detect_encoding(f)\n", "\n", "        df = pd.read_csv(path, dtype=np.unicode, encoding=encoding, index_col=False)\n", "\n", "        # Strip leading/trailing whitespace from column names\n", "        df.rename(columns=lambda x: x.strip(), inplace=True)\n", "\n", "        if not df.empty:\n", "            # Strip leading/trailing whitespace from column values\n", "            for col in df.columns:\n", "                df[col] = df[col].str.strip()\n", "\n", "        return df\n"], "language": "python", "code": "def _read_csv(self, filename: str) ->pd.DataFrame:\n    path = self._pathmap.get(filename)\n    columns = self._config.nodes.get(filename, {}).get('required_columns', [])\n    if path is None or os.path.getsize(path) == 0:\n        return empty_df(columns)\n    with open(path, 'rb') as f:\n        encoding = detect_encoding(f)\n    df = pd.read_csv(path, dtype=np.unicode, encoding=encoding, index_col=False\n        )\n    df.rename(columns=lambda x: x.strip(), inplace=True)\n    if not df.empty:\n        for col in df.columns:\n            df[col] = df[col].str.strip()\n    return df\n", "code_tokens": ["read", "csv", "self", "filename", "str", "pd", "dataframe", "path", "self", "pathmap", "get", "filename", "columns", "self", "config", "nodes", "get", "filename", "get", "required", "columns", "if", "path", "is", "none", "or", "os", "path", "getsize", "path", "0", "return", "empty", "df", "columns", "with", "open", "path", "rb", "as", "encoding", "detect", "encoding", "df", "pd", "read", "csv", "path", "dtype", "np", "unicode", "encoding", "encoding", "index", "col", "false", "df", "rename", "columns", "lambda", "strip", "inplace", "true", "if", "not", "df", "empty", "for", "col", "in", "df", "columns", "df", "col", "df", "col", "str", "strip", "return", "df"], "docstring": "how to read .csv file in an efficient way?", "docstring_tokens": ["how", "to", "read", "csv", "file", "in", "an", "efficient", "way"], "idx": 27}
{"url": "https://github.com/meejah/txtorcon/blob/14053b95adf0b4bd9dd9c317bece912a26578a93/txtorcon/socks.py#L119-L129", "repo": "txtorcon", "func_name": "send_data", "original_string": ["    def send_data(self, callback):\n", "        \"\"\"\n", "        drain all pending data by calling `callback()` on it\n", "        \"\"\"\n", "        # a \"for x in self._outgoing_data\" would potentially be more\n", "        # efficient, but then there's no good way to bubble exceptions\n", "        # from callback() out without lying about how much data we\n", "        # processed .. or eat the exceptions in here.\n", "        while len(self._outgoing_data):\n", "            data = self._outgoing_data.pop(0)\n", "            callback(data)\n"], "language": "python", "code": "def send_data(self, callback):\n    \"\"\"\"\"\"\n    while len(self._outgoing_data):\n        data = self._outgoing_data.pop(0)\n        callback(data)\n", "code_tokens": ["send", "data", "self", "callback", "while", "len", "self", "outgoing", "data", "data", "self", "outgoing", "data", "pop", "0", "callback", "data"], "docstring": "how to read .csv file in an efficient way?", "docstring_tokens": ["how", "to", "read", "csv", "file", "in", "an", "efficient", "way"], "idx": 28}
{"url": "https://github.com/mabuchilab/QNET/blob/cc20d26dad78691d34c67173e5cd67dcac94208a/src/qnet/algebra/core/matrix_algebra.py#L136-L140", "repo": "QNET", "func_name": "__mul__", "original_string": ["    def __mul__(self, other):\n", "        if isinstance(other, Matrix):\n", "            return Matrix(self.matrix.dot(other.matrix))\n", "        else:\n", "            return Matrix(self.matrix * other)\n"], "language": "python", "code": "def __mul__(self, other):\n    if isinstance(other, Matrix):\n        return Matrix(self.matrix.dot(other.matrix))\n    else:\n        return Matrix(self.matrix * other)\n", "code_tokens": ["mul", "self", "other", "if", "isinstance", "other", "matrix", "return", "matrix", "self", "matrix", "dot", "other", "matrix", "else", "return", "matrix", "self", "matrix", "other"], "docstring": "matrix multiply", "docstring_tokens": ["matrix", "multiply"], "idx": 29}
{"url": "https://github.com/mabuchilab/QNET/blob/cc20d26dad78691d34c67173e5cd67dcac94208a/src/qnet/algebra/core/matrix_algebra.py#L127-L131", "repo": "QNET", "func_name": "__add__", "original_string": ["    def __add__(self, other):\n", "        if isinstance(other, Matrix):\n", "            return Matrix(self.matrix + other.matrix)\n", "        else:\n", "            return Matrix(self.matrix + other)\n"], "language": "python", "code": "def __add__(self, other):\n    if isinstance(other, Matrix):\n        return Matrix(self.matrix + other.matrix)\n    else:\n        return Matrix(self.matrix + other)\n", "code_tokens": ["add", "self", "other", "if", "isinstance", "other", "matrix", "return", "matrix", "self", "matrix", "other", "matrix", "else", "return", "matrix", "self", "matrix", "other"], "docstring": "matrix multiply", "docstring_tokens": ["matrix", "multiply"], "idx": 30}
{"url": "https://github.com/churchill-lab/emase/blob/ae3c6955bb175c1dec88dbf9fac1a7dcc16f4449/emase/Sparse3DMatrix.py#L130-L157", "repo": "emase", "func_name": "__mul__", "original_string": ["    def __mul__(self, other):\n", "        if self.finalized:\n", "            dmat = self.__class__()\n", "            dmat.shape = self.shape\n", "            if isinstance(other, Sparse3DMatrix):  # element-wise multiplication between same kind\n", "                if other.finalized:\n", "                    for hid in xrange(self.shape[1]):\n", "                        dmat.data.append(self.data[hid].multiply(other.data[hid]))\n", "                else:\n", "                    raise RuntimeError('Both matrices must be finalized.')\n", "            elif isinstance(other, (np.ndarray, csc_matrix, csr_matrix)):  # matrix-matrix multiplication\n", "                for hid in xrange(self.shape[1]):\n", "                    dmat.data.append(self.data[hid] * other)\n", "                dmat.shape = (other.shape[1], self.shape[1], self.shape[2])\n", "            elif isinstance(other, (coo_matrix, lil_matrix)):              # matrix-matrix multiplication\n", "                other_csc = other.tocsc()\n", "                for hid in xrange(self.shape[1]):\n", "                    dmat.data.append(self.data[hid] * other_csc)\n", "                dmat.shape = (other_csc.shape[1], self.shape[1], self.shape[2])\n", "            elif isinstance(other, Number):  # rescaling of matrix\n", "                for hid in xrange(self.shape[1]):\n", "                    dmat.data.append(self.data[hid] * other)\n", "            else:\n", "                raise TypeError('This operator is not supported between the given types.')\n", "            dmat.finalized = True\n", "            return dmat\n", "        else:\n", "            raise RuntimeError('The original matrix must be finalized.')\n"], "language": "python", "code": "def __mul__(self, other):\n    if self.finalized:\n        dmat = self.__class__()\n        dmat.shape = self.shape\n        if isinstance(other, Sparse3DMatrix):\n            if other.finalized:\n                for hid in xrange(self.shape[1]):\n                    dmat.data.append(self.data[hid].multiply(other.data[hid]))\n            else:\n                raise RuntimeError('Both matrices must be finalized.')\n        elif isinstance(other, (np.ndarray, csc_matrix, csr_matrix)):\n            for hid in xrange(self.shape[1]):\n                dmat.data.append(self.data[hid] * other)\n            dmat.shape = other.shape[1], self.shape[1], self.shape[2]\n        elif isinstance(other, (coo_matrix, lil_matrix)):\n            other_csc = other.tocsc()\n            for hid in xrange(self.shape[1]):\n                dmat.data.append(self.data[hid] * other_csc)\n            dmat.shape = other_csc.shape[1], self.shape[1], self.shape[2]\n        elif isinstance(other, Number):\n            for hid in xrange(self.shape[1]):\n                dmat.data.append(self.data[hid] * other)\n        else:\n            raise TypeError(\n                'This operator is not supported between the given types.')\n        dmat.finalized = True\n        return dmat\n    else:\n        raise RuntimeError('The original matrix must be finalized.')\n", "code_tokens": ["mul", "self", "other", "if", "self", "finalized", "dmat", "self", "class", "dmat", "shape", "self", "shape", "if", "isinstance", "other", "if", "other", "finalized", "for", "hid", "in", "xrange", "self", "shape", "1", "dmat", "data", "append", "self", "data", "hid", "multiply", "other", "data", "hid", "else", "raise", "runtimeerror", "both", "matrices", "must", "be", "finalized", "elif", "isinstance", "other", "np", "ndarray", "csc", "matrix", "csr", "matrix", "for", "hid", "in", "xrange", "self", "shape", "1", "dmat", "data", "append", "self", "data", "hid", "other", "dmat", "shape", "other", "shape", "1", "self", "shape", "1", "self", "shape", "2", "elif", "isinstance", "other", "coo", "matrix", "lil", "matrix", "other", "csc", "other", "tocsc", "for", "hid", "in", "xrange", "self", "shape", "1", "dmat", "data", "append", "self", "data", "hid", "other", "csc", "dmat", "shape", "other", "csc", "shape", "1", "self", "shape", "1", "self", "shape", "2", "elif", "isinstance", "other", "number", "for", "hid", "in", "xrange", "self", "shape", "1", "dmat", "data", "append", "self", "data", "hid", "other", "else", "raise", "typeerror", "this", "operator", "is", "not", "supported", "between", "the", "given", "types", "dmat", "finalized", "true", "return", "dmat", "else", "raise", "runtimeerror", "the", "original", "matrix", "must", "be", "finalized"], "docstring": "matrix multiply", "docstring_tokens": ["matrix", "multiply"], "idx": 31}
{"url": "https://github.com/fogleman/pg/blob/124ea3803c788b2c98c4f3a428e5d26842a67b58/pg/core.py#L93-L97", "repo": "pg", "func_name": "multiply", "original_string": ["    def multiply(self, matrix):\n", "        positions = [matrix * x for x in self.positions]\n", "        normals = list(self.normals)\n", "        uvs = list(self.uvs)\n", "        return Mesh(positions, normals, uvs)\n"], "language": "python", "code": "def multiply(self, matrix):\n    positions = [(matrix * x) for x in self.positions]\n    normals = list(self.normals)\n    uvs = list(self.uvs)\n    return Mesh(positions, normals, uvs)\n", "code_tokens": ["multiply", "self", "matrix", "positions", "matrix", "for", "in", "self", "positions", "normals", "list", "self", "normals", "uvs", "list", "self", "uvs", "return", "mesh", "positions", "normals", "uvs"], "docstring": "matrix multiply", "docstring_tokens": ["matrix", "multiply"], "idx": 32}
{"url": "https://github.com/cmbruns/pyopenvr/blob/68395d26bb3df6ab1f0f059c38d441f962938be6/src/openvr/gl_renderer.py#L17-L32", "repo": "pyopenvr", "func_name": "matrixForOpenVrMatrix", "original_string": ["def matrixForOpenVrMatrix(mat):\n", "    if len(mat.m) == 4: # HmdMatrix44_t?\n", "        result = numpy.matrix(\n", "                ((mat.m[0][0], mat.m[1][0], mat.m[2][0], mat.m[3][0]),\n", "                 (mat.m[0][1], mat.m[1][1], mat.m[2][1], mat.m[3][1]), \n", "                 (mat.m[0][2], mat.m[1][2], mat.m[2][2], mat.m[3][2]), \n", "                 (mat.m[0][3], mat.m[1][3], mat.m[2][3], mat.m[3][3]),)\n", "            , numpy.float32)\n", "    elif len(mat.m) == 3: # HmdMatrix34_t?\n", "        result = numpy.matrix(\n", "                ((mat.m[0][0], mat.m[1][0], mat.m[2][0], 0.0),\n", "                 (mat.m[0][1], mat.m[1][1], mat.m[2][1], 0.0), \n", "                 (mat.m[0][2], mat.m[1][2], mat.m[2][2], 0.0), \n", "                 (mat.m[0][3], mat.m[1][3], mat.m[2][3], 1.0),)\n", "            , numpy.float32)\n", "    return result\n"], "language": "python", "code": "def matrixForOpenVrMatrix(mat):\n    if len(mat.m) == 4:\n        result = numpy.matrix(((mat.m[0][0], mat.m[1][0], mat.m[2][0], mat.\n            m[3][0]), (mat.m[0][1], mat.m[1][1], mat.m[2][1], mat.m[3][1]),\n            (mat.m[0][2], mat.m[1][2], mat.m[2][2], mat.m[3][2]), (mat.m[0]\n            [3], mat.m[1][3], mat.m[2][3], mat.m[3][3])), numpy.float32)\n    elif len(mat.m) == 3:\n        result = numpy.matrix(((mat.m[0][0], mat.m[1][0], mat.m[2][0], 0.0),\n            (mat.m[0][1], mat.m[1][1], mat.m[2][1], 0.0), (mat.m[0][2], mat\n            .m[1][2], mat.m[2][2], 0.0), (mat.m[0][3], mat.m[1][3], mat.m[2\n            ][3], 1.0)), numpy.float32)\n    return result\n", "code_tokens": ["matrixforopenvrmatrix", "mat", "if", "len", "mat", "4", "result", "numpy", "matrix", "mat", "0", "0", "mat", "1", "0", "mat", "2", "0", "mat", "3", "0", "mat", "0", "1", "mat", "1", "1", "mat", "2", "1", "mat", "3", "1", "mat", "0", "2", "mat", "1", "2", "mat", "2", "2", "mat", "3", "2", "mat", "0", "3", "mat", "1", "3", "mat", "2", "3", "mat", "3", "3", "numpy", "elif", "len", "mat", "3", "result", "numpy", "matrix", "mat", "0", "0", "mat", "1", "0", "mat", "2", "0", "0", "0", "mat", "0", "1", "mat", "1", "1", "mat", "2", "1", "0", "0", "mat", "0", "2", "mat", "1", "2", "mat", "2", "2", "0", "0", "mat", "0", "3", "mat", "1", "3", "mat", "2", "3", "1", "0", "numpy", "return", "result"], "docstring": "matrix multiply", "docstring_tokens": ["matrix", "multiply"], "idx": 33}
{"url": "https://github.com/rigetti/grove/blob/dc6bf6ec63e8c435fe52b1e00f707d5ce4cdb9b3/grove/tomography/operator_utils.py#L70-L94", "repo": "grove", "func_name": "make_diagonal_povm", "original_string": ["def make_diagonal_povm(pi_basis, confusion_rate_matrix):\n", "    \"\"\"\n", "    Create a DiagonalPOVM from a ``pi_basis`` and the ``confusion_rate_matrix`` associated with a\n", "    readout.\n", "\n", "    See also the grove documentation.\n", "\n", "    :param OperatorBasis pi_basis: An operator basis of rank-1 projection operators.\n", "    :param numpy.ndarray confusion_rate_matrix: The matrix of detection probabilities conditional\n", "    on a prepared qubit state.\n", "    :return: The POVM corresponding to confusion_rate_matrix.\n", "    :rtype: DiagonalPOVM\n", "    \"\"\"\n", "\n", "    confusion_rate_matrix = np.asarray(confusion_rate_matrix)\n", "    if not np.allclose(confusion_rate_matrix.sum(axis=0), np.ones(confusion_rate_matrix.shape[1])):\n", "        raise CRMUnnormalizedError(\"Unnormalized confusion matrix:\\n{}\".format(\n", "            confusion_rate_matrix))\n", "    if not (confusion_rate_matrix >= 0).all() or not (confusion_rate_matrix <= 1).all():\n", "        raise CRMValueError(\"Confusion matrix must have values in [0, 1]:\"\n", "                            \"\\n{}\".format(confusion_rate_matrix))\n", "\n", "    ops = [sum((pi_j * pjk for (pi_j, pjk) in izip(pi_basis.ops, pjs)), 0)\n", "           for pjs in confusion_rate_matrix]\n", "    return DiagonalPOVM(pi_basis=pi_basis, confusion_rate_matrix=confusion_rate_matrix, ops=ops)\n"], "language": "python", "code": "def make_diagonal_povm(pi_basis, confusion_rate_matrix):\n    \"\"\"\"\"\"\n    confusion_rate_matrix = np.asarray(confusion_rate_matrix)\n    if not np.allclose(confusion_rate_matrix.sum(axis=0), np.ones(\n        confusion_rate_matrix.shape[1])):\n        raise CRMUnnormalizedError('Unnormalized confusion matrix:\\n{}'.\n            format(confusion_rate_matrix))\n    if not (confusion_rate_matrix >= 0).all() or not (confusion_rate_matrix <=\n        1).all():\n        raise CRMValueError('Confusion matrix must have values in [0, 1]:\\n{}'\n            .format(confusion_rate_matrix))\n    ops = [sum((pi_j * pjk for pi_j, pjk in izip(pi_basis.ops, pjs)), 0) for\n        pjs in confusion_rate_matrix]\n    return DiagonalPOVM(pi_basis=pi_basis, confusion_rate_matrix=\n        confusion_rate_matrix, ops=ops)\n", "code_tokens": ["make", "diagonal", "povm", "pi", "basis", "confusion", "rate", "matrix", "confusion", "rate", "matrix", "np", "asarray", "confusion", "rate", "matrix", "if", "not", "np", "allclose", "confusion", "rate", "matrix", "sum", "axis", "0", "np", "ones", "confusion", "rate", "matrix", "shape", "1", "raise", "crmunnormalizederror", "unnormalized", "confusion", "matrix", "format", "confusion", "rate", "matrix", "if", "not", "confusion", "rate", "matrix", "0", "all", "or", "not", "confusion", "rate", "matrix", "1", "all", "raise", "crmvalueerror", "confusion", "matrix", "must", "have", "values", "in", "0", "1", "format", "confusion", "rate", "matrix", "ops", "sum", "pi", "pjk", "for", "pi", "pjk", "in", "izip", "pi", "basis", "ops", "pjs", "0", "for", "pjs", "in", "confusion", "rate", "matrix", "return", "diagonalpovm", "pi", "basis", "pi", "basis", "confusion", "rate", "matrix", "confusion", "rate", "matrix", "ops", "ops"], "docstring": "confusion matrix", "docstring_tokens": ["confusion", "matrix"], "idx": 34}
{"url": "https://github.com/rjrequina/Cebuano-POS-Tagger/blob/fb1b46448c40b23ac8e8e373f073f1f8934b6dc6/eval/evaluator.py#L72-L92", "repo": "Cebuano-POS-Tagger", "func_name": "confusion_matrix", "original_string": ["def confusion_matrix(actual=[], pred=[]):\n", "    idx = {\n", "        'ADJ' : 0,\n", "        'ADV' : 1,\n", "        'CONJ': 2,\n", "        'DET' : 3,\n", "        'NOUN': 4,\n", "        'NUM' : 5,\n", "        'OTH' : 6,\n", "        'PART': 7,\n", "        'PRON': 8,\n", "        'SYM' : 9,\n", "        'VERB': 10\n", "    }\n", "\n", "    matrix = [[0 for i in range(11)] for j in range(11)]\n", "\n", "    for i in range(0, len(actual)):\n", "       matrix[idx[actual[i]]][idx[pred[i]]] += 1\n", "    \n", "    return matrix\n"], "language": "python", "code": "def confusion_matrix(actual=[], pred=[]):\n    idx = {'ADJ': 0, 'ADV': 1, 'CONJ': 2, 'DET': 3, 'NOUN': 4, 'NUM': 5,\n        'OTH': 6, 'PART': 7, 'PRON': 8, 'SYM': 9, 'VERB': 10}\n    matrix = [[(0) for i in range(11)] for j in range(11)]\n    for i in range(0, len(actual)):\n        matrix[idx[actual[i]]][idx[pred[i]]] += 1\n    return matrix\n", "code_tokens": ["confusion", "matrix", "actual", "pred", "idx", "adj", "0", "adv", "1", "conj", "2", "det", "3", "noun", "4", "num", "5", "oth", "6", "part", "7", "pron", "8", "sym", "9", "verb", "10", "matrix", "0", "for", "in", "range", "11", "for", "in", "range", "11", "for", "in", "range", "0", "len", "actual", "matrix", "idx", "actual", "idx", "pred", "1", "return", "matrix"], "docstring": "confusion matrix", "docstring_tokens": ["confusion", "matrix"], "idx": 35}
{"url": "https://github.com/numenta/nupic/blob/5922fafffdccc8812e72b3324965ad2f7d4bbdad/src/nupic/regions/knn_classifier_region.py#L684-L688", "repo": "nupic", "func_name": "_getAccuracy", "original_string": ["  def _getAccuracy(self):\n", "\n", "    n = self.confusion.shape[0]\n", "    assert n == self.confusion.shape[1], \"Confusion matrix is non-square.\"\n", "    return self.confusion[range(n), range(n)].sum(), self.confusion.sum()\n"], "language": "python", "code": "def _getAccuracy(self):\n    n = self.confusion.shape[0]\n    assert n == self.confusion.shape[1], 'Confusion matrix is non-square.'\n    return self.confusion[range(n), range(n)].sum(), self.confusion.sum()\n", "code_tokens": ["getaccuracy", "self", "self", "confusion", "shape", "0", "assert", "self", "confusion", "shape", "1", "confusion", "matrix", "is", "non", "square", "return", "self", "confusion", "range", "range", "sum", "self", "confusion", "sum"], "docstring": "confusion matrix", "docstring_tokens": ["confusion", "matrix"], "idx": 36}
{"url": "https://github.com/a-tal/nagaram/blob/2edcb0ef8cb569ebd1c398be826472b4831d6110/nagaram/scrabble.py#L136-L184", "repo": "nagaram", "func_name": "valid_scrabble_word", "original_string": ["def valid_scrabble_word(word):\n", "    \"\"\"Checks if the input word could be played with a full bag of tiles.\n", "\n", "    Returns:\n", "        True or false\n", "    \"\"\"\n", "\n", "    letters_in_bag = {\n", "        \"a\": 9,\n", "        \"b\": 2,\n", "        \"c\": 2,\n", "        \"d\": 4,\n", "        \"e\": 12,\n", "        \"f\": 2,\n", "        \"g\": 3,\n", "        \"h\": 2,\n", "        \"i\": 9,\n", "        \"j\": 1,\n", "        \"k\": 1,\n", "        \"l\": 4,\n", "        \"m\": 2,\n", "        \"n\": 6,\n", "        \"o\": 8,\n", "        \"p\": 2,\n", "        \"q\": 1,\n", "        \"r\": 6,\n", "        \"s\": 4,\n", "        \"t\": 6,\n", "        \"u\": 4,\n", "        \"v\": 2,\n", "        \"w\": 2,\n", "        \"x\": 1,\n", "        \"y\": 2,\n", "        \"z\": 1,\n", "        \"_\": 2,\n", "    }\n", "\n", "    for letter in word:\n", "        if letter == \"?\":\n", "            continue\n", "        try:\n", "            letters_in_bag[letter] -= 1\n", "        except KeyError:\n", "            return False\n", "        if letters_in_bag[letter] < 0:\n", "            letters_in_bag[\"_\"] -= 1\n", "            if letters_in_bag[\"_\"] < 0:\n", "                return False\n", "    return True\n"], "language": "python", "code": "def valid_scrabble_word(word):\n    \"\"\"\"\"\"\n    letters_in_bag = {'a': 9, 'b': 2, 'c': 2, 'd': 4, 'e': 12, 'f': 2, 'g':\n        3, 'h': 2, 'i': 9, 'j': 1, 'k': 1, 'l': 4, 'm': 2, 'n': 6, 'o': 8,\n        'p': 2, 'q': 1, 'r': 6, 's': 4, 't': 6, 'u': 4, 'v': 2, 'w': 2, 'x':\n        1, 'y': 2, 'z': 1, '_': 2}\n    for letter in word:\n        if letter == '?':\n            continue\n        try:\n            letters_in_bag[letter] -= 1\n        except KeyError:\n            return False\n        if letters_in_bag[letter] < 0:\n            letters_in_bag['_'] -= 1\n            if letters_in_bag['_'] < 0:\n                return False\n    return True\n", "code_tokens": ["valid", "scrabble", "word", "word", "letters", "in", "bag", "9", "2", "2", "4", "12", "2", "3", "2", "9", "1", "1", "4", "2", "6", "8", "2", "1", "6", "4", "6", "4", "2", "2", "1", "2", "1", "2", "for", "letter", "in", "word", "if", "letter", "continue", "try", "letters", "in", "bag", "letter", "1", "except", "keyerror", "return", "false", "if", "letters", "in", "bag", "letter", "0", "letters", "in", "bag", "1", "if", "letters", "in", "bag", "0", "return", "false", "return", "true"], "docstring": "how to determine a string is a valid word", "docstring_tokens": ["how", "to", "determine", "a", "string", "is", "a", "valid", "word"], "idx": 37}
{"url": "https://github.com/phoopy/phoopy-console/blob/d38ec0eb952e79239699a0f855c07437a34024b0/phoopy/console/helper/string_helper.py#L41-L43", "repo": "phoopy-console", "func_name": "get_most_used_words", "original_string": ["    def get_most_used_words(words, stopwords):\n", "        valid_words = [word.lower() for word in words if StringHelper.is_valid_word(word, stopwords)]\n", "        return dict(Counter(valid_words).most_common(5))\n"], "language": "python", "code": "def get_most_used_words(words, stopwords):\n    valid_words = [word.lower() for word in words if StringHelper.\n        is_valid_word(word, stopwords)]\n    return dict(Counter(valid_words).most_common(5))\n", "code_tokens": ["get", "most", "used", "words", "words", "stopwords", "valid", "words", "word", "lower", "for", "word", "in", "words", "if", "stringhelper", "is", "valid", "word", "word", "stopwords", "return", "dict", "counter", "valid", "words", "most", "common", "5"], "docstring": "how to determine a string is a valid word", "docstring_tokens": ["how", "to", "determine", "a", "string", "is", "a", "valid", "word"], "idx": 38}
{"url": "https://github.com/thunlp/THULAC-Python/blob/3f1f126cd92c3d2aebdf4ab4850de3c9428a3b66/thulac/manage/TimeWord.py#L110-L117", "repo": "THULAC-Python", "func_name": "isHttpWord", "original_string": ["    def isHttpWord(self, word):\n", "        if(len(word) < 5):\n", "            return False\n", "        else:\n", "            if(word[0] == ord('h') and word[1] == ord('t') and word[2] == ord('t') and word[3] == ord('p')):\n", "                return True\n", "            else:\n", "                return False\n"], "language": "python", "code": "def isHttpWord(self, word):\n    if len(word) < 5:\n        return False\n    elif word[0] == ord('h') and word[1] == ord('t') and word[2] == ord('t'\n        ) and word[3] == ord('p'):\n        return True\n    else:\n        return False\n", "code_tokens": ["ishttpword", "self", "word", "if", "len", "word", "5", "return", "false", "elif", "word", "0", "ord", "and", "word", "1", "ord", "and", "word", "2", "ord", "and", "word", "3", "ord", "return", "true", "else", "return", "false"], "docstring": "how to determine a string is a valid word", "docstring_tokens": ["how", "to", "determine", "a", "string", "is", "a", "valid", "word"], "idx": 39}
{"url": "https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/noaa_lpd.py#L559-L575", "repo": "LiPD-utilities", "func_name": "__camel_case", "original_string": ["    def __camel_case(word):\n", "        \"\"\"\n", "        Convert underscore naming into camel case naming\n", "        :param str word:\n", "        :return str:\n", "        \"\"\"\n", "        word = word.lower()\n", "        if '_' in word:\n", "            split_word = word.split('_')\n", "        else:\n", "            split_word = word.split()\n", "        if len(split_word) > 0:\n", "            for i, word in enumerate(split_word):\n", "                if i > 0:\n", "                    split_word[i] = word.title()\n", "        strings = ''.join(split_word)\n", "        return strings\n"], "language": "python", "code": "def __camel_case(word):\n    \"\"\"\"\"\"\n    word = word.lower()\n    if '_' in word:\n        split_word = word.split('_')\n    else:\n        split_word = word.split()\n    if len(split_word) > 0:\n        for i, word in enumerate(split_word):\n            if i > 0:\n                split_word[i] = word.title()\n    strings = ''.join(split_word)\n    return strings\n", "code_tokens": ["camel", "case", "word", "word", "word", "lower", "if", "in", "word", "split", "word", "word", "split", "else", "split", "word", "word", "split", "if", "len", "split", "word", "0", "for", "word", "in", "enumerate", "split", "word", "if", "0", "split", "word", "word", "title", "strings", "join", "split", "word", "return", "strings"], "docstring": "how to determine a string is a valid word", "docstring_tokens": ["how", "to", "determine", "a", "string", "is", "a", "valid", "word"], "idx": 40}
{"url": "https://github.com/SpotlightData/preprocessing/blob/180c6472bc2642afbd7a1ece08d0b0d14968a708/preprocessing/spellcheck.py#L18-L28", "repo": "preprocessing", "func_name": "correct_word", "original_string": ["def correct_word(word_string):\n", "    '''\n", "    Finds all valid one and two letter corrections for word_string, returning the word\n", "    with the highest relative probability as type str.\n", "    '''\n", "    if word_string is None:\n", "        return \"\"\n", "    elif isinstance(word_string, str):\n", "        return max(find_candidates(word_string), key=find_word_prob)\n", "    else:\n", "        raise InputError(\"string or none type variable not passed as argument to correct_word\")\n"], "language": "python", "code": "def correct_word(word_string):\n    \"\"\"\"\"\"\n    if word_string is None:\n        return ''\n    elif isinstance(word_string, str):\n        return max(find_candidates(word_string), key=find_word_prob)\n    else:\n        raise InputError(\n            'string or none type variable not passed as argument to correct_word'\n            )\n", "code_tokens": ["correct", "word", "word", "string", "if", "word", "string", "is", "none", "return", "elif", "isinstance", "word", "string", "str", "return", "max", "find", "candidates", "word", "string", "key", "find", "word", "prob", "else", "raise", "inputerror", "string", "or", "none", "type", "variable", "not", "passed", "as", "argument", "to", "correct", "word"], "docstring": "how to determine a string is a valid word", "docstring_tokens": ["how", "to", "determine", "a", "string", "is", "a", "valid", "word"], "idx": 41}
{"url": "https://github.com/SpotlightData/preprocessing/blob/180c6472bc2642afbd7a1ece08d0b0d14968a708/preprocessing/spellcheck.py#L91-L101", "repo": "preprocessing", "func_name": "find_word_prob", "original_string": ["def find_word_prob(word_string, word_total=sum(WORD_DISTRIBUTION.values())):\n", "    '''\n", "    Finds the relative probability of the word appearing given context of a base corpus.\n", "    Returns this probability value as a float instance.\n", "    '''\n", "    if word_string is None:\n", "        return 0\n", "    elif isinstance(word_string, str):\n", "        return WORD_DISTRIBUTION[word_string] / word_total\n", "    else:\n", "        raise InputError(\"string or none type variable not passed as argument to find_word_prob\")\n"], "language": "python", "code": "def find_word_prob(word_string, word_total=sum(WORD_DISTRIBUTION.values())):\n    \"\"\"\"\"\"\n    if word_string is None:\n        return 0\n    elif isinstance(word_string, str):\n        return WORD_DISTRIBUTION[word_string] / word_total\n    else:\n        raise InputError(\n            'string or none type variable not passed as argument to find_word_prob'\n            )\n", "code_tokens": ["find", "word", "prob", "word", "string", "word", "total", "sum", "word", "distribution", "values", "if", "word", "string", "is", "none", "return", "0", "elif", "isinstance", "word", "string", "str", "return", "word", "distribution", "word", "string", "word", "total", "else", "raise", "inputerror", "string", "or", "none", "type", "variable", "not", "passed", "as", "argument", "to", "find", "word", "prob"], "docstring": "how to determine a string is a valid word", "docstring_tokens": ["how", "to", "determine", "a", "string", "is", "a", "valid", "word"], "idx": 42}
{"url": "https://github.com/benedictpaten/sonLib/blob/1decb75bb439b70721ec776f685ce98e25217d26/bioio.py#L636-L641", "repo": "sonLib", "func_name": "padWord", "original_string": ["def padWord(word, length=25):\n", "    if len(word) > length:\n", "        return word[:length]\n", "    if len(word) < length:\n", "        return word + \" \"*(length-len(word))\n", "    return word\n"], "language": "python", "code": "def padWord(word, length=25):\n    if len(word) > length:\n        return word[:length]\n    if len(word) < length:\n        return word + ' ' * (length - len(word))\n    return word\n", "code_tokens": ["padword", "word", "length", "25", "if", "len", "word", "length", "return", "word", "length", "if", "len", "word", "length", "return", "word", "length", "len", "word", "return", "word"], "docstring": "how to determine a string is a valid word", "docstring_tokens": ["how", "to", "determine", "a", "string", "is", "a", "valid", "word"], "idx": 43}
{"url": "https://github.com/dustin/twitty-twister/blob/8524750ee73adb57bbe14ef0cfd8aa08e1e59fb3/twittytwister/twitter.py#L171-L177", "repo": "twitty-twister", "func_name": "_urlencode", "original_string": ["    def _urlencode(self, h):\n", "        rv = []\n", "        for k,v in h.iteritems():\n", "            rv.append('%s=%s' %\n", "                (urllib.quote(k.encode(\"utf-8\")),\n", "                urllib.quote(v.encode(\"utf-8\"))))\n", "        return '&'.join(rv)\n"], "language": "python", "code": "def _urlencode(self, h):\n    rv = []\n    for k, v in h.iteritems():\n        rv.append('%s=%s' % (urllib.quote(k.encode('utf-8')), urllib.quote(\n            v.encode('utf-8'))))\n    return '&'.join(rv)\n", "code_tokens": ["urlencode", "self", "rv", "for", "in", "iteritems", "rv", "append", "urllib", "quote", "encode", "utf", "8", "urllib", "quote", "encode", "utf", "8", "return", "join", "rv"], "docstring": "encode url", "docstring_tokens": ["encode", "url"], "idx": 44}
{"url": "https://github.com/marvin-ai/marvin-python-toolbox/blob/7c95cb2f9698b989150ab94c1285f3a9eaaba423/marvin_python_toolbox/common/utils.py#L286-L295", "repo": "marvin-python-toolbox", "func_name": "url_encode", "original_string": ["def url_encode(url):\n", "    \"\"\"\n", "    Convert special characters using %xx escape.\n", "\n", "    :param url: str\n", "    :return: str - encoded url\n", "    \"\"\"\n", "    if isinstance(url, text_type):\n", "        url = url.encode('utf8')\n", "    return quote(url, ':/%?&=')\n"], "language": "python", "code": "def url_encode(url):\n    \"\"\"\"\"\"\n    if isinstance(url, text_type):\n        url = url.encode('utf8')\n    return quote(url, ':/%?&=')\n", "code_tokens": ["url", "encode", "url", "if", "isinstance", "url", "text", "type", "url", "url", "encode", "return", "quote", "url"], "docstring": "encode url", "docstring_tokens": ["encode", "url"], "idx": 45}
{"url": "https://github.com/yunpian/yunpian-python-sdk/blob/405a1196ec83fdf29ff454f74ef036974be11970/yunpian_python_sdk/ypclient.py#L195-L208", "repo": "yunpian-python-sdk", "func_name": "urlEncodeAndJoin", "original_string": ["    def urlEncodeAndJoin(self, seq, sepr=','):\n", "        '''sepr.join(urlencode(seq))\n", "        Args:\n", "            seq: string list to be urlencoded\n", "            sepr: join seq with sepr\n", "        Returns:\n", "            str\n", "        '''\n", "        try:\n", "            from urllib.parse import quote_plus as encode\n", "            return sepr.join([encode(x, encoding=CHARSET_UTF8) for x in seq])\n", "        except ImportError:\n", "            from urllib import quote as encode\n", "            return sepr.join([i for i in map(lambda x: encode(x), seq)])\n"], "language": "python", "code": "def urlEncodeAndJoin(self, seq, sepr=','):\n    \"\"\"\"\"\"\n    try:\n        from urllib.parse import quote_plus as encode\n        return sepr.join([encode(x, encoding=CHARSET_UTF8) for x in seq])\n    except ImportError:\n        from urllib import quote as encode\n        return sepr.join([i for i in map(lambda x: encode(x), seq)])\n", "code_tokens": ["urlencodeandjoin", "self", "seq", "sepr", "try", "from", "urllib", "parse", "import", "quote", "plus", "as", "encode", "return", "sepr", "join", "encode", "encoding", "charset", "for", "in", "seq", "except", "importerror", "from", "urllib", "import", "quote", "as", "encode", "return", "sepr", "join", "for", "in", "map", "lambda", "encode", "seq"], "docstring": "encode url", "docstring_tokens": ["encode", "url"], "idx": 46}
{"url": "https://github.com/fred49/linshare-api/blob/be646c25aa8ba3718abb6869c620b157d53d6e41/linshareapi/admin/upgradetasks.py#L177-L191", "repo": "linshare-api", "func_name": "trigger", "original_string": ["    def trigger(self, identifier, force=True):\n", "        \"\"\"Trigger an upgrade task.\"\"\"\n", "        self.debug(identifier)\n", "        url = \"{base}/{identifier}\".format(\n", "            base=self.local_base_url,\n", "            identifier=identifier\n", "        )\n", "        param = {}\n", "        if force:\n", "            param['force'] = force\n", "        encode = urllib.parse.urlencode(param)\n", "        if encode:\n", "            url += \"?\"\n", "            url += encode\n", "        return self.core.update(url, {})\n"], "language": "python", "code": "def trigger(self, identifier, force=True):\n    \"\"\"\"\"\"\n    self.debug(identifier)\n    url = '{base}/{identifier}'.format(base=self.local_base_url, identifier\n        =identifier)\n    param = {}\n    if force:\n        param['force'] = force\n    encode = urllib.parse.urlencode(param)\n    if encode:\n        url += '?'\n        url += encode\n    return self.core.update(url, {})\n", "code_tokens": ["trigger", "self", "identifier", "force", "true", "self", "debug", "identifier", "url", "base", "identifier", "format", "base", "self", "local", "base", "url", "identifier", "identifier", "param", "if", "force", "param", "force", "force", "encode", "urllib", "parse", "urlencode", "param", "if", "encode", "url", "url", "encode", "return", "self", "core", "update", "url"], "docstring": "encode url", "docstring_tokens": ["encode", "url"], "idx": 47}
{"url": "https://github.com/fred49/linshare-api/blob/be646c25aa8ba3718abb6869c620b157d53d6e41/linshareapi/admin/jwt.py#L52-L64", "repo": "linshare-api", "func_name": "_list", "original_string": ["    def _list(self, domain=None):\n", "        # pylint: disable=arguments-differ\n", "        url = \"{base}\".format(\n", "            base=self.local_base_url\n", "        )\n", "        param = {}\n", "        if domain:\n", "            param['domainUuid'] = domain\n", "        encode = urllib.parse.urlencode(param)\n", "        if encode:\n", "            url += \"?\"\n", "            url += encode\n", "        return self.core.list(url)\n"], "language": "python", "code": "def _list(self, domain=None):\n    url = '{base}'.format(base=self.local_base_url)\n    param = {}\n    if domain:\n        param['domainUuid'] = domain\n    encode = urllib.parse.urlencode(param)\n    if encode:\n        url += '?'\n        url += encode\n    return self.core.list(url)\n", "code_tokens": ["list", "self", "domain", "none", "url", "base", "format", "base", "self", "local", "base", "url", "param", "if", "domain", "param", "domainuuid", "domain", "encode", "urllib", "parse", "urlencode", "param", "if", "encode", "url", "url", "encode", "return", "self", "core", "list", "url"], "docstring": "encode url", "docstring_tokens": ["encode", "url"], "idx": 48}
{"url": "https://github.com/fadhiilrachman/line-py/blob/b7f5f2b3fc09fa3fbf6088d7ebdaf9e44d96ba69/linepy/server.py#L18-L19", "repo": "line-py", "func_name": "urlEncode", "original_string": ["    def urlEncode(self, url, path, params=[]):\n", "        return url + path + '?' + urllib.parse.urlencode(params)\n"], "language": "python", "code": "def urlEncode(self, url, path, params=[]):\n    return url + path + '?' + urllib.parse.urlencode(params)\n", "code_tokens": ["urlencode", "self", "url", "path", "params", "return", "url", "path", "urllib", "parse", "urlencode", "params"], "docstring": "encode url", "docstring_tokens": ["encode", "url"], "idx": 49}
{"url": "https://github.com/defensio/defensio-python/blob/c1d2b64be941acb63c452a6d9a5526c59cb37007/defensio/__init__.py#L121-L125", "repo": "defensio-python", "func_name": "_urlencode", "original_string": ["  def _urlencode(self, url):\n", "      if is_python3():\n", "        return urllib.parse.urlencode(url)\n", "      else:\n", "        return urllib.urlencode(url)\n"], "language": "python", "code": "def _urlencode(self, url):\n    if is_python3():\n        return urllib.parse.urlencode(url)\n    else:\n        return urllib.urlencode(url)\n", "code_tokens": ["urlencode", "self", "url", "if", "is", "return", "urllib", "parse", "urlencode", "url", "else", "return", "urllib", "urlencode", "url"], "docstring": "encode url", "docstring_tokens": ["encode", "url"], "idx": 50}
{"url": "https://github.com/djf604/chunky-pipes/blob/cd5b2a31ded28ab949da6190854228f1c8897882/chunkypipes/util/decorators/__init__.py#L6-L14", "repo": "chunky-pipes", "func_name": "timed", "original_string": ["def timed(func):\n", "    @functools.wraps(func)\n", "    def timer(*args, **kwargs):\n", "        start_time = time()\n", "        result = func(*args, **kwargs)\n", "        elapsed_time = str(timedelta(seconds=int(time() - start_time)))\n", "        print('Elapsed time is {}'.format(elapsed_time))\n", "        return result\n", "    return timer\n"], "language": "python", "code": "def timed(func):\n\n    @functools.wraps(func)\n    def timer(*args, **kwargs):\n        start_time = time()\n        result = func(*args, **kwargs)\n        elapsed_time = str(timedelta(seconds=int(time() - start_time)))\n        print('Elapsed time is {}'.format(elapsed_time))\n        return result\n    return timer\n", "code_tokens": ["timed", "func", "functools", "wraps", "func", "def", "timer", "args", "kwargs", "start", "time", "time", "result", "func", "args", "kwargs", "elapsed", "time", "str", "timedelta", "seconds", "int", "time", "start", "time", "print", "elapsed", "time", "is", "format", "elapsed", "time", "return", "result", "return", "timer"], "docstring": "finding time elapsed using a timer", "docstring_tokens": ["finding", "time", "elapsed", "using", "a", "timer"], "idx": 51}
{"url": "https://github.com/fermiPy/fermipy/blob/9df5e7e3728307fd58c5bba36fd86783c39fbad4/fermipy/timing.py#L14-L21", "repo": "fermipy", "func_name": "elapsed_time", "original_string": ["    def elapsed_time(self):\n", "        \"\"\"Get the elapsed time.\"\"\"\n", "\n", "        # Timer is running\n", "        if self._t0 is not None:\n", "            return self._time + self._get_time()\n", "        else:\n", "            return self._time\n"], "language": "python", "code": "def elapsed_time(self):\n    \"\"\"\"\"\"\n    if self._t0 is not None:\n        return self._time + self._get_time()\n    else:\n        return self._time\n", "code_tokens": ["elapsed", "time", "self", "if", "self", "is", "not", "none", "return", "self", "time", "self", "get", "time", "else", "return", "self", "time"], "docstring": "finding time elapsed using a timer", "docstring_tokens": ["finding", "time", "elapsed", "using", "a", "timer"], "idx": 52}
{"url": "https://github.com/AFriemann/simple_tools/blob/27d0f838c23309ebfc8afb59511220c6f8bb42fe/simple_tools/contextmanagers/time.py#L16-L20", "repo": "simple_tools", "func_name": "timer", "original_string": ["def timer(name):\n", "    startTime = time.time()\n", "    yield\n", "    elapsedTime = time.time() - startTime\n", "    print('[{}] finished in {} ms'.format(name, int(elapsedTime * 1000)))\n"], "language": "python", "code": "def timer(name):\n    startTime = time.time()\n    yield\n    elapsedTime = time.time() - startTime\n    print('[{}] finished in {} ms'.format(name, int(elapsedTime * 1000)))\n", "code_tokens": ["timer", "name", "starttime", "time", "time", "yield", "elapsedtime", "time", "time", "starttime", "print", "finished", "in", "ms", "format", "name", "int", "elapsedtime", "1000"], "docstring": "finding time elapsed using a timer", "docstring_tokens": ["finding", "time", "elapsed", "using", "a", "timer"], "idx": 53}
{"url": "https://github.com/alexmojaki/littleutils/blob/1132d2d2782b05741a907d1281cd8c001f1d1d9d/littleutils/__init__.py#L716-L739", "repo": "littleutils", "func_name": "keys", "original_string": ["\n", "    def keys(self):\n", "        return dir(self.x)\n", "\n", "\n", "@contextmanager\n", "def timer(description='Operation', log=None):\n", "    \"\"\"\n", "    Simple context manager which logs (if log is provided)\n", "    or prints the time taken in seconds for the block to complete.\n", "\n", "    >>> with timer():\n", "    ...    sleep(0.1)               # doctest:+ELLIPSIS\n", "    Operation took 0.1... seconds\n", "\n", "    >>> with timer('Sleeping'):\n", "    ...    sleep(0.2)               # doctest:+ELLIPSIS\n", "    Sleeping took 0.2... seconds\n", "\n", "    >>> with timer(description='Doing', log=PrintingLogger()):\n", "    ...    sleep(0.3)               # doctest:+ELLIPSIS\n", "    Doing took 0.3... seconds\n", "\n", "    \"\"\"\n"], "language": "python", "code": "def keys(self):\n    return dir(self.x)\n", "code_tokens": ["keys", "self", "return", "dir", "self"], "docstring": "finding time elapsed using a timer", "docstring_tokens": ["finding", "time", "elapsed", "using", "a", "timer"], "idx": 54}
{"url": "https://github.com/andrewramsay/sk8-drivers/blob/67347a71762fb421f5ae65a595def5c7879e8b0c/pysk8/calibration/sk8_calibration_gui.py#L49-L57", "repo": "sk8-drivers", "func_name": "update", "original_string": ["    def update(self):\n", "        # on every timer tick, record a gyro sample and exit if required time has elapsed\n", "        elapsed = int(100 * ((time.time() - self.started_at) / self.GYRO_BIAS_TIME))\n", "        self.progressBar.setValue(elapsed)\n", "        self.samples.append(self.imu.gyro)\n", "\n", "        if time.time() - self.started_at > self.GYRO_BIAS_TIME:\n", "            self.accept()\n", "            QtWidgets.QMessageBox.information(self, 'Gyro calibration', 'Calibration finished')\n"], "language": "python", "code": "def update(self):\n    elapsed = int(100 * ((time.time() - self.started_at) / self.GYRO_BIAS_TIME)\n        )\n    self.progressBar.setValue(elapsed)\n    self.samples.append(self.imu.gyro)\n    if time.time() - self.started_at > self.GYRO_BIAS_TIME:\n        self.accept()\n        QtWidgets.QMessageBox.information(self, 'Gyro calibration',\n            'Calibration finished')\n", "code_tokens": ["update", "self", "elapsed", "int", "100", "time", "time", "self", "started", "at", "self", "gyro", "bias", "time", "self", "progressbar", "setvalue", "elapsed", "self", "samples", "append", "self", "imu", "gyro", "if", "time", "time", "self", "started", "at", "self", "gyro", "bias", "time", "self", "accept", "qtwidgets", "qmessagebox", "information", "self", "gyro", "calibration", "calibration", "finished"], "docstring": "finding time elapsed using a timer", "docstring_tokens": ["finding", "time", "elapsed", "using", "a", "timer"], "idx": 55}
{"url": "https://github.com/buildbot/buildbot/blob/5df3cfae6d760557d99156633c32b1822a1e130c/master/buildbot/process/metrics.py#L74-L76", "repo": "buildbot", "func_name": "__init__", "original_string": ["    def __init__(self, timer, elapsed):\n", "        self.timer = timer\n", "        self.elapsed = elapsed\n"], "language": "python", "code": "def __init__(self, timer, elapsed):\n    self.timer = timer\n    self.elapsed = elapsed\n", "code_tokens": ["init", "self", "timer", "elapsed", "self", "timer", "timer", "self", "elapsed", "elapsed"], "docstring": "finding time elapsed using a timer", "docstring_tokens": ["finding", "time", "elapsed", "using", "a", "timer"], "idx": 56}
{"url": "https://github.com/keybase/python-triplesec/blob/0a73e18cfe542d0cd5ee57bd823a67412b4b717e/triplesec/crypto.py#L86-L92", "repo": "python-triplesec", "func_name": "encrypt", "original_string": ["    def encrypt(cls, data, key, iv_data):\n", "        validate_key_size(key, cls.key_size, \"AES\")\n", "\n", "        iv, ctr = iv_data\n", "        ciphertext = Crypto_AES.new(key, Crypto_AES.MODE_CTR,\n", "                                    counter=ctr).encrypt(data)\n", "        return iv + ciphertext\n"], "language": "python", "code": "def encrypt(cls, data, key, iv_data):\n    validate_key_size(key, cls.key_size, 'AES')\n    iv, ctr = iv_data\n    ciphertext = Crypto_AES.new(key, Crypto_AES.MODE_CTR, counter=ctr).encrypt(\n        data)\n    return iv + ciphertext\n", "code_tokens": ["encrypt", "cls", "data", "key", "iv", "data", "validate", "key", "size", "key", "cls", "key", "size", "aes", "iv", "ctr", "iv", "data", "ciphertext", "crypto", "aes", "new", "key", "crypto", "aes", "mode", "ctr", "counter", "ctr", "encrypt", "data", "return", "iv", "ciphertext"], "docstring": "encrypt aes ctr mode", "docstring_tokens": ["encrypt", "aes", "ctr", "mode"], "idx": 57}
{"url": "https://github.com/jcassee/django-geckoboard/blob/6ebdaa86015fe645360abf1ba1290132de4cf6d6/django_geckoboard/decorators.py#L446-L460", "repo": "django-geckoboard", "func_name": "_encrypt", "original_string": ["def _encrypt(data):\n", "    \"\"\"Equivalent to OpenSSL using 256 bit AES in CBC mode\"\"\"\n", "    BS = AES.block_size\n", "\n", "    def pad(s):\n", "        n = BS - len(s) % BS\n", "        char = chr(n).encode('utf8')\n", "        return s + n * char\n", "\n", "    password = settings.GECKOBOARD_PASSWORD\n", "    salt = Random.new().read(BS - len('Salted__'))\n", "    key, iv = _derive_key_and_iv(password, salt, 32, BS)\n", "    cipher = AES.new(key, AES.MODE_CBC, iv)\n", "    encrypted = b'Salted__' + salt + cipher.encrypt(pad(data))\n", "    return base64.b64encode(encrypted)\n"], "language": "python", "code": "def _encrypt(data):\n    \"\"\"\"\"\"\n    BS = AES.block_size\n\n    def pad(s):\n        n = BS - len(s) % BS\n        char = chr(n).encode('utf8')\n        return s + n * char\n    password = settings.GECKOBOARD_PASSWORD\n    salt = Random.new().read(BS - len('Salted__'))\n    key, iv = _derive_key_and_iv(password, salt, 32, BS)\n    cipher = AES.new(key, AES.MODE_CBC, iv)\n    encrypted = b'Salted__' + salt + cipher.encrypt(pad(data))\n    return base64.b64encode(encrypted)\n", "code_tokens": ["encrypt", "data", "bs", "aes", "block", "size", "def", "pad", "bs", "len", "bs", "char", "chr", "encode", "return", "char", "password", "settings", "geckoboard", "password", "salt", "random", "new", "read", "bs", "len", "salted", "key", "iv", "derive", "key", "and", "iv", "password", "salt", "32", "bs", "cipher", "aes", "new", "key", "aes", "mode", "cbc", "iv", "encrypted", "salted", "salt", "cipher", "encrypt", "pad", "data", "return", "encrypted"], "docstring": "encrypt aes ctr mode", "docstring_tokens": ["encrypt", "aes", "ctr", "mode"], "idx": 58}
{"url": "https://github.com/boldfield/s3-encryption/blob/d88549ba682745dc6b199934c5b5221de7f8d8bc/s3_encryption/crypto.py#L52-L57", "repo": "s3-encryption", "func_name": "aes_encrypt", "original_string": ["def aes_encrypt(key, data, mode='ECB', iv=None):\n", "    aes = AES()\n", "    aes.mode = mode\n", "    aes.iv = iv\n", "    aes.key = key\n", "    return aes.encrypt(data)\n"], "language": "python", "code": "def aes_encrypt(key, data, mode='ECB', iv=None):\n    aes = AES()\n    aes.mode = mode\n    aes.iv = iv\n    aes.key = key\n    return aes.encrypt(data)\n", "code_tokens": ["aes", "encrypt", "key", "data", "mode", "ecb", "iv", "none", "aes", "aes", "aes", "mode", "mode", "aes", "iv", "iv", "aes", "key", "key", "return", "aes", "encrypt", "data"], "docstring": "encrypt aes ctr mode", "docstring_tokens": ["encrypt", "aes", "ctr", "mode"], "idx": 59}
{"url": "https://github.com/ghackebeil/PyORAM/blob/b8832c1b753c0b2148ef7a143c5f5dd3bbbb61e7/src/pyoram/crypto/aes.py#L24-L27", "repo": "PyORAM", "func_name": "CTREnc", "original_string": ["    def CTREnc(key, plaintext):\n", "        iv = os.urandom(AES.block_size)\n", "        cipher = _cipher(_aes(key), _ctrmode(iv), backend=_backend).encryptor()\n", "        return iv + cipher.update(plaintext) + cipher.finalize()\n"], "language": "python", "code": "def CTREnc(key, plaintext):\n    iv = os.urandom(AES.block_size)\n    cipher = _cipher(_aes(key), _ctrmode(iv), backend=_backend).encryptor()\n    return iv + cipher.update(plaintext) + cipher.finalize()\n", "code_tokens": ["ctrenc", "key", "plaintext", "iv", "os", "urandom", "aes", "block", "size", "cipher", "cipher", "aes", "key", "ctrmode", "iv", "backend", "backend", "encryptor", "return", "iv", "cipher", "update", "plaintext", "cipher", "finalize"], "docstring": "encrypt aes ctr mode", "docstring_tokens": ["encrypt", "aes", "ctr", "mode"], "idx": 60}
{"url": "https://github.com/StorjOld/heartbeat/blob/4d54f2011f1e9f688073d4347bc51bb7bd682718/heartbeat/PySwizzle/PySwizzle.py#L162-L178", "repo": "heartbeat", "func_name": "encrypt", "original_string": ["    def encrypt(self, key):\n", "        \"\"\"This method encrypts and signs the state to make it unreadable by\n", "        the server, since it contains information that would allow faking\n", "        proof of storage.\n", "\n", "        :param key: the key to encrypt and sign with\n", "        \"\"\"\n", "        if (self.encrypted):\n", "            return\n", "        # encrypt\n", "        self.iv = Random.new().read(AES.block_size)\n", "        aes = AES.new(key, AES.MODE_CFB, self.iv)\n", "        self.f_key = aes.encrypt(self.f_key)\n", "        self.alpha_key = aes.encrypt(self.alpha_key)\n", "        self.encrypted = True\n", "        # sign\n", "        self.hmac = self.get_hmac(key)\n"], "language": "python", "code": "def encrypt(self, key):\n    \"\"\"\"\"\"\n    if self.encrypted:\n        return\n    self.iv = Random.new().read(AES.block_size)\n    aes = AES.new(key, AES.MODE_CFB, self.iv)\n    self.f_key = aes.encrypt(self.f_key)\n    self.alpha_key = aes.encrypt(self.alpha_key)\n    self.encrypted = True\n    self.hmac = self.get_hmac(key)\n", "code_tokens": ["encrypt", "self", "key", "if", "self", "encrypted", "return", "self", "iv", "random", "new", "read", "aes", "block", "size", "aes", "aes", "new", "key", "aes", "mode", "cfb", "self", "iv", "self", "key", "aes", "encrypt", "self", "key", "self", "alpha", "key", "aes", "encrypt", "self", "alpha", "key", "self", "encrypted", "true", "self", "hmac", "self", "get", "hmac", "key"], "docstring": "encrypt aes ctr mode", "docstring_tokens": ["encrypt", "aes", "ctr", "mode"], "idx": 61}
{"url": "https://github.com/teitei-tk/Simple-AES-Cipher/blob/cb4bc69d762397f3a26f6b009e2b7fa5d706ed5f/simple_aes_cipher/cipher.py#L15-L19", "repo": "Simple-AES-Cipher", "func_name": "encrypt", "original_string": ["    def encrypt(self, raw, mode=AES.MODE_CBC):\n", "        raw = self._pad(raw, AES.block_size)\n", "        iv = Random.new().read(AES.block_size)\n", "        cipher = AES.new(self.key, mode, iv)\n", "        return base64.b64encode(iv + cipher.encrypt(raw)).decode('utf-8')\n"], "language": "python", "code": "def encrypt(self, raw, mode=AES.MODE_CBC):\n    raw = self._pad(raw, AES.block_size)\n    iv = Random.new().read(AES.block_size)\n    cipher = AES.new(self.key, mode, iv)\n    return base64.b64encode(iv + cipher.encrypt(raw)).decode('utf-8')\n", "code_tokens": ["encrypt", "self", "raw", "mode", "aes", "mode", "cbc", "raw", "self", "pad", "raw", "aes", "block", "size", "iv", "random", "new", "read", "aes", "block", "size", "cipher", "aes", "new", "self", "key", "mode", "iv", "return", "iv", "cipher", "encrypt", "raw", "decode", "utf", "8"], "docstring": "encrypt aes ctr mode", "docstring_tokens": ["encrypt", "aes", "ctr", "mode"], "idx": 62}
{"url": "https://github.com/ghackebeil/PyORAM/blob/b8832c1b753c0b2148ef7a143c5f5dd3bbbb61e7/examples/aesctr_performance.py#L113-L115", "repo": "PyORAM", "func_name": "main", "original_string": ["def main():\n", "    runtest(\"AES - CTR Mode\", AES.CTREnc, AES.CTRDec)\n", "    runtest(\"AES - GCM Mode\", AES.GCMEnc, AES.GCMDec)\n"], "language": "python", "code": "def main():\n    runtest('AES - CTR Mode', AES.CTREnc, AES.CTRDec)\n    runtest('AES - GCM Mode', AES.GCMEnc, AES.GCMDec)\n", "code_tokens": ["main", "runtest", "aes", "ctr", "mode", "aes", "ctrenc", "aes", "ctrdec", "runtest", "aes", "gcm", "mode", "aes", "gcmenc", "aes", "gcmdec"], "docstring": "encrypt aes ctr mode", "docstring_tokens": ["encrypt", "aes", "ctr", "mode"], "idx": 63}
{"url": "https://github.com/boldfield/s3-encryption/blob/d88549ba682745dc6b199934c5b5221de7f8d8bc/s3_encryption/crypto.py#L20-L25", "repo": "s3-encryption", "func_name": "encrypt", "original_string": ["    def encrypt(self, data):\n", "        if self.iv is None:\n", "            cipher = pyAES.new(self.key, self.mode)\n", "        else:\n", "            cipher = pyAES.new(self.key, self.mode, self.iv)\n", "        return cipher.encrypt(pad_data(AES.str_to_bytes(data)))\n"], "language": "python", "code": "def encrypt(self, data):\n    if self.iv is None:\n        cipher = pyAES.new(self.key, self.mode)\n    else:\n        cipher = pyAES.new(self.key, self.mode, self.iv)\n    return cipher.encrypt(pad_data(AES.str_to_bytes(data)))\n", "code_tokens": ["encrypt", "self", "data", "if", "self", "iv", "is", "none", "cipher", "pyaes", "new", "self", "key", "self", "mode", "else", "cipher", "pyaes", "new", "self", "key", "self", "mode", "self", "iv", "return", "cipher", "encrypt", "pad", "data", "aes", "str", "to", "bytes", "data"], "docstring": "encrypt aes ctr mode", "docstring_tokens": ["encrypt", "aes", "ctr", "mode"], "idx": 64}
{"url": "https://github.com/wtsi-hgi/python-hgijson/blob/6e8ccb562eabcaa816a136268a16504c2e0d4664/hgijson/json_converters/_converters.py#L61-L69", "repo": "python-hgijson", "func_name": "deserialize", "original_string": ["    def deserialize(self, deserializable: PrimitiveJsonType) -> Optional[SerializableType]:\n", "        if not isinstance(self._decoder, ParsedJSONDecoder):\n", "            # Decode must take a string (even though we have a richer representation) :/\n", "            json_as_string = json.dumps(deserializable)\n", "            return self._decoder.decode(json_as_string)\n", "        else:\n", "            # Optimisation - no need to convert our relatively rich representation into a string (just to turn it back\n", "            # again!)\n", "            return self._decoder.decode_parsed(deserializable)\n"], "language": "python", "code": "def deserialize(self, deserializable: PrimitiveJsonType) ->Optional[\n    SerializableType]:\n    if not isinstance(self._decoder, ParsedJSONDecoder):\n        json_as_string = json.dumps(deserializable)\n        return self._decoder.decode(json_as_string)\n    else:\n        return self._decoder.decode_parsed(deserializable)\n", "code_tokens": ["deserialize", "self", "deserializable", "primitivejsontype", "optional", "serializabletype", "if", "not", "isinstance", "self", "decoder", "parsedjsondecoder", "json", "as", "string", "json", "dumps", "deserializable", "return", "self", "decoder", "decode", "json", "as", "string", "else", "return", "self", "decoder", "decode", "parsed", "deserializable"], "docstring": "deserialize json", "docstring_tokens": ["deserialize", "json"], "idx": 65}
{"url": "https://github.com/F483/apigen/blob/f05ce1509030764721cc3393410fa12b609e88f2/apigen/apigen.py#L234-L244", "repo": "apigen", "func_name": "_deserialize", "original_string": ["def _deserialize(kwargs):\n", "    def deserialize(item):\n", "        if isinstance(item[1], str):\n", "            try:\n", "                data = json.loads(item[1])  # load as json\n", "            except:\n", "                data = item[1].decode('utf-8')  # must be a string\n", "        else:\n", "            data = item[1]  # already deserialized (method default value)\n", "        return (item[0], data)\n", "    return dict(map(deserialize, kwargs.items()))\n"], "language": "python", "code": "def _deserialize(kwargs):\n\n    def deserialize(item):\n        if isinstance(item[1], str):\n            try:\n                data = json.loads(item[1])\n            except:\n                data = item[1].decode('utf-8')\n        else:\n            data = item[1]\n        return item[0], data\n    return dict(map(deserialize, kwargs.items()))\n", "code_tokens": ["deserialize", "kwargs", "def", "deserialize", "item", "if", "isinstance", "item", "1", "str", "try", "data", "json", "loads", "item", "1", "except", "data", "item", "1", "decode", "utf", "8", "else", "data", "item", "1", "return", "item", "0", "data", "return", "dict", "map", "deserialize", "kwargs", "items"], "docstring": "deserialize json", "docstring_tokens": ["deserialize", "json"], "idx": 66}
{"url": "https://github.com/sassoo/goldman/blob/b72540c9ad06b5c68aadb1b4fa8cb0b716260bf2/goldman/deserializers/json_7159.py#L22-L47", "repo": "goldman", "func_name": "deserialize", "original_string": ["    def deserialize(self):\n", "        \"\"\" Invoke the RFC 7159 spec compliant parser\n", "\n", "        :return:\n", "            the parsed & vetted request body\n", "        \"\"\"\n", "\n", "        super(Deserializer, self).deserialize()\n", "\n", "        try:\n", "            return json.loads(self.req.get_body())\n", "        except TypeError:\n", "            link = 'tools.ietf.org/html/rfc7159'\n", "            self.fail('Typically, this error is due to a missing JSON '\n", "                      'payload in your request when one was required. '\n", "                      'Otherwise, it could be a bug in our API.', link)\n", "        except UnicodeDecodeError:\n", "            link = 'tools.ietf.org/html/rfc7159#section-8.1'\n", "            self.fail('We failed to process your JSON payload & it is '\n", "                      'most likely due to non UTF-8 encoded characters '\n", "                      'in your JSON.', link)\n", "        except ValueError as exc:\n", "            link = 'tools.ietf.org/html/rfc7159'\n", "            self.fail('The JSON payload appears to be malformed & we '\n", "                      'failed to process it. The error with line & column '\n", "                      'numbers is: %s' % exc.message, link)\n"], "language": "python", "code": "def deserialize(self):\n    \"\"\"\"\"\"\n    super(Deserializer, self).deserialize()\n    try:\n        return json.loads(self.req.get_body())\n    except TypeError:\n        link = 'tools.ietf.org/html/rfc7159'\n        self.fail(\n            'Typically, this error is due to a missing JSON payload in your request when one was required. Otherwise, it could be a bug in our API.'\n            , link)\n    except UnicodeDecodeError:\n        link = 'tools.ietf.org/html/rfc7159#section-8.1'\n        self.fail(\n            'We failed to process your JSON payload & it is most likely due to non UTF-8 encoded characters in your JSON.'\n            , link)\n    except ValueError as exc:\n        link = 'tools.ietf.org/html/rfc7159'\n        self.fail(\n            'The JSON payload appears to be malformed & we failed to process it. The error with line & column numbers is: %s'\n             % exc.message, link)\n", "code_tokens": ["deserialize", "self", "super", "deserializer", "self", "deserialize", "try", "return", "json", "loads", "self", "req", "get", "body", "except", "typeerror", "link", "tools", "ietf", "org", "html", "self", "fail", "typically", "this", "error", "is", "due", "to", "missing", "json", "payload", "in", "your", "request", "when", "one", "was", "required", "otherwise", "it", "could", "be", "bug", "in", "our", "api", "link", "except", "unicodedecodeerror", "link", "tools", "ietf", "org", "html", "section", "8", "1", "self", "fail", "we", "failed", "to", "process", "your", "json", "payload", "it", "is", "most", "likely", "due", "to", "non", "utf", "8", "encoded", "characters", "in", "your", "json", "link", "except", "valueerror", "as", "exc", "link", "tools", "ietf", "org", "html", "self", "fail", "the", "json", "payload", "appears", "to", "be", "malformed", "we", "failed", "to", "process", "it", "the", "error", "with", "line", "column", "numbers", "is", "exc", "message", "link"], "docstring": "deserialize json", "docstring_tokens": ["deserialize", "json"], "idx": 67}
{"url": "https://github.com/ubc/github2gitlab/blob/795898f6d438621fa0c996a7156d70c382ff0493/github2gitlab/main.py#L434-L440", "repo": "github2gitlab", "func_name": "json_loads", "original_string": ["    def json_loads(payload):\n", "        \"Log the payload that cannot be parsed\"\n", "        try:\n", "            return json.loads(payload)\n", "        except ValueError as e:\n", "            log.error(\"unable to json.loads(\" + payload + \")\")\n", "            raise e\n"], "language": "python", "code": "def json_loads(payload):\n    \"\"\"\"\"\"\n    try:\n        return json.loads(payload)\n    except ValueError as e:\n        log.error('unable to json.loads(' + payload + ')')\n        raise e\n", "code_tokens": ["json", "loads", "payload", "try", "return", "json", "loads", "payload", "except", "valueerror", "as", "log", "error", "unable", "to", "json", "loads", "payload", "raise"], "docstring": "deserialize json", "docstring_tokens": ["deserialize", "json"], "idx": 68}
{"url": "https://github.com/steenzout/python-serialization-json/blob/583568e14cc02ba0bf711f56b8a0a3ad142c696d/steenzout/serialization/json/__init__.py#L50-L71", "repo": "python-serialization-json", "func_name": "deserialize", "original_string": ["def deserialize(json, cls=None):\n", "    \"\"\"Deserialize a JSON string into a Python object.\n", "\n", "    Args:\n", "        json (str): the JSON string.\n", "        cls (:py:class:`object`):\n", "            if the ``json`` is deserialized into a ``dict`` and\n", "            this argument is set,\n", "            the ``dict`` keys are passed as keyword arguments to the\n", "            given ``cls`` initializer.\n", "\n", "    Returns:\n", "        Python object representation of the given JSON string.\n", "    \"\"\"\n", "    LOGGER.debug('deserialize(%s)', json)\n", "\n", "    out = simplejson.loads(json)\n", "\n", "    if isinstance(out, dict) and cls is not None:\n", "        return cls(**out)\n", "\n", "    return out\n"], "language": "python", "code": "def deserialize(json, cls=None):\n    \"\"\"\"\"\"\n    LOGGER.debug('deserialize(%s)', json)\n    out = simplejson.loads(json)\n    if isinstance(out, dict) and cls is not None:\n        return cls(**out)\n    return out\n", "code_tokens": ["deserialize", "json", "cls", "none", "logger", "debug", "deserialize", "json", "out", "simplejson", "loads", "json", "if", "isinstance", "out", "dict", "and", "cls", "is", "not", "none", "return", "cls", "out", "return", "out"], "docstring": "deserialize json", "docstring_tokens": ["deserialize", "json"], "idx": 69}
{"url": "https://github.com/open511/open511/blob/3d573f59d7efa06ff1b5419ea5ff4d90a90b3cf8/open511/utils/serialization.py#L47-L60", "repo": "open511", "func_name": "deserialize", "original_string": ["def deserialize(s):\n", "    s = s.strip()\n", "    try:\n", "        doc = etree.fromstring(s)\n", "        if is_tmdd(doc):\n", "            # Transparently convert the TMDD on deserialize\n", "            from ..converter.tmdd import tmdd_to_json\n", "            return (tmdd_to_json(doc), 'json')\n", "        return (doc, 'xml')\n", "    except etree.XMLSyntaxError:\n", "        try:\n", "            return (json.loads(s), 'json')\n", "        except ValueError:\n", "            raise Exception(\"Doesn't look like either JSON or XML\")\n"], "language": "python", "code": "def deserialize(s):\n    s = s.strip()\n    try:\n        doc = etree.fromstring(s)\n        if is_tmdd(doc):\n            from ..converter.tmdd import tmdd_to_json\n            return tmdd_to_json(doc), 'json'\n        return doc, 'xml'\n    except etree.XMLSyntaxError:\n        try:\n            return json.loads(s), 'json'\n        except ValueError:\n            raise Exception(\"Doesn't look like either JSON or XML\")\n", "code_tokens": ["deserialize", "strip", "try", "doc", "etree", "fromstring", "if", "is", "tmdd", "doc", "from", "converter", "tmdd", "import", "tmdd", "to", "json", "return", "tmdd", "to", "json", "doc", "json", "return", "doc", "xml", "except", "etree", "xmlsyntaxerror", "try", "return", "json", "loads", "json", "except", "valueerror", "raise", "exception", "doesn", "look", "like", "either", "json", "or", "xml"], "docstring": "deserialize json", "docstring_tokens": ["deserialize", "json"], "idx": 70}
{"url": "https://github.com/go-macaroon-bakery/py-macaroon-bakery/blob/63ce1ef1dabe816eb8aaec48fbb46761c34ddf77/macaroonbakery/bakery/_macaroon.py#L242-L248", "repo": "py-macaroon-bakery", "func_name": "deserialize_json", "original_string": ["    def deserialize_json(cls, serialized_json):\n", "        '''Return a macaroon deserialized from a string\n", "        @param serialized_json The string to decode {str}\n", "        @return {Macaroon}\n", "        '''\n", "        serialized = json.loads(serialized_json)\n", "        return Macaroon.from_dict(serialized)\n"], "language": "python", "code": "def deserialize_json(cls, serialized_json):\n    \"\"\"\"\"\"\n    serialized = json.loads(serialized_json)\n    return Macaroon.from_dict(serialized)\n", "code_tokens": ["deserialize", "json", "cls", "serialized", "json", "serialized", "json", "loads", "serialized", "json", "return", "macaroon", "from", "dict", "serialized"], "docstring": "deserialize json", "docstring_tokens": ["deserialize", "json"], "idx": 71}
{"url": "https://github.com/Riffstation/flask-philo/blob/76c9d562edb4a77010c8da6dfdb6489fa29cbc9e/flask_philo/db/postgresql/connection.py#L58-L102", "repo": "flask-philo", "func_name": "initialize", "original_string": ["def initialize(g, app):\n", "    \"\"\"\n", "    If postgresql url is defined in configuration params a\n", "    scoped session will be created\n", "    \"\"\"\n", "    if 'DATABASES' in app.config and 'POSTGRESQL' in app.config['DATABASES']:\n", "        # Database connection established for console commands\n", "        for k, v in app.config['DATABASES']['POSTGRESQL'].items():\n", "            init_db_conn(k, v)\n", "\n", "        if 'test' not in sys.argv:\n", "            # Establish a new connection every request\n", "            @app.before_request\n", "            def before_request():\n", "                \"\"\"\n", "                Assign postgresql connection pool to the global\n", "                flask object at the beginning of every request\n", "                \"\"\"\n", "                # inject stack context if not testing\n", "                from flask import _app_ctx_stack\n", "                for k, v in app.config['DATABASES']['POSTGRESQL'].items():\n", "                    init_db_conn(k, v, scopefunc=_app_ctx_stack)\n", "                g.postgresql_pool = pool\n", "\n", "            # avoid to close connections if testing\n", "            @app.teardown_request\n", "            def teardown_request(exception):\n", "                \"\"\"\n", "                Releasing connection after finish request, not required in unit\n", "                testing\n", "                \"\"\"\n", "                pool = getattr(g, 'postgresql_pool', None)\n", "                if pool is not None:\n", "                    for k, v in pool.connections.items():\n", "                        v.session.remove()\n", "        else:\n", "            @app.before_request\n", "            def before_request():\n", "                \"\"\"\n", "                Assign postgresql connection pool to the global\n", "                flask object at the beginning of every request\n", "                \"\"\"\n", "                for k, v in app.config['DATABASES']['POSTGRESQL'].items():\n", "                    init_db_conn(k, v)\n", "                g.postgresql_pool = pool\n"], "language": "python", "code": "def initialize(g, app):\n    \"\"\"\"\"\"\n    if 'DATABASES' in app.config and 'POSTGRESQL' in app.config['DATABASES']:\n        for k, v in app.config['DATABASES']['POSTGRESQL'].items():\n            init_db_conn(k, v)\n        if 'test' not in sys.argv:\n\n            @app.before_request\n            def before_request():\n                \"\"\"\n                Assign postgresql connection pool to the global\n                flask object at the beginning of every request\n                \"\"\"\n                from flask import _app_ctx_stack\n                for k, v in app.config['DATABASES']['POSTGRESQL'].items():\n                    init_db_conn(k, v, scopefunc=_app_ctx_stack)\n                g.postgresql_pool = pool\n\n            @app.teardown_request\n            def teardown_request(exception):\n                \"\"\"\n                Releasing connection after finish request, not required in unit\n                testing\n                \"\"\"\n                pool = getattr(g, 'postgresql_pool', None)\n                if pool is not None:\n                    for k, v in pool.connections.items():\n                        v.session.remove()\n        else:\n\n            @app.before_request\n            def before_request():\n                \"\"\"\n                Assign postgresql connection pool to the global\n                flask object at the beginning of every request\n                \"\"\"\n                for k, v in app.config['DATABASES']['POSTGRESQL'].items():\n                    init_db_conn(k, v)\n                g.postgresql_pool = pool\n", "code_tokens": ["initialize", "app", "if", "databases", "in", "app", "config", "and", "postgresql", "in", "app", "config", "databases", "for", "in", "app", "config", "databases", "postgresql", "items", "init", "db", "conn", "if", "test", "not", "in", "sys", "argv", "app", "before", "request", "def", "before", "request", "assign", "postgresql", "connection", "pool", "to", "the", "global", "flask", "object", "at", "the", "beginning", "of", "every", "request", "from", "flask", "import", "app", "ctx", "stack", "for", "in", "app", "config", "databases", "postgresql", "items", "init", "db", "conn", "scopefunc", "app", "ctx", "stack", "postgresql", "pool", "pool", "app", "teardown", "request", "def", "teardown", "request", "exception", "releasing", "connection", "after", "finish", "request", "not", "required", "in", "unit", "testing", "pool", "getattr", "postgresql", "pool", "none", "if", "pool", "is", "not", "none", "for", "in", "pool", "connections", "items", "session", "remove", "else", "app", "before", "request", "def", "before", "request", "assign", "postgresql", "connection", "pool", "to", "the", "global", "flask", "object", "at", "the", "beginning", "of", "every", "request", "for", "in", "app", "config", "databases", "postgresql", "items", "init", "db", "conn", "postgresql", "pool", "pool"], "docstring": "postgresql connection", "docstring_tokens": ["postgresql", "connection"], "idx": 72}
{"url": "https://github.com/CivicSpleen/ambry/blob/d7f2be4bf1f7ffd086f3fadd4fcae60c32473e42/ambry/orm/database.py#L785-L793", "repo": "ambry", "func_name": "migrate", "original_string": ["    def migrate(self, connection):\n", "        # use transactions\n", "        if connection.engine.name == 'sqlite':\n", "            self._migrate_sqlite(connection)\n", "        elif connection.engine.name == 'postgresql':\n", "            self._migrate_postgresql(connection)\n", "        else:\n", "            raise DatabaseMissingError(\n", "                'Do not know how to migrate {} engine.'.format(self.connection))\n"], "language": "python", "code": "def migrate(self, connection):\n    if connection.engine.name == 'sqlite':\n        self._migrate_sqlite(connection)\n    elif connection.engine.name == 'postgresql':\n        self._migrate_postgresql(connection)\n    else:\n        raise DatabaseMissingError('Do not know how to migrate {} engine.'.\n            format(self.connection))\n", "code_tokens": ["migrate", "self", "connection", "if", "connection", "engine", "name", "sqlite", "self", "migrate", "sqlite", "connection", "elif", "connection", "engine", "name", "postgresql", "self", "migrate", "postgresql", "connection", "else", "raise", "databasemissingerror", "do", "not", "know", "how", "to", "migrate", "engine", "format", "self", "connection"], "docstring": "postgresql connection", "docstring_tokens": ["postgresql", "connection"], "idx": 73}
{"url": "https://github.com/gabfl/dbschema/blob/37722e6654e9f0374fac5518ebdca22f4c39f92f/src/schema_change.py#L81-L91", "repo": "dbschema", "func_name": "get_connection", "original_string": ["def get_connection(engine, host, user, port, password, database, ssl={}):\n", "    \"\"\" Returns a PostgreSQL or MySQL connection \"\"\"\n", "\n", "    if engine == 'mysql':\n", "        # Connection\n", "        return get_mysql_connection(host, user, port, password, database, ssl)\n", "    elif engine == 'postgresql':\n", "        # Connection\n", "        return get_pg_connection(host, user, port, password, database, ssl)\n", "    else:\n", "        raise RuntimeError('`%s` is not a valid engine.' % engine)\n"], "language": "python", "code": "def get_connection(engine, host, user, port, password, database, ssl={}):\n    \"\"\"\"\"\"\n    if engine == 'mysql':\n        return get_mysql_connection(host, user, port, password, database, ssl)\n    elif engine == 'postgresql':\n        return get_pg_connection(host, user, port, password, database, ssl)\n    else:\n        raise RuntimeError('`%s` is not a valid engine.' % engine)\n", "code_tokens": ["get", "connection", "engine", "host", "user", "port", "password", "database", "ssl", "if", "engine", "mysql", "return", "get", "mysql", "connection", "host", "user", "port", "password", "database", "ssl", "elif", "engine", "postgresql", "return", "get", "pg", "connection", "host", "user", "port", "password", "database", "ssl", "else", "raise", "runtimeerror", "is", "not", "valid", "engine", "engine"], "docstring": "postgresql connection", "docstring_tokens": ["postgresql", "connection"], "idx": 74}
{"url": "https://github.com/sendgrid/sendgrid-python/blob/266c2abde7a35dfcce263e06bedc6a0bbdebeac9/examples/helpers/stats/stats_example.py#L13-L14", "repo": "sendgrid-python", "func_name": "pprint_json", "original_string": ["def pprint_json(json_raw):\n", "    print(json.dumps(json.loads(json_raw), indent=2, sort_keys=True))\n"], "language": "python", "code": "def pprint_json(json_raw):\n    print(json.dumps(json.loads(json_raw), indent=2, sort_keys=True))\n", "code_tokens": ["pprint", "json", "json", "raw", "print", "json", "dumps", "json", "loads", "json", "raw", "indent", "2", "sort", "keys", "true"], "docstring": "pretty print json", "docstring_tokens": ["pretty", "print", "json"], "idx": 75}
{"url": "https://github.com/markbaas/python-iresolve/blob/ba91e37221e91265e4ac5dbc6e8f5cffa955a04f/iresolve.py#L136-L141", "repo": "python-iresolve", "func_name": "output", "original_string": ["def output(results, output_format='pretty'):\n", "    if output_format == 'pretty':\n", "        for u, meta in results.items():\n", "            print('* {} can be imported from: {}'.format(u, ', '.join(meta['paths'])))\n", "    elif output_format == 'json':\n", "        print(json.dumps(results))\n"], "language": "python", "code": "def output(results, output_format='pretty'):\n    if output_format == 'pretty':\n        for u, meta in results.items():\n            print('* {} can be imported from: {}'.format(u, ', '.join(meta[\n                'paths'])))\n    elif output_format == 'json':\n        print(json.dumps(results))\n", "code_tokens": ["output", "results", "output", "format", "pretty", "if", "output", "format", "pretty", "for", "meta", "in", "results", "items", "print", "can", "be", "imported", "from", "format", "join", "meta", "paths", "elif", "output", "format", "json", "print", "json", "dumps", "results"], "docstring": "pretty print json", "docstring_tokens": ["pretty", "print", "json"], "idx": 76}
{"url": "https://github.com/deeshugupta/tes/blob/217db49aa211ebca2d9258380765a0c31abfca91/es_commands/base.py#L38-L40", "repo": "tes", "func_name": "pretty_print", "original_string": ["def pretty_print(response):\n", "    parsed = json.loads(json.dumps(response))\n", "    click.echo(json.dumps(parsed, indent=4, sort_keys=True))\n"], "language": "python", "code": "def pretty_print(response):\n    parsed = json.loads(json.dumps(response))\n    click.echo(json.dumps(parsed, indent=4, sort_keys=True))\n", "code_tokens": ["pretty", "print", "response", "parsed", "json", "loads", "json", "dumps", "response", "click", "echo", "json", "dumps", "parsed", "indent", "4", "sort", "keys", "true"], "docstring": "pretty print json", "docstring_tokens": ["pretty", "print", "json"], "idx": 77}
{"url": "https://github.com/alejandroesquiva/AutomaticApiRest-PythonConnector/blob/90e55d5cf953dc8d7186fc6df82a6c6a089a3bbe/build/lib/aarpy/AARConnector.py#L36-L38", "repo": "AutomaticApiRest-PythonConnector", "func_name": "printJson", "original_string": ["    def printJson(self):\n", "        jsonobject = self.getJson()\n", "        print(json.dumps(jsonobject, indent=1, sort_keys=True))\n"], "language": "python", "code": "def printJson(self):\n    jsonobject = self.getJson()\n    print(json.dumps(jsonobject, indent=1, sort_keys=True))\n", "code_tokens": ["printjson", "self", "jsonobject", "self", "getjson", "print", "json", "dumps", "jsonobject", "indent", "1", "sort", "keys", "true"], "docstring": "pretty print json", "docstring_tokens": ["pretty", "print", "json"], "idx": 78}
{"url": "https://github.com/MacHu-GWU/angora-project/blob/689a60da51cd88680ddbe26e28dbe81e6b01d275/angora/dtypes/dicttree.py#L378-L382", "repo": "angora-project", "func_name": "prettyprint", "original_string": ["    def prettyprint(d):\n", "        \"\"\"Print dicttree in Json-like format. keys are sorted\n", "        \"\"\"\n", "        print(json.dumps(d, sort_keys=True, \n", "                         indent=4, separators=(\",\" , \": \")))\n"], "language": "python", "code": "def prettyprint(d):\n    \"\"\"\"\"\"\n    print(json.dumps(d, sort_keys=True, indent=4, separators=(',', ': ')))\n", "code_tokens": ["prettyprint", "print", "json", "dumps", "sort", "keys", "true", "indent", "4", "separators"], "docstring": "pretty print json", "docstring_tokens": ["pretty", "print", "json"], "idx": 79}
{"url": "https://github.com/PSPC-SPAC-buyandsell/von_anchor/blob/78ac1de67be42a676274f4bf71fe12f66e72f309/von_anchor/frill.py#L30-L45", "repo": "von_anchor", "func_name": "ppjson", "original_string": ["def ppjson(dumpit: Any, elide_to: int = None) -> str:\n", "    \"\"\"\n", "    JSON pretty printer, whether already json-encoded or not\n", "\n", "    :param dumpit: object to pretty-print\n", "    :param elide_to: optional maximum length including ellipses ('...')\n", "    :return: json pretty-print\n", "    \"\"\"\n", "\n", "    if elide_to is not None:\n", "        elide_to = max(elide_to, 3)  # make room for ellipses '...'\n", "    try:\n", "        rv = json.dumps(json.loads(dumpit) if isinstance(dumpit, str) else dumpit, indent=4)\n", "    except TypeError:\n", "        rv = '{}'.format(pformat(dumpit, indent=4, width=120))\n", "    return rv if elide_to is None or len(rv) <= elide_to else '{}...'.format(rv[0:(elide_to - 3)])\n"], "language": "python", "code": "def ppjson(dumpit: Any, elide_to: int=None) ->str:\n    \"\"\"\"\"\"\n    if elide_to is not None:\n        elide_to = max(elide_to, 3)\n    try:\n        rv = json.dumps(json.loads(dumpit) if isinstance(dumpit, str) else\n            dumpit, indent=4)\n    except TypeError:\n        rv = '{}'.format(pformat(dumpit, indent=4, width=120))\n    return rv if elide_to is None or len(rv) <= elide_to else '{}...'.format(rv\n        [0:elide_to - 3])\n", "code_tokens": ["ppjson", "dumpit", "any", "elide", "to", "int", "none", "str", "if", "elide", "to", "is", "not", "none", "elide", "to", "max", "elide", "to", "3", "try", "rv", "json", "dumps", "json", "loads", "dumpit", "if", "isinstance", "dumpit", "str", "else", "dumpit", "indent", "4", "except", "typeerror", "rv", "format", "pformat", "dumpit", "indent", "4", "width", "120", "return", "rv", "if", "elide", "to", "is", "none", "or", "len", "rv", "elide", "to", "else", "format", "rv", "0", "elide", "to", "3"], "docstring": "pretty print json", "docstring_tokens": ["pretty", "print", "json"], "idx": 80}
{"url": "https://github.com/mosesschwartz/scrypture/blob/d51eb0c9835a5122a655078268185ce8ab9ec86a/scrypture/demo_scripts/Utils/json_pretty_print.py#L8-L14", "repo": "scrypture", "func_name": "json_pretty_print", "original_string": ["def json_pretty_print(s):\n", "    '''pretty print JSON'''\n", "    s = json.loads(s)\n", "    return json.dumps(s,\n", "                      sort_keys=True,\n", "                      indent=4,\n", "                      separators=(',', ': '))\n"], "language": "python", "code": "def json_pretty_print(s):\n    \"\"\"\"\"\"\n    s = json.loads(s)\n    return json.dumps(s, sort_keys=True, indent=4, separators=(',', ': '))\n", "code_tokens": ["json", "pretty", "print", "json", "loads", "return", "json", "dumps", "sort", "keys", "true", "indent", "4", "separators"], "docstring": "pretty print json", "docstring_tokens": ["pretty", "print", "json"], "idx": 81}
{"url": "https://github.com/gtaylor/django-athumb/blob/69261ace0dff81e33156a54440874456a7b38dfb/athumb/backends/s3boto.py#L143-L150", "repo": "django-athumb", "func_name": "_compress_content", "original_string": ["    def _compress_content(self, content):\n", "        \"\"\"Gzip a given string.\"\"\"\n", "        zbuf = StringIO()\n", "        zfile = GzipFile(mode='wb', compresslevel=6, fileobj=zbuf)\n", "        zfile.write(content.read())\n", "        zfile.close()\n", "        content.file = zbuf\n", "        return content\n"], "language": "python", "code": "def _compress_content(self, content):\n    \"\"\"\"\"\"\n    zbuf = StringIO()\n    zfile = GzipFile(mode='wb', compresslevel=6, fileobj=zbuf)\n    zfile.write(content.read())\n    zfile.close()\n    content.file = zbuf\n    return content\n", "code_tokens": ["compress", "content", "self", "content", "zbuf", "stringio", "zfile", "gzipfile", "mode", "wb", "compresslevel", "6", "fileobj", "zbuf", "zfile", "write", "content", "read", "zfile", "close", "content", "file", "zbuf", "return", "content"], "docstring": "how to read the contents of a .gz compressed file?", "docstring_tokens": ["how", "to", "read", "the", "contents", "of", "a", "gz", "compressed", "file"], "idx": 82}
{"url": "https://github.com/sunlightlabs/django-mediasync/blob/aa8ce4cfff757bbdb488463c64c0863cca6a1932/mediasync/__init__.py#L33-L38", "repo": "django-mediasync", "func_name": "compress", "original_string": ["def compress(s):\n", "    zbuf = cStringIO.StringIO()\n", "    zfile = gzip.GzipFile(mode='wb', compresslevel=6, fileobj=zbuf)\n", "    zfile.write(s)\n", "    zfile.close()\n", "    return zbuf.getvalue()\n"], "language": "python", "code": "def compress(s):\n    zbuf = cStringIO.StringIO()\n    zfile = gzip.GzipFile(mode='wb', compresslevel=6, fileobj=zbuf)\n    zfile.write(s)\n    zfile.close()\n    return zbuf.getvalue()\n", "code_tokens": ["compress", "zbuf", "cstringio", "stringio", "zfile", "gzip", "gzipfile", "mode", "wb", "compresslevel", "6", "fileobj", "zbuf", "zfile", "write", "zfile", "close", "return", "zbuf", "getvalue"], "docstring": "how to read the contents of a .gz compressed file?", "docstring_tokens": ["how", "to", "read", "the", "contents", "of", "a", "gz", "compressed", "file"], "idx": 83}
{"url": "https://github.com/zeroSteiner/AdvancedHTTPServer/blob/8c53cf7e1ddbf7ae9f573c82c5fe5f6992db7b5a/advancedhttpserver.py#L1920-L1927", "repo": "AdvancedHTTPServer", "func_name": "_serve_ready", "original_string": ["\tdef _serve_ready(self):\n", "\t\tread_check = [self.__wakeup_fd]\n", "\t\tfor sub_server in self.sub_servers:\n", "\t\t\tread_check.extend(sub_server.read_checkable_fds)\n", "\t\tall_read_ready, _, _ = select.select(read_check, [], [])\n", "\t\tfor read_ready in all_read_ready:\n", "\t\t\tif isinstance(read_ready, (_RequestEmbryo, http.server.HTTPServer)):\n", "\t\t\t\tread_ready.serve_ready()\n"], "language": "python", "code": "def _serve_ready(self):\n    read_check = [self.__wakeup_fd]\n    for sub_server in self.sub_servers:\n        read_check.extend(sub_server.read_checkable_fds)\n    all_read_ready, _, _ = select.select(read_check, [], [])\n    for read_ready in all_read_ready:\n        if isinstance(read_ready, (_RequestEmbryo, http.server.HTTPServer)):\n            read_ready.serve_ready()\n", "code_tokens": ["serve", "ready", "self", "read", "check", "self", "wakeup", "fd", "for", "sub", "server", "in", "self", "sub", "servers", "read", "check", "extend", "sub", "server", "read", "checkable", "fds", "all", "read", "ready", "select", "select", "read", "check", "for", "read", "ready", "in", "all", "read", "ready", "if", "isinstance", "read", "ready", "requestembryo", "http", "server", "httpserver", "read", "ready", "serve", "ready"], "docstring": "readonly array", "docstring_tokens": ["readonly", "array"], "idx": 84}
{"url": "https://github.com/mirukan/lunafind/blob/77bdfe02df98a7f74d0ae795fee3b1729218995d/lunafind/order.py#L53-L79", "repo": "lunafind", "func_name": "sort", "original_string": ["def sort(posts: List[Post], by: str) -> List[Post]:\n", "    by_val  = by.replace(\"asc_\", \"\").replace(\"desc_\", \"\")\n", "\n", "    in_dict = (ORDER_NUM   if by_val in ORDER_NUM   else\n", "               ORDER_DATE  if by_val in ORDER_DATE  else\n", "               ORDER_FUNCS if by_val in ORDER_FUNCS else None)\n", "\n", "    if not in_dict:\n", "        raise ValueError(\n", "            f\"Got {by_val!r} as ordering method, must be one of: %s\" %\n", "            \", \".join(set(ORDER_NUM) | set(ORDER_DATE) | set(ORDER_FUNCS))\n", "        )\n", "\n", "    if in_dict == ORDER_FUNCS:\n", "        posts.sort(key=ORDER_FUNCS[by], reverse=(by != \"random\"))\n", "        return posts\n", "\n", "    by_full = by if by.startswith(\"asc_\") or by.startswith(\"desc_\") else \\\n", "              f\"%s_{by}\" % in_dict[by][0]\n", "\n", "    def sort_key(post: Post) -> int:\n", "        key = in_dict[by_val][1]\n", "        key = post.info[key] if not callable(key) else key(post.info)\n", "        return pend.parse(key) if in_dict == ORDER_DATE else key\n", "\n", "    posts.sort(key=sort_key, reverse=by_full.startswith(\"desc_\"))\n", "    return posts\n"], "language": "python", "code": "def sort(posts: List[Post], by: str) ->List[Post]:\n    by_val = by.replace('asc_', '').replace('desc_', '')\n    in_dict = (ORDER_NUM if by_val in ORDER_NUM else ORDER_DATE if by_val in\n        ORDER_DATE else ORDER_FUNCS if by_val in ORDER_FUNCS else None)\n    if not in_dict:\n        raise ValueError(\n            f'Got {by_val!r} as ordering method, must be one of: %s' % ', '\n            .join(set(ORDER_NUM) | set(ORDER_DATE) | set(ORDER_FUNCS)))\n    if in_dict == ORDER_FUNCS:\n        posts.sort(key=ORDER_FUNCS[by], reverse=by != 'random')\n        return posts\n    by_full = by if by.startswith('asc_') or by.startswith('desc_'\n        ) else f'%s_{by}' % in_dict[by][0]\n\n    def sort_key(post: Post) ->int:\n        key = in_dict[by_val][1]\n        key = post.info[key] if not callable(key) else key(post.info)\n        return pend.parse(key) if in_dict == ORDER_DATE else key\n    posts.sort(key=sort_key, reverse=by_full.startswith('desc_'))\n    return posts\n", "code_tokens": ["sort", "posts", "list", "post", "by", "str", "list", "post", "by", "val", "by", "replace", "asc", "replace", "desc", "in", "dict", "order", "num", "if", "by", "val", "in", "order", "num", "else", "order", "date", "if", "by", "val", "in", "order", "date", "else", "order", "funcs", "if", "by", "val", "in", "order", "funcs", "else", "none", "if", "not", "in", "dict", "raise", "valueerror", "got", "by", "val", "as", "ordering", "method", "must", "be", "one", "of", "join", "set", "order", "num", "set", "order", "date", "set", "order", "funcs", "if", "in", "dict", "order", "funcs", "posts", "sort", "key", "order", "funcs", "by", "reverse", "by", "random", "return", "posts", "by", "full", "by", "if", "by", "startswith", "asc", "or", "by", "startswith", "desc", "else", "by", "in", "dict", "by", "0", "def", "sort", "key", "post", "post", "int", "key", "in", "dict", "by", "val", "1", "key", "post", "info", "key", "if", "not", "callable", "key", "else", "key", "post", "info", "return", "pend", "parse", "key", "if", "in", "dict", "order", "date", "else", "key", "posts", "sort", "key", "sort", "key", "reverse", "by", "full", "startswith", "desc", "return", "posts"], "docstring": "sort string list", "docstring_tokens": ["sort", "string", "list"], "idx": 85}
{"url": "https://github.com/readbeyond/aeneas/blob/9d95535ad63eef4a98530cfdff033b8c35315ee1/aeneas/idsortingalgorithm.py#L79-L109", "repo": "aeneas", "func_name": "sort", "original_string": ["    def sort(self, ids):\n", "        \"\"\"\n", "        Sort the given list of identifiers,\n", "        returning a new (sorted) list.\n", "\n", "        :param list ids: the list of identifiers to be sorted\n", "        :rtype: list\n", "        \"\"\"\n", "        def extract_int(string):\n", "            \"\"\"\n", "            Extract an integer from the given string.\n", "\n", "            :param string string: the identifier string\n", "            :rtype: int\n", "            \"\"\"\n", "            return int(re.sub(r\"[^0-9]\", \"\", string))\n", "\n", "        tmp = list(ids)\n", "        if self.algorithm == IDSortingAlgorithm.UNSORTED:\n", "            self.log(u\"Sorting using UNSORTED\")\n", "        elif self.algorithm == IDSortingAlgorithm.LEXICOGRAPHIC:\n", "            self.log(u\"Sorting using LEXICOGRAPHIC\")\n", "            tmp = sorted(ids)\n", "        elif self.algorithm == IDSortingAlgorithm.NUMERIC:\n", "            self.log(u\"Sorting using NUMERIC\")\n", "            tmp = ids\n", "            try:\n", "                tmp = sorted(tmp, key=extract_int)\n", "            except (ValueError, TypeError) as exc:\n", "                self.log_exc(u\"Not all id values contain a numeric part. Returning the id list unchanged.\", exc, False, None)\n", "        return tmp\n"], "language": "python", "code": "def sort(self, ids):\n    \"\"\"\"\"\"\n\n    def extract_int(string):\n        \"\"\"\n            Extract an integer from the given string.\n\n            :param string string: the identifier string\n            :rtype: int\n            \"\"\"\n        return int(re.sub('[^0-9]', '', string))\n    tmp = list(ids)\n    if self.algorithm == IDSortingAlgorithm.UNSORTED:\n        self.log(u'Sorting using UNSORTED')\n    elif self.algorithm == IDSortingAlgorithm.LEXICOGRAPHIC:\n        self.log(u'Sorting using LEXICOGRAPHIC')\n        tmp = sorted(ids)\n    elif self.algorithm == IDSortingAlgorithm.NUMERIC:\n        self.log(u'Sorting using NUMERIC')\n        tmp = ids\n        try:\n            tmp = sorted(tmp, key=extract_int)\n        except (ValueError, TypeError) as exc:\n            self.log_exc(\n                u'Not all id values contain a numeric part. Returning the id list unchanged.'\n                , exc, False, None)\n    return tmp\n", "code_tokens": ["sort", "self", "ids", "def", "extract", "int", "string", "extract", "an", "integer", "from", "the", "given", "string", "param", "string", "string", "the", "identifier", "string", "rtype", "int", "return", "int", "re", "sub", "0", "9", "string", "tmp", "list", "ids", "if", "self", "algorithm", "idsortingalgorithm", "unsorted", "self", "log", "sorting", "using", "unsorted", "elif", "self", "algorithm", "idsortingalgorithm", "lexicographic", "self", "log", "sorting", "using", "lexicographic", "tmp", "sorted", "ids", "elif", "self", "algorithm", "idsortingalgorithm", "numeric", "self", "log", "sorting", "using", "numeric", "tmp", "ids", "try", "tmp", "sorted", "tmp", "key", "extract", "int", "except", "valueerror", "typeerror", "as", "exc", "self", "log", "exc", "not", "all", "id", "values", "contain", "numeric", "part", "returning", "the", "id", "list", "unchanged", "exc", "false", "none", "return", "tmp"], "docstring": "sort string list", "docstring_tokens": ["sort", "string", "list"], "idx": 86}
{"url": "https://github.com/wdbm/pyprel/blob/c1253ea3f8c60a2f5493a0d5a61ca3c84df7c21d/pyprel_examples_database.py#L54-L115", "repo": "pyprel", "func_name": "main", "original_string": ["def main(options):\n", "\n", "    filename_database = options[\"--database\"]\n", "    name_table        = options[\"--table\"]\n", "\n", "    print(\"\\npyprel database examples\\n\")\n", "\n", "    if os.path.exists(filename_database):\n", "        print(\"create database {database}\".format(\n", "            database = filename_database\n", "        ))\n", "        create_database(filename = \"database.db\")\n", "\n", "    print(\"access database {filename}\".format(\n", "        filename = filename_database\n", "    ))\n", "    database = dataset.connect(\n", "        \"sqlite:///{filename_database}\".format(\n", "            filename_database = filename_database\n", "        )\n", "    )\n", "    table = database[name_table]\n", "\n", "    print(\"add data to database\")\n", "    table.insert(dict(\n", "        name     = \"Legolas Greenleaf\",\n", "        age      = 2000,\n", "        country  = \"Mirkwood\",\n", "        uuid4    = str(uuid.uuid4())\n", "    ))\n", "    table.insert(dict(\n", "        name     = \"Cody Rapol\",\n", "        age      = 30,\n", "        country  = \"USA\",\n", "        activity = \"DDR\",\n", "        uuid4    = str(uuid.uuid4())\n", "    ))\n", "\n", "    print(\n", "\"\"\"\n", "database tables:\\n{tables}\n", "\\ntable {table} columns:\\n{columns}\n", "\\ntable {table} row one:\\n{row}\n", "\"\"\".format(\n", "            tables  = database.tables,\n", "            table   = name_table,\n", "            columns = database[name_table].columns,\n", "            row     = [entry for entry in table.find(id = \"1\")]\n", "        )\n", "    )\n", "\n", "    print(\"table {table} printout:\\n\".format(\n", "        table = name_table\n", "    ))\n", "\n", "    print(\n", "        pyprel.Table(\n", "            contents = pyprel.table_dataset_database_table(\n", "                table = database[name_table]\n", "            )\n", "        )\n", "    )\n"], "language": "python", "code": "def main(options):\n    filename_database = options['--database']\n    name_table = options['--table']\n    print('\\npyprel database examples\\n')\n    if os.path.exists(filename_database):\n        print('create database {database}'.format(database=filename_database))\n        create_database(filename='database.db')\n    print('access database {filename}'.format(filename=filename_database))\n    database = dataset.connect('sqlite:///{filename_database}'.format(\n        filename_database=filename_database))\n    table = database[name_table]\n    print('add data to database')\n    table.insert(dict(name='Legolas Greenleaf', age=2000, country=\n        'Mirkwood', uuid4=str(uuid.uuid4())))\n    table.insert(dict(name='Cody Rapol', age=30, country='USA', activity=\n        'DDR', uuid4=str(uuid.uuid4())))\n    print(\n        \"\"\"\ndatabase tables:\n{tables}\n\ntable {table} columns:\n{columns}\n\ntable {table} row one:\n{row}\n\"\"\"\n        .format(tables=database.tables, table=name_table, columns=database[\n        name_table].columns, row=[entry for entry in table.find(id='1')]))\n    print('table {table} printout:\\n'.format(table=name_table))\n    print(pyprel.Table(contents=pyprel.table_dataset_database_table(table=\n        database[name_table])))\n", "code_tokens": ["main", "options", "filename", "database", "options", "database", "name", "table", "options", "table", "print", "npyprel", "database", "examples", "if", "os", "path", "exists", "filename", "database", "print", "create", "database", "database", "format", "database", "filename", "database", "create", "database", "filename", "database", "db", "print", "access", "database", "filename", "format", "filename", "filename", "database", "database", "dataset", "connect", "sqlite", "filename", "database", "format", "filename", "database", "filename", "database", "table", "database", "name", "table", "print", "add", "data", "to", "database", "table", "insert", "dict", "name", "legolas", "greenleaf", "age", "2000", "country", "mirkwood", "str", "uuid", "table", "insert", "dict", "name", "cody", "rapol", "age", "30", "country", "usa", "activity", "ddr", "str", "uuid", "print", "database", "tables", "tables", "table", "table", "columns", "columns", "table", "table", "row", "one", "row", "format", "tables", "database", "tables", "table", "name", "table", "columns", "database", "name", "table", "columns", "row", "entry", "for", "entry", "in", "table", "find", "id", "1", "print", "table", "table", "printout", "format", "table", "name", "table", "print", "pyprel", "table", "contents", "pyprel", "table", "dataset", "database", "table", "table", "database", "name", "table"], "docstring": "how to get database table name", "docstring_tokens": ["how", "to", "get", "database", "table", "name"], "idx": 87}
{"url": "https://github.com/firstprayer/monsql/blob/6285c15b574c8664046eae2edfeb548c7b173efd/monsql/wrapper_postgresql.py#L48-L50", "repo": "monsql", "func_name": "get_table_obj", "original_string": ["    def get_table_obj(self, name):\n", "        table = PostgreSQLTable(db=self.db, name=name, mode=self.mode)\n", "        return table\n"], "language": "python", "code": "def get_table_obj(self, name):\n    table = PostgreSQLTable(db=self.db, name=name, mode=self.mode)\n    return table\n", "code_tokens": ["get", "table", "obj", "self", "name", "table", "postgresqltable", "db", "self", "db", "name", "name", "mode", "self", "mode", "return", "table"], "docstring": "how to get database table name", "docstring_tokens": ["how", "to", "get", "database", "table", "name"], "idx": 88}
{"url": "https://github.com/synw/dataswim/blob/4a4a53f80daa7cd8e8409d76a19ce07296269da2/dataswim/db/infos.py#L56-L78", "repo": "dataswim", "func_name": "table", "original_string": ["\n", "    def table(self, name: str):\n", "        \"\"\"\n", "        Display info about a table: number of rows\n", "        and columns\n", "\n", "        :param name: name of the table\n", "        :type name: str\n", "\n", "        :example: ``tables = ds.table(\"mytable\")``\n", "        \"\"\"\n", "        if self._check_db() is False:\n", "            return\n", "        try:\n", "            res = self.getall_(name)\n", "        except Exception as e:\n", "            self.err(e, self.table, \"Can not get records from database\")\n", "            return\n", "        if res is None:\n", "            self.warning(\"Table\", name, \"does not contain any record\")\n", "            return\n", "        num = len(res)\n", "        self.info(num, \"rows\")\n"], "language": "python", "code": "def table(self, name: str):\n    \"\"\"\"\"\"\n    if self._check_db() is False:\n        return\n    try:\n        res = self.getall_(name)\n    except Exception as e:\n        self.err(e, self.table, 'Can not get records from database')\n        return\n    if res is None:\n        self.warning('Table', name, 'does not contain any record')\n        return\n    num = len(res)\n    self.info(num, 'rows')\n", "code_tokens": ["table", "self", "name", "str", "if", "self", "check", "db", "is", "false", "return", "try", "res", "self", "getall", "name", "except", "exception", "as", "self", "err", "self", "table", "can", "not", "get", "records", "from", "database", "return", "if", "res", "is", "none", "self", "warning", "table", "name", "does", "not", "contain", "any", "record", "return", "num", "len", "res", "self", "info", "num", "rows"], "docstring": "how to get database table name", "docstring_tokens": ["how", "to", "get", "database", "table", "name"], "idx": 89}
{"url": "https://github.com/mnick/scikit-tensor/blob/fe517e9661a08164b8d30d2dddf7c96aeeabcf36/sktensor/cp.py#L190-L205", "repo": "scikit-tensor", "func_name": "_init", "original_string": ["def _init(init, X, N, rank, dtype):\n", "    \"\"\"\n", "    Initialization for CP models\n", "    \"\"\"\n", "    Uinit = [None for _ in range(N)]\n", "    if isinstance(init, list):\n", "        Uinit = init\n", "    elif init == 'random':\n", "        for n in range(1, N):\n", "            Uinit[n] = array(rand(X.shape[n], rank), dtype=dtype)\n", "    elif init == 'nvecs':\n", "        for n in range(1, N):\n", "            Uinit[n] = array(nvecs(X, n, rank), dtype=dtype)\n", "    else:\n", "        raise 'Unknown option (init=%s)' % str(init)\n", "    return Uinit\n"], "language": "python", "code": "def _init(init, X, N, rank, dtype):\n    \"\"\"\"\"\"\n    Uinit = [None for _ in range(N)]\n    if isinstance(init, list):\n        Uinit = init\n    elif init == 'random':\n        for n in range(1, N):\n            Uinit[n] = array(rand(X.shape[n], rank), dtype=dtype)\n    elif init == 'nvecs':\n        for n in range(1, N):\n            Uinit[n] = array(nvecs(X, n, rank), dtype=dtype)\n    else:\n        raise ('Unknown option (init=%s)' % str(init))\n    return Uinit\n", "code_tokens": ["init", "init", "rank", "dtype", "uinit", "none", "for", "in", "range", "if", "isinstance", "init", "list", "uinit", "init", "elif", "init", "random", "for", "in", "range", "1", "uinit", "array", "rand", "shape", "rank", "dtype", "dtype", "elif", "init", "nvecs", "for", "in", "range", "1", "uinit", "array", "nvecs", "rank", "dtype", "dtype", "else", "raise", "unknown", "option", "init", "str", "init", "return", "uinit"], "docstring": "initializing array", "docstring_tokens": ["initializing", "array"], "idx": 90}
{"url": "https://github.com/gbiggs/rtsprofile/blob/fded6eddcb0b25fe9808b1b12336a4413ea00905/rtsprofile/rts_profile.py#L500-L503", "repo": "rtsprofile", "func_name": "initializing", "original_string": ["    def initializing(self, initializing):\n", "        validate_attribute(initializing, 'rts_profile.Initializing',\n", "                           expected_type=Initialize, required=False)\n", "        self._initializing = initializing\n"], "language": "python", "code": "def initializing(self, initializing):\n    validate_attribute(initializing, 'rts_profile.Initializing',\n        expected_type=Initialize, required=False)\n    self._initializing = initializing\n", "code_tokens": ["initializing", "self", "initializing", "validate", "attribute", "initializing", "rts", "profile", "initializing", "expected", "type", "initialize", "required", "false", "self", "initializing", "initializing"], "docstring": "initializing array", "docstring_tokens": ["initializing", "array"], "idx": 91}
{"url": "https://github.com/aboSamoor/polyglot/blob/d0d2aa8d06cec4e03bd96618ae960030f7069a17/polyglot/decorators.py#L23-L32", "repo": "polyglot", "func_name": "memoize", "original_string": ["def memoize(obj):\n", "  cache = obj.cache = {}\n", "\n", "  @functools.wraps(obj)\n", "  def memoizer(*args, **kwargs):\n", "    key = tuple(list(args) + sorted(kwargs.items()))\n", "    if key not in cache:\n", "      cache[key] = obj(*args, **kwargs)\n", "    return cache[key]\n", "  return memoizer\n"], "language": "python", "code": "def memoize(obj):\n    cache = obj.cache = {}\n\n    @functools.wraps(obj)\n    def memoizer(*args, **kwargs):\n        key = tuple(list(args) + sorted(kwargs.items()))\n        if key not in cache:\n            cache[key] = obj(*args, **kwargs)\n        return cache[key]\n    return memoizer\n", "code_tokens": ["memoize", "obj", "cache", "obj", "cache", "functools", "wraps", "obj", "def", "memoizer", "args", "kwargs", "key", "tuple", "list", "args", "sorted", "kwargs", "items", "if", "key", "not", "in", "cache", "cache", "key", "obj", "args", "kwargs", "return", "cache", "key", "return", "memoizer"], "docstring": "memoize to disk  - persistent memoization", "docstring_tokens": ["memoize", "to", "disk", "persistent", "memoization"], "idx": 92}
{"url": "https://github.com/fridiculous/estimators/blob/ab5b3d70f16f8372ae1114ac7e54e7791631eb74/estimators/hashing.py#L120-L127", "repo": "estimators", "func_name": "memoize", "original_string": ["    def memoize(self, obj):\n", "        # We want hashing to be sensitive to value instead of reference.\n", "        # For example we want ['aa', 'aa'] and ['aa', 'aaZ'[:2]]\n", "        # to hash to the same value and that's why we disable memoization\n", "        # for strings\n", "        if isinstance(obj, _bytes_or_unicode):\n", "            return\n", "        Pickler.memoize(self, obj)\n"], "language": "python", "code": "def memoize(self, obj):\n    if isinstance(obj, _bytes_or_unicode):\n        return\n    Pickler.memoize(self, obj)\n", "code_tokens": ["memoize", "self", "obj", "if", "isinstance", "obj", "bytes", "or", "unicode", "return", "pickler", "memoize", "self", "obj"], "docstring": "memoize to disk  - persistent memoization", "docstring_tokens": ["memoize", "to", "disk", "persistent", "memoization"], "idx": 93}
{"url": "https://github.com/Erotemic/utool/blob/3b27e1f4e6e6fb23cd8744af7b7195b57d99e03a/utool/util_decor.py#L601-L651", "repo": "utool", "func_name": "memoize", "original_string": ["def memoize(func):\n", "    \"\"\"\n", "    simple memoization decorator\n", "\n", "    References:\n", "        https://wiki.python.org/moin/PythonDecoratorLibrary#Memoize\n", "\n", "    Args:\n", "        func (function):  live python function\n", "\n", "    Returns:\n", "        func:\n", "\n", "    CommandLine:\n", "        python -m utool.util_decor memoize\n", "\n", "    Example:\n", "        >>> # ENABLE_DOCTEST\n", "        >>> from utool.util_decor import *  # NOQA\n", "        >>> import utool as ut\n", "        >>> closure = {'a': 'b', 'c': 'd'}\n", "        >>> incr = [0]\n", "        >>> def foo(key):\n", "        >>>     value = closure[key]\n", "        >>>     incr[0] += 1\n", "        >>>     return value\n", "        >>> foo_memo = memoize(foo)\n", "        >>> assert foo('a') == 'b' and foo('c') == 'd'\n", "        >>> assert incr[0] == 2\n", "        >>> print('Call memoized version')\n", "        >>> assert foo_memo('a') == 'b' and foo_memo('c') == 'd'\n", "        >>> assert incr[0] == 4\n", "        >>> assert foo_memo('a') == 'b' and foo_memo('c') == 'd'\n", "        >>> print('Counter should no longer increase')\n", "        >>> assert incr[0] == 4\n", "        >>> print('Closure changes result without memoization')\n", "        >>> closure = {'a': 0, 'c': 1}\n", "        >>> assert foo('a') == 0 and foo('c') == 1\n", "        >>> assert incr[0] == 6\n", "        >>> assert foo_memo('a') == 'b' and foo_memo('c') == 'd'\n", "    \"\"\"\n", "    cache = func._util_decor_memoize_cache = {}\n", "    # @functools.wraps(func)\n", "    def memoizer(*args, **kwargs):\n", "        key = str(args) + str(kwargs)\n", "        if key not in cache:\n", "            cache[key] = func(*args, **kwargs)\n", "        return cache[key]\n", "    memoizer = preserve_sig(memoizer, func)\n", "    memoizer.cache = cache\n", "    return memoizer\n"], "language": "python", "code": "def memoize(func):\n    \"\"\"\"\"\"\n    cache = func._util_decor_memoize_cache = {}\n\n    def memoizer(*args, **kwargs):\n        key = str(args) + str(kwargs)\n        if key not in cache:\n            cache[key] = func(*args, **kwargs)\n        return cache[key]\n    memoizer = preserve_sig(memoizer, func)\n    memoizer.cache = cache\n    return memoizer\n", "code_tokens": ["memoize", "func", "cache", "func", "util", "decor", "memoize", "cache", "def", "memoizer", "args", "kwargs", "key", "str", "args", "str", "kwargs", "if", "key", "not", "in", "cache", "cache", "key", "func", "args", "kwargs", "return", "cache", "key", "memoizer", "preserve", "sig", "memoizer", "func", "memoizer", "cache", "cache", "return", "memoizer"], "docstring": "memoize to disk  - persistent memoization", "docstring_tokens": ["memoize", "to", "disk", "persistent", "memoization"], "idx": 94}
{"url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/psutil/_pslinux.py#L122-L146", "repo": "gurumate", "func_name": "virtual_memory", "original_string": ["def virtual_memory():\n", "    total, free, buffers, shared, _, _ = _psutil_linux.get_sysinfo()\n", "    cached = active = inactive = None\n", "    f = open('/proc/meminfo', 'r')\n", "    try:\n", "        for line in f:\n", "            if line.startswith('Cached:'):\n", "                cached = int(line.split()[1]) * 1024\n", "            elif line.startswith('Active:'):\n", "                active = int(line.split()[1]) * 1024\n", "            elif line.startswith('Inactive:'):\n", "                inactive = int(line.split()[1]) * 1024\n", "            if cached is not None \\\n", "            and active is not None \\\n", "            and inactive is not None:\n", "                break\n", "        else:\n", "            raise RuntimeError(\"line(s) not found\")\n", "    finally:\n", "        f.close()\n", "    avail = free + buffers + cached\n", "    used = total - free\n", "    percent = usage_percent((total - avail), total, _round=1)\n", "    return nt_virtmem_info(total, avail, percent, used, free,\n", "                           active, inactive, buffers, cached)\n"], "language": "python", "code": "def virtual_memory():\n    total, free, buffers, shared, _, _ = _psutil_linux.get_sysinfo()\n    cached = active = inactive = None\n    f = open('/proc/meminfo', 'r')\n    try:\n        for line in f:\n            if line.startswith('Cached:'):\n                cached = int(line.split()[1]) * 1024\n            elif line.startswith('Active:'):\n                active = int(line.split()[1]) * 1024\n            elif line.startswith('Inactive:'):\n                inactive = int(line.split()[1]) * 1024\n            if (cached is not None and active is not None and inactive is not\n                None):\n                break\n        else:\n            raise RuntimeError('line(s) not found')\n    finally:\n        f.close()\n    avail = free + buffers + cached\n    used = total - free\n    percent = usage_percent(total - avail, total, _round=1)\n    return nt_virtmem_info(total, avail, percent, used, free, active,\n        inactive, buffers, cached)\n", "code_tokens": ["virtual", "memory", "total", "free", "buffers", "shared", "psutil", "linux", "get", "sysinfo", "cached", "active", "inactive", "none", "open", "proc", "meminfo", "try", "for", "line", "in", "if", "line", "startswith", "cached", "cached", "int", "line", "split", "1", "1024", "elif", "line", "startswith", "active", "active", "int", "line", "split", "1", "1024", "elif", "line", "startswith", "inactive", "inactive", "int", "line", "split", "1", "1024", "if", "cached", "is", "not", "none", "and", "active", "is", "not", "none", "and", "inactive", "is", "not", "none", "break", "else", "raise", "runtimeerror", "line", "not", "found", "finally", "close", "avail", "free", "buffers", "cached", "used", "total", "free", "percent", "usage", "percent", "total", "avail", "total", "round", "1", "return", "nt", "virtmem", "info", "total", "avail", "percent", "used", "free", "active", "inactive", "buffers", "cached"], "docstring": "memoize to disk  - persistent memoization", "docstring_tokens": ["memoize", "to", "disk", "persistent", "memoization"], "idx": 95}
{"url": "https://github.com/peri-source/peri/blob/61beed5deaaf978ab31ed716e8470d86ba639867/peri/util.py#L1055-L1114", "repo": "peri", "func_name": "newcache", "original_string": ["            print('\\r{lett:>{screen}}'.format(**{'lett':'', 'screen': self.screen}))\n", "\n", "\n", "# ============================================================================\n", "# useful decorators\n", "# ============================================================================\n", "import functools\n", "import types\n", "\n", "\n", "def newcache():\n", "    out = {}\n", "    out['hits'] = 0\n", "    out['misses'] = 0\n", "    out['size'] = 0\n", "    return out\n", "\n", "\n", "def memoize(cache_max_size=1e9):\n", "    def memoize_inner(obj):\n", "        cache_name = str(obj)\n", "\n", "        @functools.wraps(obj)\n", "        def wrapper(self, *args, **kwargs):\n", "            # add the memoize cache to the object first, if not present\n", "            # provide a method to the object to clear the cache too\n", "            if not hasattr(self, '_memoize_caches'):\n", "                def clear_cache(self):\n", "                    for k,v in iteritems(self._memoize_caches):\n", "                        self._memoize_caches[k] = newcache()\n", "                self._memoize_caches = {}\n", "                self._memoize_clear = types.MethodType(clear_cache, self)\n", "\n", "            # next, add the particular cache for this method if it does\n", "            # not already exist in the parent 'self'\n", "            cache = self._memoize_caches.get(cache_name)\n", "            if not cache:\n", "                cache = newcache()\n", "                self._memoize_caches[cache_name] = cache\n", "\n", "            size = 0\n", "            hashed = []\n", "\n", "            # let's hash the arguments (both args, kwargs) and be mindful of\n", "            # numpy arrays -- that is, only take care of its data, not the obj\n", "            # itself\n", "            for arg in args:\n", "                if isinstance(arg, np.ndarray):\n", "                    hashed.append(arg.tostring())\n", "                else:\n", "                    hashed.append(arg)\n", "            for k,v in iteritems(kwargs):\n", "                if isinstance(v, np.ndarray):\n", "                    hashed.append(v.tostring())\n", "                else:\n", "                    hashed.append(v)\n", "\n", "            hashed = tuple(hashed)\n", "            if hashed not in cache:\n", "                ans = obj(self, *args, **kwargs)\n"], "language": "python", "code": "def newcache():\n    out = {}\n    out['hits'] = 0\n    out['misses'] = 0\n    out['size'] = 0\n    return out\n", "code_tokens": ["newcache", "out", "out", "hits", "0", "out", "misses", "0", "out", "size", "0", "return", "out"], "docstring": "memoize to disk  - persistent memoization", "docstring_tokens": ["memoize", "to", "disk", "persistent", "memoization"], "idx": 96}
{"url": "https://github.com/fedora-python/pyp2rpm/blob/853eb3d226689a5ccdcdb9358b1a3394fafbd2b5/pyp2rpm/utils.py#L44-L56", "repo": "pyp2rpm", "func_name": "memoize_by_args", "original_string": ["def memoize_by_args(func):\n", "    \"\"\"Memoizes return value of a func based on args.\"\"\"\n", "    memory = {}\n", "\n", "    @functools.wraps(func)\n", "    def memoized(*args):\n", "        if args not in memory.keys():\n", "            value = func(*args)\n", "            memory[args] = value\n", "\n", "        return memory[args]\n", "\n", "    return memoized\n"], "language": "python", "code": "def memoize_by_args(func):\n    \"\"\"\"\"\"\n    memory = {}\n\n    @functools.wraps(func)\n    def memoized(*args):\n        if args not in memory.keys():\n            value = func(*args)\n            memory[args] = value\n        return memory[args]\n    return memoized\n", "code_tokens": ["memoize", "by", "args", "func", "memory", "functools", "wraps", "func", "def", "memoized", "args", "if", "args", "not", "in", "memory", "keys", "value", "func", "args", "memory", "args", "value", "return", "memory", "args", "return", "memoized"], "docstring": "memoize to disk  - persistent memoization", "docstring_tokens": ["memoize", "to", "disk", "persistent", "memoization"], "idx": 97}
{"url": "https://github.com/KelSolaar/Foundations/blob/5c141330faf09dad70a12bc321f4c564917d0a91/foundations/decorators.py#L80-L129", "repo": "Foundations", "func_name": "memoize", "original_string": ["def memoize(cache=None):\n", "    \"\"\"\n", "    | Implements method / definition memoization.\n", "    | Any method / definition decorated will get its return value cached and restored whenever called with the same arguments.\n", "\n", "    :param cache: Alternate cache.\n", "    :type cache: dict\n", "    :return: Object.\n", "    :rtype: object\n", "    \"\"\"\n", "\n", "    if cache is None:\n", "        cache = {}\n", "\n", "    def memoize_decorator(object):\n", "        \"\"\"\n", "        Implements method / definition memoization.\n", "\n", "        :param object: Object to decorate.\n", "        :type object: object\n", "        :return: Object.\n", "        :rtype: object\n", "        \"\"\"\n", "\n", "        @functools.wraps(object)\n", "        def memoize_wrapper(*args, **kwargs):\n", "            \"\"\"\n", "            Implements method / definition memoization.\n", "\n", "            :param \\*args: Arguments.\n", "            :type \\*args: \\*\n", "            :param \\*\\*kwargs: Keywords arguments.\n", "            :type \\*\\*kwargs: \\*\\*\n", "            :return: Object.\n", "            :rtype: object\n", "            \"\"\"\n", "\n", "            if kwargs:\n", "                key = args, frozenset(kwargs.iteritems())\n", "            else:\n", "                key = args\n", "\n", "            if key not in cache:\n", "                cache[key] = object(*args, **kwargs)\n", "\n", "            return cache[key]\n", "\n", "        return memoize_wrapper\n", "\n", "    return memoize_decorator\n"], "language": "python", "code": "def memoize(cache=None):\n    \"\"\"\"\"\"\n    if cache is None:\n        cache = {}\n\n    def memoize_decorator(object):\n        \"\"\"\n        Implements method / definition memoization.\n\n        :param object: Object to decorate.\n        :type object: object\n        :return: Object.\n        :rtype: object\n        \"\"\"\n\n        @functools.wraps(object)\n        def memoize_wrapper(*args, **kwargs):\n            \"\"\"\n            Implements method / definition memoization.\n\n            :param \\\\*args: Arguments.\n            :type \\\\*args: \\\\*\n            :param \\\\*\\\\*kwargs: Keywords arguments.\n            :type \\\\*\\\\*kwargs: \\\\*\\\\*\n            :return: Object.\n            :rtype: object\n            \"\"\"\n            if kwargs:\n                key = args, frozenset(kwargs.iteritems())\n            else:\n                key = args\n            if key not in cache:\n                cache[key] = object(*args, **kwargs)\n            return cache[key]\n        return memoize_wrapper\n    return memoize_decorator\n", "code_tokens": ["memoize", "cache", "none", "if", "cache", "is", "none", "cache", "def", "memoize", "decorator", "object", "implements", "method", "definition", "memoization", "param", "object", "object", "to", "decorate", "type", "object", "object", "return", "object", "rtype", "object", "functools", "wraps", "object", "def", "memoize", "wrapper", "args", "kwargs", "implements", "method", "definition", "memoization", "param", "args", "arguments", "type", "args", "param", "kwargs", "keywords", "arguments", "type", "kwargs", "return", "object", "rtype", "object", "if", "kwargs", "key", "args", "frozenset", "kwargs", "iteritems", "else", "key", "args", "if", "key", "not", "in", "cache", "cache", "key", "object", "args", "kwargs", "return", "cache", "key", "return", "memoize", "wrapper", "return", "memoize", "decorator"], "docstring": "memoize to disk  - persistent memoization", "docstring_tokens": ["memoize", "to", "disk", "persistent", "memoization"], "idx": 98}
{"url": "https://github.com/bram85/topydo/blob/b59fcfca5361869a6b78d4c9808c7c6cd0a18b58/topydo/lib/Utils.py#L28-L46", "repo": "topydo", "func_name": "date_string_to_date", "original_string": ["def date_string_to_date(p_date):\n", "    \"\"\"\n", "    Given a date in YYYY-MM-DD, returns a Python date object. Throws a\n", "    ValueError if the date is invalid.\n", "    \"\"\"\n", "    result = None\n", "\n", "    if p_date:\n", "        parsed_date = re.match(r'(\\d{4})-(\\d{2})-(\\d{2})', p_date)\n", "        if parsed_date:\n", "            result = date(\n", "                int(parsed_date.group(1)),  # year\n", "                int(parsed_date.group(2)),  # month\n", "                int(parsed_date.group(3))   # day\n", "            )\n", "        else:\n", "            raise ValueError\n", "\n", "    return result\n"], "language": "python", "code": "def date_string_to_date(p_date):\n    \"\"\"\"\"\"\n    result = None\n    if p_date:\n        parsed_date = re.match('(\\\\d{4})-(\\\\d{2})-(\\\\d{2})', p_date)\n        if parsed_date:\n            result = date(int(parsed_date.group(1)), int(parsed_date.group(\n                2)), int(parsed_date.group(3)))\n        else:\n            raise ValueError\n    return result\n", "code_tokens": ["date", "string", "to", "date", "date", "result", "none", "if", "date", "parsed", "date", "re", "match", "4", "2", "2", "date", "if", "parsed", "date", "result", "date", "int", "parsed", "date", "group", "1", "int", "parsed", "date", "group", "2", "int", "parsed", "date", "group", "3", "else", "raise", "valueerror", "return", "result"], "docstring": "convert a date string into yyyymmdd", "docstring_tokens": ["convert", "a", "date", "string", "into", "yyyymmdd"], "idx": 99}
{"url": "https://github.com/why2pac/dp-tornado/blob/a5948f5693f6ee2d9bab31f611fedc074e1caa96/dp_tornado/helper/datetime/time.py#L38-L53", "repo": "dp-tornado", "func_name": "convert", "original_string": ["    def convert(self,\n", "                auto=None,\n", "                datetime=None,\n", "                timezone=None,\n", "                timestamp=None,\n", "                yyyymmdd=None,\n", "                yyyymmddhhiiss=None,\n", "                ms=False):\n", "        return self.helper.datetime.convert(\n", "            auto=auto,\n", "            datetime=datetime,\n", "            timezone=timezone,\n", "            timestamp=timestamp,\n", "            yyyymmdd=yyyymmdd,\n", "            yyyymmddhhiiss=yyyymmddhhiiss,\n", "            ms=ms)\n"], "language": "python", "code": "def convert(self, auto=None, datetime=None, timezone=None, timestamp=None,\n    yyyymmdd=None, yyyymmddhhiiss=None, ms=False):\n    return self.helper.datetime.convert(auto=auto, datetime=datetime,\n        timezone=timezone, timestamp=timestamp, yyyymmdd=yyyymmdd,\n        yyyymmddhhiiss=yyyymmddhhiiss, ms=ms)\n", "code_tokens": ["convert", "self", "auto", "none", "datetime", "none", "timezone", "none", "timestamp", "none", "yyyymmdd", "none", "yyyymmddhhiiss", "none", "ms", "false", "return", "self", "helper", "datetime", "convert", "auto", "auto", "datetime", "datetime", "timezone", "timezone", "timestamp", "timestamp", "yyyymmdd", "yyyymmdd", "yyyymmddhhiiss", "yyyymmddhhiiss", "ms", "ms"], "docstring": "convert a date string into yyyymmdd", "docstring_tokens": ["convert", "a", "date", "string", "into", "yyyymmdd"], "idx": 100}
{"url": "https://github.com/gambogi/CSHLDAP/blob/09cb754b1e72437834e0d8cb4c7ac1830cfa6829/CSHLDAP.py#L336-L348", "repo": "CSHLDAP", "func_name": "dateFromLDAPTimestamp", "original_string": ["def dateFromLDAPTimestamp(timestamp):\n", "    \"\"\" Takes an LDAP date (In the form YYYYmmdd\n", "        with whatever is after that) and returns a\n", "        datetime.date object.\n", "    \"\"\"\n", "    # only check the first 8 characters: YYYYmmdd\n", "    numberOfCharacters = len(\"YYYYmmdd\")\n", "    timestamp = timestamp[:numberOfCharacters]\n", "    try:\n", "        day = datetime.strptime(timestamp, '%Y%m%d')\n", "        return date(year=day.year, month=day.month, day=day.day)\n", "    except:\n", "        print(timestamp)\n"], "language": "python", "code": "def dateFromLDAPTimestamp(timestamp):\n    \"\"\"\"\"\"\n    numberOfCharacters = len('YYYYmmdd')\n    timestamp = timestamp[:numberOfCharacters]\n    try:\n        day = datetime.strptime(timestamp, '%Y%m%d')\n        return date(year=day.year, month=day.month, day=day.day)\n    except:\n        print(timestamp)\n", "code_tokens": ["datefromldaptimestamp", "timestamp", "numberofcharacters", "len", "yyyymmdd", "timestamp", "timestamp", "numberofcharacters", "try", "day", "datetime", "strptime", "timestamp", "return", "date", "year", "day", "year", "month", "day", "month", "day", "day", "day", "except", "print", "timestamp"], "docstring": "convert a date string into yyyymmdd", "docstring_tokens": ["convert", "a", "date", "string", "into", "yyyymmdd"], "idx": 101}
{"url": "https://github.com/google/transitfeed/blob/eb2991a3747ba541b2cb66502b305b6304a1f85f/merge.py#L1166-L1206", "repo": "transitfeed", "func_name": "DisjoinCalendars", "original_string": ["  def DisjoinCalendars(self, cutoff):\n", "    \"\"\"Forces the old and new calendars to be disjoint about a cutoff date.\n", "\n", "    This truncates the service periods of the old schedule so that service\n", "    stops one day before the given cutoff date and truncates the new schedule\n", "    so that service only begins on the cutoff date.\n", "\n", "    Args:\n", "      cutoff: The cutoff date as a string in YYYYMMDD format. The timezone\n", "              is the same as used in the calendar.txt file.\n", "    \"\"\"\n", "\n", "    def TruncatePeriod(service_period, start, end):\n", "      \"\"\"Truncate the service period to into the range [start, end].\n", "\n", "      Args:\n", "        service_period: The service period to truncate.\n", "        start: The start date as a string in YYYYMMDD format.\n", "        end: The end date as a string in YYYYMMDD format.\n", "      \"\"\"\n", "      service_period.start_date = max(service_period.start_date, start)\n", "      service_period.end_date = min(service_period.end_date, end)\n", "      dates_to_delete = []\n", "      for k in service_period.date_exceptions:\n", "        if (k < start) or (k > end):\n", "          dates_to_delete.append(k)\n", "      for k in dates_to_delete:\n", "        del service_period.date_exceptions[k]\n", "\n", "    # find the date one day before cutoff\n", "    year = int(cutoff[:4])\n", "    month = int(cutoff[4:6])\n", "    day = int(cutoff[6:8])\n", "    cutoff_date = datetime.date(year, month, day)\n", "    one_day_delta = datetime.timedelta(days=1)\n", "    before = (cutoff_date - one_day_delta).strftime('%Y%m%d')\n", "\n", "    for a in self.feed_merger.a_schedule.GetServicePeriodList():\n", "      TruncatePeriod(a, 0, before)\n", "    for b in self.feed_merger.b_schedule.GetServicePeriodList():\n", "      TruncatePeriod(b, cutoff, '9'*8)\n"], "language": "python", "code": "def DisjoinCalendars(self, cutoff):\n    \"\"\"\"\"\"\n\n    def TruncatePeriod(service_period, start, end):\n        \"\"\"Truncate the service period to into the range [start, end].\n\n      Args:\n        service_period: The service period to truncate.\n        start: The start date as a string in YYYYMMDD format.\n        end: The end date as a string in YYYYMMDD format.\n      \"\"\"\n        service_period.start_date = max(service_period.start_date, start)\n        service_period.end_date = min(service_period.end_date, end)\n        dates_to_delete = []\n        for k in service_period.date_exceptions:\n            if k < start or k > end:\n                dates_to_delete.append(k)\n        for k in dates_to_delete:\n            del service_period.date_exceptions[k]\n    year = int(cutoff[:4])\n    month = int(cutoff[4:6])\n    day = int(cutoff[6:8])\n    cutoff_date = datetime.date(year, month, day)\n    one_day_delta = datetime.timedelta(days=1)\n    before = (cutoff_date - one_day_delta).strftime('%Y%m%d')\n    for a in self.feed_merger.a_schedule.GetServicePeriodList():\n        TruncatePeriod(a, 0, before)\n    for b in self.feed_merger.b_schedule.GetServicePeriodList():\n        TruncatePeriod(b, cutoff, '9' * 8)\n", "code_tokens": ["disjoincalendars", "self", "cutoff", "def", "truncateperiod", "service", "period", "start", "end", "truncate", "the", "service", "period", "to", "into", "the", "range", "start", "end", "args", "service", "period", "the", "service", "period", "to", "truncate", "start", "the", "start", "date", "as", "string", "in", "yyyymmdd", "format", "end", "the", "end", "date", "as", "string", "in", "yyyymmdd", "format", "service", "period", "start", "date", "max", "service", "period", "start", "date", "start", "service", "period", "end", "date", "min", "service", "period", "end", "date", "end", "dates", "to", "delete", "for", "in", "service", "period", "date", "exceptions", "if", "start", "or", "end", "dates", "to", "delete", "append", "for", "in", "dates", "to", "delete", "del", "service", "period", "date", "exceptions", "year", "int", "cutoff", "4", "month", "int", "cutoff", "4", "6", "day", "int", "cutoff", "6", "8", "cutoff", "date", "datetime", "date", "year", "month", "day", "one", "day", "delta", "datetime", "timedelta", "days", "1", "before", "cutoff", "date", "one", "day", "delta", "strftime", "for", "in", "self", "feed", "merger", "schedule", "getserviceperiodlist", "truncateperiod", "0", "before", "for", "in", "self", "feed", "merger", "schedule", "getserviceperiodlist", "truncateperiod", "cutoff", "9", "8"], "docstring": "convert a date string into yyyymmdd", "docstring_tokens": ["convert", "a", "date", "string", "into", "yyyymmdd"], "idx": 102}
{"url": "https://github.com/why2pac/dp-tornado/blob/a5948f5693f6ee2d9bab31f611fedc074e1caa96/dp_tornado/helper/datetime/date.py#L74-L76", "repo": "dp-tornado", "func_name": "yyyymmdd", "original_string": ["    def yyyymmdd(self, auto=None, datetime=None, timezone=None, timestamp=None, ms=False, concat=''):\n", "        datetime = self.convert(auto=auto, datetime=datetime, timezone=timezone, timestamp=timestamp, ms=ms)\n", "        return '%04d%s%02d%s%02d' % (datetime.year, concat, datetime.month, concat, datetime.day)\n"], "language": "python", "code": "def yyyymmdd(self, auto=None, datetime=None, timezone=None, timestamp=None,\n    ms=False, concat=''):\n    datetime = self.convert(auto=auto, datetime=datetime, timezone=timezone,\n        timestamp=timestamp, ms=ms)\n    return '%04d%s%02d%s%02d' % (datetime.year, concat, datetime.month,\n        concat, datetime.day)\n", "code_tokens": ["yyyymmdd", "self", "auto", "none", "datetime", "none", "timezone", "none", "timestamp", "none", "ms", "false", "concat", "datetime", "self", "convert", "auto", "auto", "datetime", "datetime", "timezone", "timezone", "timestamp", "timestamp", "ms", "ms", "return", "datetime", "year", "concat", "datetime", "month", "concat", "datetime", "day"], "docstring": "convert a date string into yyyymmdd", "docstring_tokens": ["convert", "a", "date", "string", "into", "yyyymmdd"], "idx": 103}
{"url": "https://github.com/mushkevych/scheduler/blob/6740331360f49083c208085fb5a60ce80ebf418b/synergy/system/time_helper.py#L78-L82", "repo": "scheduler", "func_name": "day_to_month", "original_string": ["def day_to_month(timeperiod):\n", "    \"\"\":param timeperiod: as string in YYYYMMDD00 format\n", "    :return string in YYYYMM0000 format\"\"\"\n", "    t = datetime.strptime(timeperiod, SYNERGY_DAILY_PATTERN)\n", "    return t.strftime(SYNERGY_MONTHLY_PATTERN)\n"], "language": "python", "code": "def day_to_month(timeperiod):\n    \"\"\"\"\"\"\n    t = datetime.strptime(timeperiod, SYNERGY_DAILY_PATTERN)\n    return t.strftime(SYNERGY_MONTHLY_PATTERN)\n", "code_tokens": ["day", "to", "month", "timeperiod", "datetime", "strptime", "timeperiod", "synergy", "daily", "pattern", "return", "strftime", "synergy", "monthly", "pattern"], "docstring": "convert a date string into yyyymmdd", "docstring_tokens": ["convert", "a", "date", "string", "into", "yyyymmdd"], "idx": 104}
{"url": "https://github.com/QualiSystems/vCenterShell/blob/e2e24cd938a92a68f4a8e6a860810d3ef72aae6d/package/cloudshell/cp/vcenter/commands/save_sandbox.py#L38-L91", "repo": "vCenterShell", "func_name": "save_app", "original_string": ["    def save_app(self, si, logger, vcenter_data_model, reservation_id, save_app_actions, cancellation_context):\n", "        \"\"\"\n", "        Cretaes an artifact of an app, that can later be restored\n", "\n", "        :param vcenter_data_model: VMwarevCenterResourceModel\n", "        :param vim.ServiceInstance si: py_vmomi service instance\n", "        :type si: vim.ServiceInstance\n", "        :param logger: Logger\n", "        :type logger: cloudshell.core.logger.qs_logger.get_qs_logger\n", "        :param list[SaveApp] save_app_actions:\n", "        :param cancellation_context:\n", "        \"\"\"\n", "        results = []\n", "\n", "        logger.info('Save Sandbox command starting on ' + vcenter_data_model.default_datacenter)\n", "\n", "        if not save_app_actions:\n", "            raise Exception('Failed to save app, missing data in request.')\n", "\n", "        actions_grouped_by_save_types = groupby(save_app_actions, lambda x: x.actionParams.saveDeploymentModel)\n", "        # artifactSaver or artifactHandler are different ways to save artifacts. For example, currently\n", "        # we clone a vm, thenk take a snapshot. restore will be to deploy from linked snapshot\n", "        # a future artifact handler we might develop is save vm to OVF file and restore from file.\n", "        artifactSaversToActions = {ArtifactHandler.factory(k,\n", "                                                           self.pyvmomi_service,\n", "                                                           vcenter_data_model,\n", "                                                           si,\n", "                                                           logger,\n", "                                                           self.deployer,\n", "                                                           reservation_id,\n", "                                                           self.resource_model_parser,\n", "                                                           self.snapshot_saver,\n", "                                                           self.task_waiter,\n", "                                                           self.folder_manager,\n", "                                                           self.port_group_configurer,\n", "                                                           self.cs)\n", "                                   : list(g)\n", "                                   for k, g in actions_grouped_by_save_types}\n", "\n", "        self.validate_requested_save_types_supported(artifactSaversToActions,\n", "                                                     logger,\n", "                                                     results)\n", "\n", "        error_results = [r for r in results if not r.success]\n", "        if not error_results:\n", "            logger.info('Handling Save App requests')\n", "            results = self._execute_save_actions_using_pool(artifactSaversToActions,\n", "                                                            cancellation_context,\n", "                                                            logger,\n", "                                                            results)\n", "            logger.info('Completed Save Sandbox command')\n", "        else:\n", "            logger.error('Some save app requests were not valid, Save Sandbox command failed.')\n", "        return results\n"], "language": "python", "code": "def save_app(self, si, logger, vcenter_data_model, reservation_id,\n    save_app_actions, cancellation_context):\n    \"\"\"\"\"\"\n    results = []\n    logger.info('Save Sandbox command starting on ' + vcenter_data_model.\n        default_datacenter)\n    if not save_app_actions:\n        raise Exception('Failed to save app, missing data in request.')\n    actions_grouped_by_save_types = groupby(save_app_actions, lambda x: x.\n        actionParams.saveDeploymentModel)\n    artifactSaversToActions = {ArtifactHandler.factory(k, self.\n        pyvmomi_service, vcenter_data_model, si, logger, self.deployer,\n        reservation_id, self.resource_model_parser, self.snapshot_saver,\n        self.task_waiter, self.folder_manager, self.port_group_configurer,\n        self.cs): list(g) for k, g in actions_grouped_by_save_types}\n    self.validate_requested_save_types_supported(artifactSaversToActions,\n        logger, results)\n    error_results = [r for r in results if not r.success]\n    if not error_results:\n        logger.info('Handling Save App requests')\n        results = self._execute_save_actions_using_pool(artifactSaversToActions\n            , cancellation_context, logger, results)\n        logger.info('Completed Save Sandbox command')\n    else:\n        logger.error(\n            'Some save app requests were not valid, Save Sandbox command failed.'\n            )\n    return results\n", "code_tokens": ["save", "app", "self", "si", "logger", "vcenter", "data", "model", "reservation", "id", "save", "app", "actions", "cancellation", "context", "results", "logger", "info", "save", "sandbox", "command", "starting", "on", "vcenter", "data", "model", "default", "datacenter", "if", "not", "save", "app", "actions", "raise", "exception", "failed", "to", "save", "app", "missing", "data", "in", "request", "actions", "grouped", "by", "save", "types", "groupby", "save", "app", "actions", "lambda", "actionparams", "savedeploymentmodel", "artifactsaverstoactions", "artifacthandler", "factory", "self", "pyvmomi", "service", "vcenter", "data", "model", "si", "logger", "self", "deployer", "reservation", "id", "self", "resource", "model", "parser", "self", "snapshot", "saver", "self", "task", "waiter", "self", "folder", "manager", "self", "port", "group", "configurer", "self", "cs", "list", "for", "in", "actions", "grouped", "by", "save", "types", "self", "validate", "requested", "save", "types", "supported", "artifactsaverstoactions", "logger", "results", "error", "results", "for", "in", "results", "if", "not", "success", "if", "not", "error", "results", "logger", "info", "handling", "save", "app", "requests", "results", "self", "execute", "save", "actions", "using", "pool", "artifactsaverstoactions", "cancellation", "context", "logger", "results", "logger", "info", "completed", "save", "sandbox", "command", "else", "logger", "error", "some", "save", "app", "requests", "were", "not", "valid", "save", "sandbox", "command", "failed", "return", "results"], "docstring": "save list to file", "docstring_tokens": ["save", "list", "to", "file"], "idx": 105}
{"url": "https://github.com/StanfordBioinformatics/loom/blob/db2031a1a87124fee1aeb7414a668c03d774a698/worker/loomengine_worker/outputs.py#L28-L35", "repo": "loom", "func_name": "save", "original_string": ["    def save(self):\n", "        filename_list = self.output['source']['filenames']\n", "        file_path_list = [\n", "            os.path.join(\n", "                self.working_dir, filename)\n", "            for filename in filename_list]\n", "        self.import_manager.import_result_file_list(\n", "            self.output, file_path_list, retry=True)\n"], "language": "python", "code": "def save(self):\n    filename_list = self.output['source']['filenames']\n    file_path_list = [os.path.join(self.working_dir, filename) for filename in\n        filename_list]\n    self.import_manager.import_result_file_list(self.output, file_path_list,\n        retry=True)\n", "code_tokens": ["save", "self", "filename", "list", "self", "output", "source", "filenames", "file", "path", "list", "os", "path", "join", "self", "working", "dir", "filename", "for", "filename", "in", "filename", "list", "self", "import", "manager", "import", "result", "file", "list", "self", "output", "file", "path", "list", "retry", "true"], "docstring": "save list to file", "docstring_tokens": ["save", "list", "to", "file"], "idx": 106}
{"url": "https://github.com/ioam/lancet/blob/1fbbf88fa0e8974ff9ed462e3cb11722ddebdd6e/lancet/filetypes.py#L264-L267", "repo": "lancet", "func_name": "save", "original_string": ["    def save(self, filename, metadata={}, **data):\n", "        super(NumpyFile, self).save(filename, metadata, **data)\n", "        savefn = numpy.savez_compressed if self.compress else numpy.savez\n", "        savefn(self._savepath(filename), metadata=metadata, **data)\n"], "language": "python", "code": "def save(self, filename, metadata={}, **data):\n    super(NumpyFile, self).save(filename, metadata, **data)\n    savefn = numpy.savez_compressed if self.compress else numpy.savez\n    savefn(self._savepath(filename), metadata=metadata, **data)\n", "code_tokens": ["save", "self", "filename", "metadata", "data", "super", "numpyfile", "self", "save", "filename", "metadata", "data", "savefn", "numpy", "savez", "compressed", "if", "self", "compress", "else", "numpy", "savez", "savefn", "self", "savepath", "filename", "metadata", "metadata", "data"], "docstring": "save list to file", "docstring_tokens": ["save", "list", "to", "file"], "idx": 107}
{"url": "https://github.com/StanfordBioinformatics/loom/blob/db2031a1a87124fee1aeb7414a668c03d774a698/worker/loomengine_worker/outputs.py#L71-L83", "repo": "loom", "func_name": "save", "original_string": ["    def save(self):\n", "        filename_list = self.output['source']['filenames']\n", "        if not isinstance(filename_list, list):\n", "            filename_list = filename_list.split(' ')\n", "        contents_list = []\n", "        for filename in filename_list:\n", "            file_path = os.path.join(\n", "                self.working_dir, filename)\n", "            contents_list.append(self._read_file(file_path))\n", "        self.output.update({'data': {'contents': contents_list}})\n", "        self.connection.update_task_attempt_output(\n", "            self.output['uuid'],\n", "            self.output)\n"], "language": "python", "code": "def save(self):\n    filename_list = self.output['source']['filenames']\n    if not isinstance(filename_list, list):\n        filename_list = filename_list.split(' ')\n    contents_list = []\n    for filename in filename_list:\n        file_path = os.path.join(self.working_dir, filename)\n        contents_list.append(self._read_file(file_path))\n    self.output.update({'data': {'contents': contents_list}})\n    self.connection.update_task_attempt_output(self.output['uuid'], self.output\n        )\n", "code_tokens": ["save", "self", "filename", "list", "self", "output", "source", "filenames", "if", "not", "isinstance", "filename", "list", "list", "filename", "list", "filename", "list", "split", "contents", "list", "for", "filename", "in", "filename", "list", "file", "path", "os", "path", "join", "self", "working", "dir", "filename", "contents", "list", "append", "self", "read", "file", "file", "path", "self", "output", "update", "data", "contents", "contents", "list", "self", "connection", "update", "task", "attempt", "output", "self", "output", "uuid", "self", "output"], "docstring": "save list to file", "docstring_tokens": ["save", "list", "to", "file"], "idx": 108}
{"url": "https://github.com/acutesoftware/AIKIF/blob/fcf1582dc5f884b9a4fa7c6e20e9de9d94d21d03/aikif/lib/cls_filelist.py#L192-L228", "repo": "AIKIF", "func_name": "save_filelist", "original_string": ["    def save_filelist(self, opFile, opFormat, delim=',', qu='\"'):\n", "        \"\"\"\n", "        uses a List of files and collects meta data on them and saves\n", "        to an text file as a list or with metadata depending on opFormat.\n", "        \"\"\"\n", "\n", "        op_folder = os.path.dirname(opFile)\n", "        if op_folder is not None:   # short filename passed\n", "            if not os.path.exists(op_folder):\n", "                os.makedirs(op_folder)\n", "\n", "\n", "        with open(opFile,'w') as fout:\n", "            fout.write(\"fullFilename\" + delim)\n", "            for colHeading in opFormat:\n", "                fout.write(colHeading + delim)\n", "            fout.write('\\n')\n", "            for f in self.filelist:\n", "                line = qu + f + qu + delim\n", "                try:\n", "                    for fld in opFormat:\n", "                        if fld == \"name\":\n", "                            line = line + qu + os.path.basename(f) + qu + delim\n", "                        if fld == \"date\":\n", "                            line = line + qu + self.GetDateAsString(f) + qu + delim\n", "                        if fld == \"size\":\n", "                            line = line + qu + str(os.path.getsize(f)) + qu + delim\n", "                        if fld == \"path\":\n", "                            line = line + qu + os.path.dirname(f) + qu + delim\n", "                except IOError:\n", "                    line += '\\n'   # no metadata\n", "                try:\n", "                    fout.write (str(line.encode('ascii', 'ignore').decode('utf-8')))\n", "                    fout.write ('\\n')\n", "                except IOError:\n", "                    #print(\"Cant print line - cls_filelist line 304\")\n", "                    pass\n"], "language": "python", "code": "def save_filelist(self, opFile, opFormat, delim=',', qu='\"'):\n    \"\"\"\"\"\"\n    op_folder = os.path.dirname(opFile)\n    if op_folder is not None:\n        if not os.path.exists(op_folder):\n            os.makedirs(op_folder)\n    with open(opFile, 'w') as fout:\n        fout.write('fullFilename' + delim)\n        for colHeading in opFormat:\n            fout.write(colHeading + delim)\n        fout.write('\\n')\n        for f in self.filelist:\n            line = qu + f + qu + delim\n            try:\n                for fld in opFormat:\n                    if fld == 'name':\n                        line = line + qu + os.path.basename(f) + qu + delim\n                    if fld == 'date':\n                        line = line + qu + self.GetDateAsString(f) + qu + delim\n                    if fld == 'size':\n                        line = line + qu + str(os.path.getsize(f)) + qu + delim\n                    if fld == 'path':\n                        line = line + qu + os.path.dirname(f) + qu + delim\n            except IOError:\n                line += '\\n'\n            try:\n                fout.write(str(line.encode('ascii', 'ignore').decode('utf-8')))\n                fout.write('\\n')\n            except IOError:\n                pass\n", "code_tokens": ["save", "filelist", "self", "opfile", "opformat", "delim", "qu", "op", "folder", "os", "path", "dirname", "opfile", "if", "op", "folder", "is", "not", "none", "if", "not", "os", "path", "exists", "op", "folder", "os", "makedirs", "op", "folder", "with", "open", "opfile", "as", "fout", "fout", "write", "fullfilename", "delim", "for", "colheading", "in", "opformat", "fout", "write", "colheading", "delim", "fout", "write", "for", "in", "self", "filelist", "line", "qu", "qu", "delim", "try", "for", "fld", "in", "opformat", "if", "fld", "name", "line", "line", "qu", "os", "path", "basename", "qu", "delim", "if", "fld", "date", "line", "line", "qu", "self", "getdateasstring", "qu", "delim", "if", "fld", "size", "line", "line", "qu", "str", "os", "path", "getsize", "qu", "delim", "if", "fld", "path", "line", "line", "qu", "os", "path", "dirname", "qu", "delim", "except", "ioerror", "line", "try", "fout", "write", "str", "line", "encode", "ascii", "ignore", "decode", "utf", "8", "fout", "write", "except", "ioerror", "pass"], "docstring": "save list to file", "docstring_tokens": ["save", "list", "to", "file"], "idx": 109}
{"url": "https://github.com/geometalab/pyGeoTile/blob/b1f44271698f5fc4d18c2add935797ed43254aa6/pygeotile/point.py#L12-L16", "repo": "pyGeoTile", "func_name": "from_latitude_longitude", "original_string": ["    def from_latitude_longitude(cls, latitude=0.0, longitude=0.0):\n", "        \"\"\"Creates a point from lat/lon in WGS84\"\"\"\n", "        assert -180.0 <= longitude <= 180.0, 'Longitude needs to be a value between -180.0 and 180.0.'\n", "        assert -90.0 <= latitude <= 90.0, 'Latitude needs to be a value between -90.0 and 90.0.'\n", "        return cls(latitude=latitude, longitude=longitude)\n"], "language": "python", "code": "def from_latitude_longitude(cls, latitude=0.0, longitude=0.0):\n    \"\"\"\"\"\"\n    assert -180.0 <= longitude <= 180.0, 'Longitude needs to be a value between -180.0 and 180.0.'\n    assert -90.0 <= latitude <= 90.0, 'Latitude needs to be a value between -90.0 and 90.0.'\n    return cls(latitude=latitude, longitude=longitude)\n", "code_tokens": ["from", "latitude", "longitude", "cls", "latitude", "0", "0", "longitude", "0", "0", "assert", "180", "0", "longitude", "180", "0", "longitude", "needs", "to", "be", "value", "between", "180", "0", "and", "180", "0", "assert", "90", "0", "latitude", "90", "0", "latitude", "needs", "to", "be", "value", "between", "90", "0", "and", "90", "0", "return", "cls", "latitude", "latitude", "longitude", "longitude"], "docstring": "extract latitude and longitude from given input", "docstring_tokens": ["extract", "latitude", "and", "longitude", "from", "given", "input"], "idx": 110}
{"url": "https://github.com/nickmckay/LiPD-utilities/blob/5dab6bbeffc5effd68e3a6beaca6b76aa928e860/Python/lipd/excel.py#L1227-L1269", "repo": "LiPD-utilities", "func_name": "compile_geometry", "original_string": ["\n", "\n", "def compile_geometry(lat, lon, elev):\n", "    \"\"\"\n", "    Take in lists of lat and lon coordinates, and determine what geometry to create\n", "    :param list lat: Latitude values\n", "    :param list lon: Longitude values\n", "    :param float elev: Elevation value\n", "    :return dict:\n", "    \"\"\"\n", "    logger_excel.info(\"enter compile_geometry\")\n", "    lat = _remove_geo_placeholders(lat)\n", "    lon = _remove_geo_placeholders(lon)\n", "\n", "    # 4 coordinate values\n", "    if len(lat) == 2 and len(lon) == 2:\n", "        logger_excel.info(\"found 4 coordinates\")\n", "        geo_dict = geometry_linestring(lat, lon, elev)\n", "\n", "        # # 4 coordinate values\n", "        # if (lat[0] != lat[1]) and (lon[0] != lon[1]):\n", "        #     geo_dict = geometry_polygon(lat, lon)\n", "        # # 3 unique coordinates\n", "        # else:\n", "        #     geo_dict = geometry_multipoint(lat, lon)\n", "        #\n", "\n", "    # 2 coordinate values\n", "    elif len(lat) == 1 and len(lon) == 1:\n", "        logger_excel.info(\"found 2 coordinates\")\n", "        geo_dict = geometry_point(lat, lon, elev)\n", "\n", "    # coordinate range. one value given but not the other.\n", "    elif (None in lon and None not in lat) or (len(lat) > 0 and len(lon) == 0):\n", "        geo_dict = geometry_range(lat, elev, \"lat\")\n", "\n", "    elif (None in lat and None not in lon) or (len(lon) > 0 and len(lat) == 0):\n", "        geo_dict = geometry_range(lat, elev, \"lon\")\n", "\n", "    # Too many points, or no points\n", "    else:\n", "        geo_dict = {}\n", "        logger_excel.warn(\"compile_geometry: invalid coordinates: lat: {}, lon: {}\".format(lat, lon))\n"], "language": "python", "code": "def compile_geometry(lat, lon, elev):\n    \"\"\"\"\"\"\n    logger_excel.info('enter compile_geometry')\n    lat = _remove_geo_placeholders(lat)\n    lon = _remove_geo_placeholders(lon)\n    if len(lat) == 2 and len(lon) == 2:\n        logger_excel.info('found 4 coordinates')\n        geo_dict = geometry_linestring(lat, lon, elev)\n    elif len(lat) == 1 and len(lon) == 1:\n        logger_excel.info('found 2 coordinates')\n        geo_dict = geometry_point(lat, lon, elev)\n    elif None in lon and None not in lat or len(lat) > 0 and len(lon) == 0:\n        geo_dict = geometry_range(lat, elev, 'lat')\n    elif None in lat and None not in lon or len(lon) > 0 and len(lat) == 0:\n        geo_dict = geometry_range(lat, elev, 'lon')\n    else:\n        geo_dict = {}\n        logger_excel.warn(\n            'compile_geometry: invalid coordinates: lat: {}, lon: {}'.\n            format(lat, lon))\n", "code_tokens": ["compile", "geometry", "lat", "lon", "elev", "logger", "excel", "info", "enter", "compile", "geometry", "lat", "remove", "geo", "placeholders", "lat", "lon", "remove", "geo", "placeholders", "lon", "if", "len", "lat", "2", "and", "len", "lon", "2", "logger", "excel", "info", "found", "4", "coordinates", "geo", "dict", "geometry", "linestring", "lat", "lon", "elev", "elif", "len", "lat", "1", "and", "len", "lon", "1", "logger", "excel", "info", "found", "2", "coordinates", "geo", "dict", "geometry", "point", "lat", "lon", "elev", "elif", "none", "in", "lon", "and", "none", "not", "in", "lat", "or", "len", "lat", "0", "and", "len", "lon", "0", "geo", "dict", "geometry", "range", "lat", "elev", "lat", "elif", "none", "in", "lat", "and", "none", "not", "in", "lon", "or", "len", "lon", "0", "and", "len", "lat", "0", "geo", "dict", "geometry", "range", "lat", "elev", "lon", "else", "geo", "dict", "logger", "excel", "warn", "compile", "geometry", "invalid", "coordinates", "lat", "lon", "format", "lat", "lon"], "docstring": "extract latitude and longitude from given input", "docstring_tokens": ["extract", "latitude", "and", "longitude", "from", "given", "input"], "idx": 111}
{"url": "https://github.com/biocore/burrito-fillings/blob/02ab71a46119b40793bd56a4ae00ca15f6dc3329/bfillings/mothur.py#L391-L399", "repo": "burrito-fillings", "func_name": "_set_WorkingDir", "original_string": ["    def _set_WorkingDir(self, path):\n", "        \"\"\"Sets the working directory\n", "        \"\"\"\n", "        self._curr_working_dir = path\n", "        try:\n", "            mkdir(self.WorkingDir)\n", "        except OSError:\n", "            # Directory already exists\n", "            pass\n"], "language": "python", "code": "def _set_WorkingDir(self, path):\n    \"\"\"\"\"\"\n    self._curr_working_dir = path\n    try:\n        mkdir(self.WorkingDir)\n    except OSError:\n        pass\n", "code_tokens": ["set", "workingdir", "self", "path", "self", "curr", "working", "dir", "path", "try", "mkdir", "self", "workingdir", "except", "oserror", "pass"], "docstring": "set working directory", "docstring_tokens": ["set", "working", "directory"], "idx": 112}
{"url": "https://github.com/xolox/python-vcs-repo-mgr/blob/fdad2441a3e7ba5deeeddfa1c2f5ebc00c393aed/vcs_repo_mgr/__init__.py#L1875-L1896", "repo": "python-vcs-repo-mgr", "func_name": "update_context", "original_string": ["    def update_context(self):\n", "        \"\"\"\n", "        Try to ensure that external commands are executed in the local repository.\n", "\n", "        What :func:`update_context()` does depends on whether the directory\n", "        given by :attr:`local` exists:\n", "\n", "        - If :attr:`local` exists then the working directory of :attr:`context`\n", "          will be set to :attr:`local`. This is to ensure that version control\n", "          commands are run inside of the intended version control repository.\n", "\n", "        - If :attr:`local` doesn't exist then the working directory of\n", "          :attr:`context` is cleared. This avoids external commands from\n", "          failing due to an invalid (non existing) working directory.\n", "        \"\"\"\n", "        if self.context.is_directory(self.local):\n", "            # Set the working directory of the execution context\n", "            # to the directory containing the local repository.\n", "            self.context.options['directory'] = self.local\n", "        else:\n", "            # Clear the execution context's working directory.\n", "            self.context.options.pop('directory', None)\n"], "language": "python", "code": "def update_context(self):\n    \"\"\"\"\"\"\n    if self.context.is_directory(self.local):\n        self.context.options['directory'] = self.local\n    else:\n        self.context.options.pop('directory', None)\n", "code_tokens": ["update", "context", "self", "if", "self", "context", "is", "directory", "self", "local", "self", "context", "options", "directory", "self", "local", "else", "self", "context", "options", "pop", "directory", "none"], "docstring": "set working directory", "docstring_tokens": ["set", "working", "directory"], "idx": 113}
{"url": "https://github.com/rshipp/python-dshield/blob/1b003d0dfac0bc2ee8b86ca5f1a44b765b8cc6e0/dshield.py#L57-L74", "repo": "python-dshield", "func_name": "ip", "original_string": ["def ip(ip_address, return_format=None):\n", "    \"\"\"Returns a summary of the information our database holds for a\n", "    particular IP address (similar to /ipinfo.html).\n", "\n", "    In the returned data:\n", "\n", "    Count: (also reports or records) total number of packets blocked from\n", "    this IP.\n", "    Attacks: (also targets) number of unique destination IP addresses for\n", "    these packets.\n", "\n", "    :param ip_address: a valid IP address\n", "    \"\"\"\n", "    response = _get('ip/{address}'.format(address=ip_address), return_format)\n", "    if 'bad IP address' in str(response):\n", "        raise Error('Bad IP address, {address}'.format(address=ip_address))\n", "    else:\n", "        return response\n"], "language": "python", "code": "def ip(ip_address, return_format=None):\n    \"\"\"\"\"\"\n    response = _get('ip/{address}'.format(address=ip_address), return_format)\n    if 'bad IP address' in str(response):\n        raise Error('Bad IP address, {address}'.format(address=ip_address))\n    else:\n        return response\n", "code_tokens": ["ip", "ip", "address", "return", "format", "none", "response", "get", "ip", "address", "format", "address", "ip", "address", "return", "format", "if", "bad", "ip", "address", "in", "str", "response", "raise", "error", "bad", "ip", "address", "address", "format", "address", "ip", "address", "else", "return", "response"], "docstring": "get current ip address", "docstring_tokens": ["get", "current", "ip", "address"], "idx": 114}
{"url": "https://github.com/douban/pymesos/blob/c8096d3d510648649963b7cead01d4d06b5dcf26/pymesos/scheduler.py#L473-L490", "repo": "pymesos", "func_name": "onNewMasterDetectedMessage", "original_string": ["    def onNewMasterDetectedMessage(self, data):\n", "        master = None\n", "        try:\n", "            if isinstance(data, six.binary_type):\n", "                data = data.decode('utf-8')\n", "\n", "            parsed = json.loads(data)\n", "            if parsed and \"address\" in parsed:\n", "                ip = parsed[\"address\"].get(\"ip\")\n", "                port = parsed[\"address\"].get(\"port\")\n", "                if ip and port:\n", "                    master = \"%s:%s\" % (ip, port)\n", "        except Exception:\n", "            logger.exception(\"No JSON content, probably connecting \"\n", "                             \"to older Mesos version.\")\n", "\n", "        if master:\n", "            self.change_master(master)\n"], "language": "python", "code": "def onNewMasterDetectedMessage(self, data):\n    master = None\n    try:\n        if isinstance(data, six.binary_type):\n            data = data.decode('utf-8')\n        parsed = json.loads(data)\n        if parsed and 'address' in parsed:\n            ip = parsed['address'].get('ip')\n            port = parsed['address'].get('port')\n            if ip and port:\n                master = '%s:%s' % (ip, port)\n    except Exception:\n        logger.exception(\n            'No JSON content, probably connecting to older Mesos version.')\n    if master:\n        self.change_master(master)\n", "code_tokens": ["onnewmasterdetectedmessage", "self", "data", "master", "none", "try", "if", "isinstance", "data", "six", "binary", "type", "data", "data", "decode", "utf", "8", "parsed", "json", "loads", "data", "if", "parsed", "and", "address", "in", "parsed", "ip", "parsed", "address", "get", "ip", "port", "parsed", "address", "get", "port", "if", "ip", "and", "port", "master", "ip", "port", "except", "exception", "logger", "exception", "no", "json", "content", "probably", "connecting", "to", "older", "mesos", "version", "if", "master", "self", "change", "master", "master"], "docstring": "get current ip address", "docstring_tokens": ["get", "current", "ip", "address"], "idx": 115}
{"url": "https://github.com/danielfrg/datasciencebox/blob/6b7aa642c6616a46547035fcb815acc1de605a6f/datasciencebox/core/cloud/instance.py#L80-L83", "repo": "datasciencebox", "func_name": "get_ip", "original_string": ["    def get_ip(self):\n", "        if self._ip is None:\n", "            self._ip = self.fetch_ip()\n", "        return self._ip\n"], "language": "python", "code": "def get_ip(self):\n    if self._ip is None:\n        self._ip = self.fetch_ip()\n    return self._ip\n", "code_tokens": ["get", "ip", "self", "if", "self", "ip", "is", "none", "self", "ip", "self", "fetch", "ip", "return", "self", "ip"], "docstring": "get current ip address", "docstring_tokens": ["get", "current", "ip", "address"], "idx": 116}
{"url": "https://github.com/dlancer/django-crispy-contact-form/blob/3d422556add5aea3607344a034779c214f84da04/contact_form/helpers.py#L8-L19", "repo": "django-crispy-contact-form", "func_name": "get_user_ip", "original_string": ["def get_user_ip(request):\n", "    \"\"\"Return user ip\n", "\n", "    :param request: Django request object\n", "    :return: user ip\n", "    \"\"\"\n", "    ip = get_real_ip(request)\n", "    if ip is None:\n", "        ip = get_ip(request)\n", "        if ip is None:\n", "            ip = '127.0.0.1'\n", "    return ip\n"], "language": "python", "code": "def get_user_ip(request):\n    \"\"\"\"\"\"\n    ip = get_real_ip(request)\n    if ip is None:\n        ip = get_ip(request)\n        if ip is None:\n            ip = '127.0.0.1'\n    return ip\n", "code_tokens": ["get", "user", "ip", "request", "ip", "get", "real", "ip", "request", "if", "ip", "is", "none", "ip", "get", "ip", "request", "if", "ip", "is", "none", "ip", "127", "0", "0", "1", "return", "ip"], "docstring": "get current ip address", "docstring_tokens": ["get", "current", "ip", "address"], "idx": 117}
{"url": "https://github.com/emc-openstack/storops/blob/24b4b13bf065c0ef0538dd0b5ebb8f25d24176bd/storops/vnx/resource/mover.py#L203-L208", "repo": "storops", "func_name": "get_ip", "original_string": ["    def get_ip(self):\n", "        if self._ip is not None:\n", "            ret = self._ip\n", "        else:\n", "            ret = self.ip_addr\n", "        return ret\n"], "language": "python", "code": "def get_ip(self):\n    if self._ip is not None:\n        ret = self._ip\n    else:\n        ret = self.ip_addr\n    return ret\n", "code_tokens": ["get", "ip", "self", "if", "self", "ip", "is", "not", "none", "ret", "self", "ip", "else", "ret", "self", "ip", "addr", "return", "ret"], "docstring": "get current ip address", "docstring_tokens": ["get", "current", "ip", "address"], "idx": 118}
{"url": "https://github.com/dade-ai/snipy/blob/408520867179f99b3158b57520e2619f3fecd69b/snipy/img/imageutil.py#L479-L482", "repo": "snipy", "func_name": "_convert_uint8", "original_string": ["def _convert_uint8(im):\n", "    if im.dtype != np.uint8:\n", "        im = np.uint8(im * 255)\n", "    return im\n"], "language": "python", "code": "def _convert_uint8(im):\n    if im.dtype != np.uint8:\n        im = np.uint8(im * 255)\n    return im\n", "code_tokens": ["convert", "im", "if", "im", "dtype", "np", "im", "np", "im", "255", "return", "im"], "docstring": "converting uint8 array to image", "docstring_tokens": ["converting", "uint8", "array", "to", "image"], "idx": 119}
{"url": "https://github.com/radjkarl/imgProcessor/blob/7c5a28718f81c01a430152c60a686ac50afbfd7c/imgProcessor/transformations.py#L11-L75", "repo": "imgProcessor", "func_name": "toUIntArray", "original_string": ["def toUIntArray(img, dtype=None, cutNegative=True, cutHigh=True,\n", "                range=None, copy=True):\n", "    '''\n", "    transform a float to an unsigned integer array of a fitting dtype\n", "    adds an offset, to get rid of negative values\n", "\n", "    range = (min, max) - scale values between given range\n", "\n", "    cutNegative - all values <0 will be set to 0\n", "    cutHigh - set to False to rather scale values to fit\n", "    '''\n", "    mn, mx = None, None\n", "    if range is not None:\n", "        mn, mx = range\n", "\n", "    if dtype is None:\n", "        if mx is None:\n", "            mx = np.nanmax(img)\n", "        dtype = np.uint16 if mx > 255 else np.uint8\n", "\n", "    dtype = np.dtype(dtype)\n", "    if dtype == img.dtype:\n", "        return img\n", "\n", "    # get max px value:\n", "    b = {'uint8': 255,\n", "         'uint16': 65535,\n", "         'uint32': 4294967295,\n", "         'uint64': 18446744073709551615}[dtype.name]\n", "\n", "    if copy:\n", "        img = img.copy()\n", "\n", "    if range is not None:\n", "        img = np.asfarray(img)\n", "        img -= mn\n", "        # img[img<0]=0\n", "        # print np.nanmin(img), np.nanmax(img), mn, mx, range, b\n", "\n", "        img *= b / (mx - mn)\n", "        # print np.nanmin(img), np.nanmax(img), mn, mx, range, b\n", "        img = np.clip(img, 0, b)\n", "\n", "    else:\n", "        if cutNegative:\n", "            img[img < 0] = 0\n", "        else:\n", "            # add an offset to all values:\n", "            mn = np.min(img)\n", "            if mn < 0:\n", "                img -= mn  # set minimum to 0\n", "\n", "        if cutHigh:\n", "            #ind = img > b\n", "            img[img > b] = b\n", "        else:\n", "            # scale values\n", "            mx = np.nanmax(img)\n", "            img = np.asfarray(img) * (float(b) / mx)\n", "\n", "    img = img.astype(dtype)\n", "\n", "#     if range is not None and cutHigh:\n", "#         img[ind] = b\n", "    return img\n"], "language": "python", "code": "def toUIntArray(img, dtype=None, cutNegative=True, cutHigh=True, range=None,\n    copy=True):\n    \"\"\"\"\"\"\n    mn, mx = None, None\n    if range is not None:\n        mn, mx = range\n    if dtype is None:\n        if mx is None:\n            mx = np.nanmax(img)\n        dtype = np.uint16 if mx > 255 else np.uint8\n    dtype = np.dtype(dtype)\n    if dtype == img.dtype:\n        return img\n    b = {'uint8': 255, 'uint16': 65535, 'uint32': 4294967295, 'uint64': \n        18446744073709551615}[dtype.name]\n    if copy:\n        img = img.copy()\n    if range is not None:\n        img = np.asfarray(img)\n        img -= mn\n        img *= b / (mx - mn)\n        img = np.clip(img, 0, b)\n    else:\n        if cutNegative:\n            img[img < 0] = 0\n        else:\n            mn = np.min(img)\n            if mn < 0:\n                img -= mn\n        if cutHigh:\n            img[img > b] = b\n        else:\n            mx = np.nanmax(img)\n            img = np.asfarray(img) * (float(b) / mx)\n    img = img.astype(dtype)\n    return img\n", "code_tokens": ["touintarray", "img", "dtype", "none", "cutnegative", "true", "cuthigh", "true", "range", "none", "copy", "true", "mn", "mx", "none", "none", "if", "range", "is", "not", "none", "mn", "mx", "range", "if", "dtype", "is", "none", "if", "mx", "is", "none", "mx", "np", "nanmax", "img", "dtype", "np", "if", "mx", "255", "else", "np", "dtype", "np", "dtype", "dtype", "if", "dtype", "img", "dtype", "return", "img", "255", "65535", "4294967295", "18446744073709551615", "dtype", "name", "if", "copy", "img", "img", "copy", "if", "range", "is", "not", "none", "img", "np", "asfarray", "img", "img", "mn", "img", "mx", "mn", "img", "np", "clip", "img", "0", "else", "if", "cutnegative", "img", "img", "0", "0", "else", "mn", "np", "min", "img", "if", "mn", "0", "img", "mn", "if", "cuthigh", "img", "img", "else", "mx", "np", "nanmax", "img", "img", "np", "asfarray", "img", "float", "mx", "img", "img", "astype", "dtype", "return", "img"], "docstring": "converting uint8 array to image", "docstring_tokens": ["converting", "uint8", "array", "to", "image"], "idx": 120}
{"url": "https://github.com/radjkarl/imgProcessor/blob/7c5a28718f81c01a430152c60a686ac50afbfd7c/imgProcessor/imgIO.py#L39-L73", "repo": "imgProcessor", "func_name": "imread", "original_string": ["def imread(img, color=None, dtype=None):\n", "    '''\n", "    dtype = 'noUint', uint8, float, 'float', ...\n", "    '''\n", "    COLOR2CV = {'gray': cv2.IMREAD_GRAYSCALE,\n", "                'all': cv2.IMREAD_COLOR,\n", "                None: cv2.IMREAD_ANYCOLOR\n", "                }\n", "    c = COLOR2CV[color]\n", "    if callable(img):\n", "        img = img()\n", "    elif isinstance(img, string_types):\n", "        #         from_file = True\n", "        #         try:\n", "        #             ftype = img[img.find('.'):]\n", "        #             img = READERS[ftype](img)[0]\n", "        #         except KeyError:\n", "        # open with openCV\n", "        # grey - 8 bit\n", "        if dtype in (None, \"noUint\") or np.dtype(dtype) != np.uint8:\n", "            c |= cv2.IMREAD_ANYDEPTH\n", "        img2 = cv2.imread(img, c)\n", "        if img2 is None:\n", "            raise IOError(\"image '%s' is not existing\" % img)\n", "        img = img2\n", "\n", "    elif color == 'gray' and img.ndim == 3:  # multi channel img like rgb\n", "        # cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) #cannot handle float64\n", "        img = toGray(img)\n", "    # transform array to uint8 array due to openCV restriction\n", "    if dtype is not None:\n", "        if isinstance(img, np.ndarray):\n", "            img = _changeArrayDType(img, dtype, cutHigh=False)\n", "\n", "    return img\n"], "language": "python", "code": "def imread(img, color=None, dtype=None):\n    \"\"\"\"\"\"\n    COLOR2CV = {'gray': cv2.IMREAD_GRAYSCALE, 'all': cv2.IMREAD_COLOR, None:\n        cv2.IMREAD_ANYCOLOR}\n    c = COLOR2CV[color]\n    if callable(img):\n        img = img()\n    elif isinstance(img, string_types):\n        if dtype in (None, 'noUint') or np.dtype(dtype) != np.uint8:\n            c |= cv2.IMREAD_ANYDEPTH\n        img2 = cv2.imread(img, c)\n        if img2 is None:\n            raise IOError(\"image '%s' is not existing\" % img)\n        img = img2\n    elif color == 'gray' and img.ndim == 3:\n        img = toGray(img)\n    if dtype is not None:\n        if isinstance(img, np.ndarray):\n            img = _changeArrayDType(img, dtype, cutHigh=False)\n    return img\n", "code_tokens": ["imread", "img", "color", "none", "dtype", "none", "gray", "imread", "grayscale", "all", "imread", "color", "none", "imread", "anycolor", "color", "if", "callable", "img", "img", "img", "elif", "isinstance", "img", "string", "types", "if", "dtype", "in", "none", "nouint", "or", "np", "dtype", "dtype", "np", "imread", "anydepth", "imread", "img", "if", "is", "none", "raise", "ioerror", "image", "is", "not", "existing", "img", "img", "elif", "color", "gray", "and", "img", "ndim", "3", "img", "togray", "img", "if", "dtype", "is", "not", "none", "if", "isinstance", "img", "np", "ndarray", "img", "changearraydtype", "img", "dtype", "cuthigh", "false", "return", "img"], "docstring": "converting uint8 array to image", "docstring_tokens": ["converting", "uint8", "array", "to", "image"], "idx": 121}
{"url": "https://github.com/sirfoga/pyhal/blob/4394d8a1f7e45bea28a255ec390f4962ee64d33a/hal/internet/web.py#L171-L180", "repo": "pyhal", "func_name": "get_html_source", "original_string": ["    def get_html_source(self):\n", "        \"\"\"Gets source page of url\n", "        :return: HTML source\n", "        \"\"\"\n", "        req = urllib.request.Request(self.url)\n", "        req.add_header(\"user-agent\", random.choice(USER_AGENTS))\n", "        req_text = urllib.request.urlopen(req).read()\n", "        self.source = str(req_text)\n", "        self.soup = BeautifulSoup(self.source, \"html.parser\")\n", "        return self.source\n"], "language": "python", "code": "def get_html_source(self):\n    \"\"\"\"\"\"\n    req = urllib.request.Request(self.url)\n    req.add_header('user-agent', random.choice(USER_AGENTS))\n    req_text = urllib.request.urlopen(req).read()\n    self.source = str(req_text)\n    self.soup = BeautifulSoup(self.source, 'html.parser')\n    return self.source\n", "code_tokens": ["get", "html", "source", "self", "req", "urllib", "request", "request", "self", "url", "req", "add", "header", "user", "agent", "random", "choice", "user", "agents", "req", "text", "urllib", "request", "urlopen", "req", "read", "self", "source", "str", "req", "text", "self", "soup", "beautifulsoup", "self", "source", "html", "parser", "return", "self", "source"], "docstring": "how to get html of website", "docstring_tokens": ["how", "to", "get", "html", "of", "website"], "idx": 122}
{"url": "https://github.com/dariosky/wfcli/blob/87a9ed30dbd456f801135a55099f0541b0614ccb/wfcli/wfapi.py#L123-L134", "repo": "wfcli", "func_name": "update_website", "original_string": ["    def update_website(self, website):\n", "        self.connect()\n", "        website = self.server.update_website(\n", "            self.session_id,\n", "            website['name'],\n", "            website['ip'],\n", "            website['https'],\n", "            website['subdomains'],\n", "            website['certificate'],\n", "            *website['website_apps']\n", "        )\n", "        return website\n"], "language": "python", "code": "def update_website(self, website):\n    self.connect()\n    website = self.server.update_website(self.session_id, website['name'],\n        website['ip'], website['https'], website['subdomains'], website[\n        'certificate'], *website['website_apps'])\n    return website\n", "code_tokens": ["update", "website", "self", "website", "self", "connect", "website", "self", "server", "update", "website", "self", "session", "id", "website", "name", "website", "ip", "website", "https", "website", "subdomains", "website", "certificate", "website", "website", "apps", "return", "website"], "docstring": "how to get html of website", "docstring_tokens": ["how", "to", "get", "html", "of", "website"], "idx": 123}
{"url": "https://github.com/Netflix-Skunkworks/cloudaux/blob/c4b0870c3ac68b1c69e71d33cf78b6a8bdf437ea/cloudaux/orchestration/aws/s3.py#L160-L178", "repo": "cloudaux", "func_name": "get_website", "original_string": ["def get_website(bucket_name, **conn):\n", "    try:\n", "        result = get_bucket_website(Bucket=bucket_name, **conn)\n", "    except ClientError as e:\n", "        if \"NoSuchWebsiteConfiguration\" not in str(e):\n", "            raise e\n", "        return None\n", "\n", "    website = {}\n", "    if result.get(\"IndexDocument\"):\n", "        website[\"IndexDocument\"] = result[\"IndexDocument\"]\n", "    if result.get(\"RoutingRules\"):\n", "        website[\"RoutingRules\"] = result[\"RoutingRules\"]\n", "    if result.get(\"RedirectAllRequestsTo\"):\n", "        website[\"RedirectAllRequestsTo\"] = result[\"RedirectAllRequestsTo\"]\n", "    if result.get(\"ErrorDocument\"):\n", "        website[\"ErrorDocument\"] = result[\"ErrorDocument\"]\n", "\n", "    return website\n"], "language": "python", "code": "def get_website(bucket_name, **conn):\n    try:\n        result = get_bucket_website(Bucket=bucket_name, **conn)\n    except ClientError as e:\n        if 'NoSuchWebsiteConfiguration' not in str(e):\n            raise e\n        return None\n    website = {}\n    if result.get('IndexDocument'):\n        website['IndexDocument'] = result['IndexDocument']\n    if result.get('RoutingRules'):\n        website['RoutingRules'] = result['RoutingRules']\n    if result.get('RedirectAllRequestsTo'):\n        website['RedirectAllRequestsTo'] = result['RedirectAllRequestsTo']\n    if result.get('ErrorDocument'):\n        website['ErrorDocument'] = result['ErrorDocument']\n    return website\n", "code_tokens": ["get", "website", "bucket", "name", "conn", "try", "result", "get", "bucket", "website", "bucket", "bucket", "name", "conn", "except", "clienterror", "as", "if", "nosuchwebsiteconfiguration", "not", "in", "str", "raise", "return", "none", "website", "if", "result", "get", "indexdocument", "website", "indexdocument", "result", "indexdocument", "if", "result", "get", "routingrules", "website", "routingrules", "result", "routingrules", "if", "result", "get", "redirectallrequeststo", "website", "redirectallrequeststo", "result", "redirectallrequeststo", "if", "result", "get", "errordocument", "website", "errordocument", "result", "errordocument", "return", "website"], "docstring": "how to get html of website", "docstring_tokens": ["how", "to", "get", "html", "of", "website"], "idx": 124}
{"url": "https://github.com/audreyr/alotofeffort/blob/06deca82a70fa9896496fd44c8c6f24707396c50/alotofeffort/main.py#L9-L30", "repo": "alotofeffort", "func_name": "main", "original_string": ["def main():\n", "    \"\"\" Entry point for the package, as defined in setup.py. \"\"\"\n", "\n", "    # Log info and above to console\n", "    logging.basicConfig(\n", "        format='%(levelname)s: %(message)s', level=logging.INFO)\n", "\n", "    # Get command line input/output arguments\n", "    msg = 'Instantly deploy static HTML sites to S3 at the command line.'\n", "    parser = argparse.ArgumentParser(description=msg)\n", "    parser.add_argument(\n", "        'www_dir',\n", "        help='Directory containing the HTML files for your website.'\n", "    )\n", "    parser.add_argument(\n", "        'bucket_name',\n", "        help='Name of S3 bucket to deploy to, e.g. mybucket.'\n", "    )\n", "    args = parser.parse_args()\n", "\n", "    # Deploy the site to S3!\n", "    deploy(args.www_dir, args.bucket_name)\n"], "language": "python", "code": "def main():\n    \"\"\"\"\"\"\n    logging.basicConfig(format='%(levelname)s: %(message)s', level=logging.INFO\n        )\n    msg = 'Instantly deploy static HTML sites to S3 at the command line.'\n    parser = argparse.ArgumentParser(description=msg)\n    parser.add_argument('www_dir', help=\n        'Directory containing the HTML files for your website.')\n    parser.add_argument('bucket_name', help=\n        'Name of S3 bucket to deploy to, e.g. mybucket.')\n    args = parser.parse_args()\n    deploy(args.www_dir, args.bucket_name)\n", "code_tokens": ["main", "logging", "basicconfig", "format", "levelname", "message", "level", "logging", "info", "msg", "instantly", "deploy", "static", "html", "sites", "to", "at", "the", "command", "line", "parser", "argparse", "argumentparser", "description", "msg", "parser", "add", "argument", "www", "dir", "help", "directory", "containing", "the", "html", "files", "for", "your", "website", "parser", "add", "argument", "bucket", "name", "help", "name", "of", "bucket", "to", "deploy", "to", "mybucket", "args", "parser", "parse", "args", "deploy", "args", "www", "dir", "args", "bucket", "name"], "docstring": "how to get html of website", "docstring_tokens": ["how", "to", "get", "html", "of", "website"], "idx": 125}
{"url": "https://github.com/bitesofcode/projex/blob/d31743ec456a41428709968ab11a2cf6c6c76247/projex/init.py#L368-L384", "repo": "projex", "func_name": "website", "original_string": ["def website(app=None, mode='home', subcontext='UserGuide'):\n", "    \"\"\"\n", "    Returns the website location for projex software.\n", "    \n", "    :param      app  | <str> || None\n", "                mode | <str> (home, docs, blog, dev)\n", "    \n", "    :return     <str>\n", "    \"\"\"\n", "    base_url = WEBSITES.get(mode, '')\n", "\n", "    if app and base_url:\n", "        opts = {'app': app, 'base_url': base_url}\n", "        base_url = SUBCONTEXT_MAP.get((mode, subcontext), base_url)\n", "        base_url %= opts\n", "\n", "    return base_url"], "language": "python", "code": "def website(app=None, mode='home', subcontext='UserGuide'):\n    \"\"\"\"\"\"\n    base_url = WEBSITES.get(mode, '')\n    if app and base_url:\n        opts = {'app': app, 'base_url': base_url}\n        base_url = SUBCONTEXT_MAP.get((mode, subcontext), base_url)\n        base_url %= opts\n    return base_url\n", "code_tokens": ["website", "app", "none", "mode", "home", "subcontext", "userguide", "base", "url", "websites", "get", "mode", "if", "app", "and", "base", "url", "opts", "app", "app", "base", "url", "base", "url", "base", "url", "subcontext", "map", "get", "mode", "subcontext", "base", "url", "base", "url", "opts", "return", "base", "url"], "docstring": "how to get html of website", "docstring_tokens": ["how", "to", "get", "html", "of", "website"], "idx": 126}
{"url": "https://github.com/Shapeways/coyote_framework/blob/cb29899b984a21d56bf65d0b1d907073948fe16c/coyote_framework/webdriver/webdriverwrapper/WebElementWrapper.py#L655-L660", "repo": "coyote_framework", "func_name": "checkbox_check", "original_string": ["    def checkbox_check(self, force_check=False):\n", "        \"\"\"\n", "        Wrapper to check a  checkbox\n", "        \"\"\"\n", "        if not self.get_attribute('checked'):\n", "            self.click(force_click=force_check)\n"], "language": "python", "code": "def checkbox_check(self, force_check=False):\n    \"\"\"\"\"\"\n    if not self.get_attribute('checked'):\n        self.click(force_click=force_check)\n", "code_tokens": ["checkbox", "check", "self", "force", "check", "false", "if", "not", "self", "get", "attribute", "checked", "self", "click", "force", "click", "force", "check"], "docstring": "how to check if a checkbox is checked", "docstring_tokens": ["how", "to", "check", "if", "a", "checkbox", "is", "checked"], "idx": 127}
{"url": "https://github.com/bbangert/lettuce_webdriver/blob/d11f8531c43bb7150c316e0dc4ccd083617becf7/lettuce_webdriver/webdriver.py#L337-L341", "repo": "lettuce_webdriver", "func_name": "check_checkbox", "original_string": ["def check_checkbox(step, value):\n", "    with AssertContextManager(step):\n", "        check_box = find_field(world.browser, 'checkbox', value)\n", "        if not check_box.is_selected():\n", "            check_box.click()\n"], "language": "python", "code": "def check_checkbox(step, value):\n    with AssertContextManager(step):\n        check_box = find_field(world.browser, 'checkbox', value)\n        if not check_box.is_selected():\n            check_box.click()\n", "code_tokens": ["check", "checkbox", "step", "value", "with", "assertcontextmanager", "step", "check", "box", "find", "field", "world", "browser", "checkbox", "value", "if", "not", "check", "box", "is", "selected", "check", "box", "click"], "docstring": "how to check if a checkbox is checked", "docstring_tokens": ["how", "to", "check", "if", "a", "checkbox", "is", "checked"], "idx": 128}
{"url": "https://github.com/jbm950/pygame_toolbox/blob/3fe32145fc149e4dd0963c30a2b6a4dddd4fac0e/pygame_toolbox/graphics/widgets.py#L141-L162", "repo": "pygame_toolbox", "func_name": "__call__", "original_string": ["    def __call__(self, *arg):\n", "        if self.status:\n", "            # If the checkbox was previously checked, set the status\n", "            # to 0 and turn the whole box back to white.\n", "            self.status = 0\n", "            self.image.fill((255, 255, 255))\n", "\n", "            # Draw the checkbox on the external surface\n", "            ptg.Button.set_position(self, self.position, self.midpoint,\n", "                                    self.surface)\n", "        else:\n", "            # If the checkbox was previously unchecked, set the status\n", "            # to 1 and draw a check in the center of the checkbox.\n", "            self.status = 1\n", "            if self.checktype == 'r':\n", "                self.draw_rect_check()\n", "            elif self.checktype == 'c':\n", "                self.draw_circle_check()\n", "\n", "            # Draw the new image on the external surface\n", "            ptg.Button.set_position(self, self.position, self.midpoint,\n", "                                    self.surface)\n"], "language": "python", "code": "def __call__(self, *arg):\n    if self.status:\n        self.status = 0\n        self.image.fill((255, 255, 255))\n        ptg.Button.set_position(self, self.position, self.midpoint, self.\n            surface)\n    else:\n        self.status = 1\n        if self.checktype == 'r':\n            self.draw_rect_check()\n        elif self.checktype == 'c':\n            self.draw_circle_check()\n        ptg.Button.set_position(self, self.position, self.midpoint, self.\n            surface)\n", "code_tokens": ["call", "self", "arg", "if", "self", "status", "self", "status", "0", "self", "image", "fill", "255", "255", "255", "ptg", "button", "set", "position", "self", "self", "position", "self", "midpoint", "self", "surface", "else", "self", "status", "1", "if", "self", "checktype", "self", "draw", "rect", "check", "elif", "self", "checktype", "self", "draw", "circle", "check", "ptg", "button", "set", "position", "self", "self", "position", "self", "midpoint", "self", "surface"], "docstring": "how to check if a checkbox is checked", "docstring_tokens": ["how", "to", "check", "if", "a", "checkbox", "is", "checked"], "idx": 129}
{"url": "https://github.com/bbangert/lettuce_webdriver/blob/d11f8531c43bb7150c316e0dc4ccd083617becf7/lettuce_webdriver/webdriver.py#L359-L361", "repo": "lettuce_webdriver", "func_name": "assert_not_checked_checkbox", "original_string": ["def assert_not_checked_checkbox(step, value):\n", "    check_box = find_field(world.browser, 'checkbox', value)\n", "    assert_true(step, not check_box.is_selected())\n"], "language": "python", "code": "def assert_not_checked_checkbox(step, value):\n    check_box = find_field(world.browser, 'checkbox', value)\n    assert_true(step, not check_box.is_selected())\n", "code_tokens": ["assert", "not", "checked", "checkbox", "step", "value", "check", "box", "find", "field", "world", "browser", "checkbox", "value", "assert", "true", "step", "not", "check", "box", "is", "selected"], "docstring": "how to check if a checkbox is checked", "docstring_tokens": ["how", "to", "check", "if", "a", "checkbox", "is", "checked"], "idx": 130}
{"url": "https://github.com/locationlabs/mockredis/blob/fd4e3117066ff0c24e86ebca007853a8092e3254/mockredis/client.py#L1193-L1196", "repo": "mockredis", "func_name": "zrank", "original_string": ["    def zrank(self, name, value):\n", "        zset = self._get_zset(name, \"ZRANK\")\n", "\n", "        return zset.rank(self._encode(value)) if zset else None\n"], "language": "python", "code": "def zrank(self, name, value):\n    zset = self._get_zset(name, 'ZRANK')\n    return zset.rank(self._encode(value)) if zset else None\n", "code_tokens": ["zrank", "self", "name", "value", "zset", "self", "get", "zset", "name", "zrank", "return", "zset", "rank", "self", "encode", "value", "if", "zset", "else", "none"], "docstring": "fuzzy match ranking", "docstring_tokens": ["fuzzy", "match", "ranking"], "idx": 131}
{"url": "https://github.com/azraq27/gini/blob/3c2b5265d096d606b303bfe25ac9adb74b8cee14/gini/matching.py#L63-L68", "repo": "gini", "func_name": "best_item_from_list", "original_string": ["def best_item_from_list(item,options,fuzzy=90,fname_match=True,fuzzy_fragment=None,guess=False):\n", "    '''Returns just the best item, or ``None``'''\n", "    match = best_match_from_list(item,options,fuzzy,fname_match,fuzzy_fragment,guess)\n", "    if match:\n", "        return match[0]\n", "    return None"], "language": "python", "code": "def best_item_from_list(item, options, fuzzy=90, fname_match=True,\n    fuzzy_fragment=None, guess=False):\n    \"\"\"\"\"\"\n    match = best_match_from_list(item, options, fuzzy, fname_match,\n        fuzzy_fragment, guess)\n    if match:\n        return match[0]\n    return None\n", "code_tokens": ["best", "item", "from", "list", "item", "options", "fuzzy", "90", "fname", "match", "true", "fuzzy", "fragment", "none", "guess", "false", "match", "best", "match", "from", "list", "item", "options", "fuzzy", "fname", "match", "fuzzy", "fragment", "guess", "if", "match", "return", "match", "0", "return", "none"], "docstring": "fuzzy match ranking", "docstring_tokens": ["fuzzy", "match", "ranking"], "idx": 132}
{"url": "https://github.com/mbodenhamer/syn/blob/aeaa3ad8a49bac8f50cf89b6f1fe97ad43d1d258/syn/base_utils/logic.py#L59-L61", "repo": "syn", "func_name": "fuzzy_xor", "original_string": ["def fuzzy_xor(a, b):\n", "    return fuzzy_and(fuzzy_or(a, b),\n", "                     fuzzy_not(fuzzy_equiv(a, b)))\n"], "language": "python", "code": "def fuzzy_xor(a, b):\n    return fuzzy_and(fuzzy_or(a, b), fuzzy_not(fuzzy_equiv(a, b)))\n", "code_tokens": ["fuzzy", "xor", "return", "fuzzy", "and", "fuzzy", "or", "fuzzy", "not", "fuzzy", "equiv"], "docstring": "fuzzy match ranking", "docstring_tokens": ["fuzzy", "match", "ranking"], "idx": 133}
{"url": "https://github.com/hearsaycorp/normalize/blob/8b36522ddca6d41b434580bd848f3bdaa7a999c8/normalize/diff.py#L547-L588", "repo": "normalize", "func_name": "_fuzzy_match", "original_string": ["\n", "\n", "def _fuzzy_match(set_a, set_b):\n", "    seen = dict()\n", "    scores = list()\n", "\n", "    # Yes, this is O(n.m), but python's equality operator is\n", "    # fast for hashable types.\n", "    for a_pk_seq, b_pk_seq in product(set_a, set_b):\n", "        a_pk, a_seq = a_pk_seq\n", "        b_pk, b_seq = b_pk_seq\n", "        if (a_pk, b_pk) in seen:\n", "            if seen[a_pk, b_pk][0]:\n", "                score = list(seen[a_pk, b_pk])\n", "                scores.append(score + [a_pk_seq, b_pk_seq])\n", "        else:\n", "            match = 0\n", "            common = min((len(a_pk), len(b_pk)))\n", "            no_match = max((len(a_pk), len(b_pk))) - common\n", "            for i in range(0, common):\n", "                if a_pk[i] == b_pk[i]:\n", "                    if not _nested_falsy(a_pk[i]):\n", "                        match += 1\n", "                else:\n", "                    no_match += 1\n", "            seen[a_pk, b_pk] = (match, no_match)\n", "            if match:\n", "                scores.append([match, no_match, a_pk_seq, b_pk_seq])\n", "\n", "    remaining_a = set(set_a)\n", "    remaining_b = set(set_b)\n", "\n", "    for match, no_match, a_pk_seq, b_pk_seq in sorted(\n", "        scores,\n", "        key=lambda x: x[0] - x[1],\n", "        reverse=True,\n", "    ):\n", "        if a_pk_seq in remaining_a and b_pk_seq in remaining_b:\n", "            remaining_a.remove(a_pk_seq)\n", "            remaining_b.remove(b_pk_seq)\n", "            yield a_pk_seq, b_pk_seq\n", "\n"], "language": "python", "code": "def _fuzzy_match(set_a, set_b):\n    seen = dict()\n    scores = list()\n    for a_pk_seq, b_pk_seq in product(set_a, set_b):\n        a_pk, a_seq = a_pk_seq\n        b_pk, b_seq = b_pk_seq\n        if (a_pk, b_pk) in seen:\n            if seen[a_pk, b_pk][0]:\n                score = list(seen[a_pk, b_pk])\n                scores.append(score + [a_pk_seq, b_pk_seq])\n        else:\n            match = 0\n            common = min((len(a_pk), len(b_pk)))\n            no_match = max((len(a_pk), len(b_pk))) - common\n            for i in range(0, common):\n                if a_pk[i] == b_pk[i]:\n                    if not _nested_falsy(a_pk[i]):\n                        match += 1\n                else:\n                    no_match += 1\n            seen[a_pk, b_pk] = match, no_match\n            if match:\n                scores.append([match, no_match, a_pk_seq, b_pk_seq])\n    remaining_a = set(set_a)\n    remaining_b = set(set_b)\n    for match, no_match, a_pk_seq, b_pk_seq in sorted(scores, key=lambda x:\n        x[0] - x[1], reverse=True):\n        if a_pk_seq in remaining_a and b_pk_seq in remaining_b:\n            remaining_a.remove(a_pk_seq)\n            remaining_b.remove(b_pk_seq)\n            yield a_pk_seq, b_pk_seq\n", "code_tokens": ["fuzzy", "match", "set", "set", "seen", "dict", "scores", "list", "for", "pk", "seq", "pk", "seq", "in", "product", "set", "set", "pk", "seq", "pk", "seq", "pk", "seq", "pk", "seq", "if", "pk", "pk", "in", "seen", "if", "seen", "pk", "pk", "0", "score", "list", "seen", "pk", "pk", "scores", "append", "score", "pk", "seq", "pk", "seq", "else", "match", "0", "common", "min", "len", "pk", "len", "pk", "no", "match", "max", "len", "pk", "len", "pk", "common", "for", "in", "range", "0", "common", "if", "pk", "pk", "if", "not", "nested", "falsy", "pk", "match", "1", "else", "no", "match", "1", "seen", "pk", "pk", "match", "no", "match", "if", "match", "scores", "append", "match", "no", "match", "pk", "seq", "pk", "seq", "remaining", "set", "set", "remaining", "set", "set", "for", "match", "no", "match", "pk", "seq", "pk", "seq", "in", "sorted", "scores", "key", "lambda", "0", "1", "reverse", "true", "if", "pk", "seq", "in", "remaining", "and", "pk", "seq", "in", "remaining", "remaining", "remove", "pk", "seq", "remaining", "remove", "pk", "seq", "yield", "pk", "seq", "pk", "seq"], "docstring": "fuzzy match ranking", "docstring_tokens": ["fuzzy", "match", "ranking"], "idx": 134}
{"url": "https://github.com/PrefPy/prefpy/blob/f395ba3782f05684fa5de0cece387a6da9391d02/prefpy/egmm_mixpl.py#L69-L90", "repo": "prefpy", "func_name": "Dictionarize", "original_string": ["def Dictionarize(rankings, m):\n", "    rankcnt = {}\n", "    #print(\"1\",rankings[1])\n", "    for ranking in rankings:\n", "        #print(\"2\",ranking)\n", "        flag = 0\n", "        l = len(ranking)\n", "        if len(set(ranking)) < l:\n", "            print(\"Orders with duplicate alternatives are ignored!\")\n", "            continue\n", "        for i in range(l):\n", "            if ranking[i] >= m or ranking[i] < 0:\n", "                flag = 1\n", "        if flag == 1:\n", "            print(\"Alternative index out of range! Ranking ignored!\")\n", "            continue\n", "        key = rank2str(ranking)\n", "        if key in rankcnt:\n", "            rankcnt[key] += 1\n", "        else:\n", "            rankcnt[key] = 1\n", "    return rankcnt\n"], "language": "python", "code": "def Dictionarize(rankings, m):\n    rankcnt = {}\n    for ranking in rankings:\n        flag = 0\n        l = len(ranking)\n        if len(set(ranking)) < l:\n            print('Orders with duplicate alternatives are ignored!')\n            continue\n        for i in range(l):\n            if ranking[i] >= m or ranking[i] < 0:\n                flag = 1\n        if flag == 1:\n            print('Alternative index out of range! Ranking ignored!')\n            continue\n        key = rank2str(ranking)\n        if key in rankcnt:\n            rankcnt[key] += 1\n        else:\n            rankcnt[key] = 1\n    return rankcnt\n", "code_tokens": ["dictionarize", "rankings", "rankcnt", "for", "ranking", "in", "rankings", "flag", "0", "len", "ranking", "if", "len", "set", "ranking", "print", "orders", "with", "duplicate", "alternatives", "are", "ignored", "continue", "for", "in", "range", "if", "ranking", "or", "ranking", "0", "flag", "1", "if", "flag", "1", "print", "alternative", "index", "out", "of", "range", "ranking", "ignored", "continue", "key", "ranking", "if", "key", "in", "rankcnt", "rankcnt", "key", "1", "else", "rankcnt", "key", "1", "return", "rankcnt"], "docstring": "fuzzy match ranking", "docstring_tokens": ["fuzzy", "match", "ranking"], "idx": 135}
{"url": "https://github.com/toejough/pimento/blob/cdb00a93976733aa5521f8504152cedeedfc711a/pimento/__init__.py#L298-L311", "repo": "pimento", "func_name": "_exact_match", "original_string": ["def _exact_match(response, matches, insensitive, fuzzy):\n", "    '''\n", "    returns an exact match, if it exists, given parameters\n", "    for the match\n", "    '''\n", "    for match in matches:\n", "        if response == match:\n", "            return match\n", "        elif insensitive and response.lower() == match.lower():\n", "            return match\n", "        elif fuzzy and _exact_fuzzy_match(response, match, insensitive):\n", "            return match\n", "    else:\n", "        return None\n"], "language": "python", "code": "def _exact_match(response, matches, insensitive, fuzzy):\n    \"\"\"\"\"\"\n    for match in matches:\n        if response == match:\n            return match\n        elif insensitive and response.lower() == match.lower():\n            return match\n        elif fuzzy and _exact_fuzzy_match(response, match, insensitive):\n            return match\n    else:\n        return None\n", "code_tokens": ["exact", "match", "response", "matches", "insensitive", "fuzzy", "for", "match", "in", "matches", "if", "response", "match", "return", "match", "elif", "insensitive", "and", "response", "lower", "match", "lower", "return", "match", "elif", "fuzzy", "and", "exact", "fuzzy", "match", "response", "match", "insensitive", "return", "match", "else", "return", "none"], "docstring": "fuzzy match ranking", "docstring_tokens": ["fuzzy", "match", "ranking"], "idx": 136}
{"url": "https://github.com/gpagliuca/pyfas/blob/5daa1199bd124d315d02bef0ad3888a8f58355b2/build/lib/pyfas/ppl.py#L127-L151", "repo": "pyfas", "func_name": "to_excel", "original_string": ["    def to_excel(self, *args):\n", "        \"\"\"\n", "        Dump all the data to excel, fname and path can be passed as args\n", "        \"\"\"\n", "        path = os.getcwd()\n", "        fname = self.fname.replace(\".ppl\", \"_ppl\") + \".xlsx\"\n", "        if len(args) > 0 and args[0] != \"\":\n", "            path = args[0]\n", "            if os.path.exists(path) == False:\n", "                os.mkdir(path)\n", "        xl_file = pd.ExcelWriter(path + os.sep + fname)\n", "        for idx in self.filter_data(\"\"):\n", "            self.extract(idx)\n", "        labels = list(self.filter_data(\"\").values())\n", "        for prof in self.data:\n", "            data_df = pd.DataFrame()\n", "            data_df[\"X\"] = self.data[prof][0]\n", "            for timestep, data in zip(self.time, self.data[prof][1]):\n", "                data_df[timestep] = data\n", "            myvar = labels[prof-1].split(\" \")[0]\n", "            br_label = labels[prof-1].split(\"\\'\")[5]\n", "            unit = labels[prof-1].split(\"\\'\")[7].replace(\"/\", \"-\")\n", "            mylabel = \"{} - {} - {}\".format(myvar, br_label, unit)\n", "            data_df.to_excel(xl_file, sheet_name=mylabel)\n", "        xl_file.save()\n"], "language": "python", "code": "def to_excel(self, *args):\n    \"\"\"\"\"\"\n    path = os.getcwd()\n    fname = self.fname.replace('.ppl', '_ppl') + '.xlsx'\n    if len(args) > 0 and args[0] != '':\n        path = args[0]\n        if os.path.exists(path) == False:\n            os.mkdir(path)\n    xl_file = pd.ExcelWriter(path + os.sep + fname)\n    for idx in self.filter_data(''):\n        self.extract(idx)\n    labels = list(self.filter_data('').values())\n    for prof in self.data:\n        data_df = pd.DataFrame()\n        data_df['X'] = self.data[prof][0]\n        for timestep, data in zip(self.time, self.data[prof][1]):\n            data_df[timestep] = data\n        myvar = labels[prof - 1].split(' ')[0]\n        br_label = labels[prof - 1].split(\"'\")[5]\n        unit = labels[prof - 1].split(\"'\")[7].replace('/', '-')\n        mylabel = '{} - {} - {}'.format(myvar, br_label, unit)\n        data_df.to_excel(xl_file, sheet_name=mylabel)\n    xl_file.save()\n", "code_tokens": ["to", "excel", "self", "args", "path", "os", "getcwd", "fname", "self", "fname", "replace", "ppl", "ppl", "xlsx", "if", "len", "args", "0", "and", "args", "0", "path", "args", "0", "if", "os", "path", "exists", "path", "false", "os", "mkdir", "path", "xl", "file", "pd", "excelwriter", "path", "os", "sep", "fname", "for", "idx", "in", "self", "filter", "data", "self", "extract", "idx", "labels", "list", "self", "filter", "data", "values", "for", "prof", "in", "self", "data", "data", "df", "pd", "dataframe", "data", "df", "self", "data", "prof", "0", "for", "timestep", "data", "in", "zip", "self", "time", "self", "data", "prof", "1", "data", "df", "timestep", "data", "myvar", "labels", "prof", "1", "split", "0", "br", "label", "labels", "prof", "1", "split", "5", "unit", "labels", "prof", "1", "split", "7", "replace", "mylabel", "format", "myvar", "br", "label", "unit", "data", "df", "to", "excel", "xl", "file", "sheet", "name", "mylabel", "xl", "file", "save"], "docstring": "export to excel", "docstring_tokens": ["export", "to", "excel"], "idx": 137}
{"url": "https://github.com/patarapolw/AnkiTools/blob/fab6836dfd9cf5171d9cbff5c55fbb14d2786f05/AnkiTools/excel.py#L155-L157", "repo": "AnkiTools", "func_name": "save", "original_string": ["    def save(self):\n", "        pyexcel_export.save_data(self.excel_filename, data=self.excel_raw, retain_meta=True,\n", "                                 created=self.created, modified=datetime.now().isoformat())\n"], "language": "python", "code": "def save(self):\n    pyexcel_export.save_data(self.excel_filename, data=self.excel_raw,\n        retain_meta=True, created=self.created, modified=datetime.now().\n        isoformat())\n", "code_tokens": ["save", "self", "pyexcel", "export", "save", "data", "self", "excel", "filename", "data", "self", "excel", "raw", "retain", "meta", "true", "created", "self", "created", "modified", "datetime", "now", "isoformat"], "docstring": "export to excel", "docstring_tokens": ["export", "to", "excel"], "idx": 138}
{"url": "https://github.com/housecanary/hc-api-python/blob/2bb9e2208b34e8617575de45934357ee33b8531c/housecanary/excel/__init__.py#L18-L28", "repo": "hc-api-python", "func_name": "export_analytics_data_to_excel", "original_string": ["def export_analytics_data_to_excel(data, output_file_name, result_info_key, identifier_keys):\n", "    \"\"\"Creates an Excel file containing data returned by the Analytics API\n", "\n", "    Args:\n", "        data: Analytics API data as a list of dicts\n", "        output_file_name: File name for output Excel file (use .xlsx extension).\n", "\n", "    \"\"\"\n", "    workbook = create_excel_workbook(data, result_info_key, identifier_keys)\n", "    workbook.save(output_file_name)\n", "    print('Saved Excel file to {}'.format(output_file_name))\n"], "language": "python", "code": "def export_analytics_data_to_excel(data, output_file_name, result_info_key,\n    identifier_keys):\n    \"\"\"\"\"\"\n    workbook = create_excel_workbook(data, result_info_key, identifier_keys)\n    workbook.save(output_file_name)\n    print('Saved Excel file to {}'.format(output_file_name))\n", "code_tokens": ["export", "analytics", "data", "to", "excel", "data", "output", "file", "name", "result", "info", "key", "identifier", "keys", "workbook", "create", "excel", "workbook", "data", "result", "info", "key", "identifier", "keys", "workbook", "save", "output", "file", "name", "print", "saved", "excel", "file", "to", "format", "output", "file", "name"], "docstring": "export to excel", "docstring_tokens": ["export", "to", "excel"], "idx": 139}
{"url": "https://github.com/rwl/pylon/blob/916514255db1ae1661406f0283df756baf960d14/contrib/pylontk.py#L377-L381", "repo": "pylon", "func_name": "on_excel", "original_string": ["    def on_excel(self):\n", "        from pylon.io.excel import ExcelWriter\n", "        filename = asksaveasfilename(filetypes=[(\"Excel file\", \".xls\")])\n", "        if filename:\n", "            ExcelWriter(self.case).write(filename)\n"], "language": "python", "code": "def on_excel(self):\n    from pylon.io.excel import ExcelWriter\n    filename = asksaveasfilename(filetypes=[('Excel file', '.xls')])\n    if filename:\n        ExcelWriter(self.case).write(filename)\n", "code_tokens": ["on", "excel", "self", "from", "pylon", "io", "excel", "import", "excelwriter", "filename", "asksaveasfilename", "filetypes", "excel", "file", "xls", "if", "filename", "excelwriter", "self", "case", "write", "filename"], "docstring": "export to excel", "docstring_tokens": ["export", "to", "excel"], "idx": 140}
{"url": "https://github.com/MacHu-GWU/pymongo_mate-project/blob/be53170c2db54cb705b9e548d32ef26c773ff7f3/pymongo_mate/pkg/pandas_mate/sql_io.py#L55-L97", "repo": "pymongo_mate-project", "func_name": "excel_to_sql", "original_string": ["def excel_to_sql(excel_file_path, engine,\n", "                 read_excel_kwargs=None,\n", "                 to_generic_type_kwargs=None,\n", "                 to_sql_kwargs=None):\n", "    \"\"\"Create a database from excel.\n", "\n", "    :param read_excel_kwargs: dict, arguments for ``pandas.read_excel`` method.\n", "      example: ``{\"employee\": {\"skiprows\": 10}, \"department\": {}}``\n", "    :param to_sql_kwargs: dict, arguments for ``pandas.DataFrame.to_sql`` \n", "      method.\n", "\n", "    limitation:\n", "\n", "    1. If a integer column has None value, data type in database will be float.\n", "      Because pandas thinks that it is ``np.nan``.\n", "    2. If a string column looks like integer, ``pandas.read_excel()`` method\n", "      doesn't have options to convert it to string.\n", "    \"\"\"\n", "    if read_excel_kwargs is None:\n", "        read_excel_kwargs = dict()\n", "\n", "    if to_sql_kwargs is None:\n", "        to_sql_kwargs = dict()\n", "\n", "    if to_generic_type_kwargs is None:\n", "        to_generic_type_kwargs = dict()\n", "\n", "    xl = pd.ExcelFile(excel_file_path)\n", "    for sheet_name in xl.sheet_names:\n", "        df = pd.read_excel(\n", "            excel_file_path, sheet_name,\n", "            **read_excel_kwargs.get(sheet_name, dict())\n", "        )\n", "\n", "        kwargs = to_generic_type_kwargs.get(sheet_name)\n", "        if kwargs:\n", "            data = to_dict_list_generic_type(df, **kwargs)\n", "            smart_insert(data, sheet_name, engine)\n", "        else:\n", "            df.to_sql(\n", "                sheet_name, engine, index=False,\n", "                **to_sql_kwargs.get(sheet_name, dict(if_exists=\"replace\"))\n", "            )\n"], "language": "python", "code": "def excel_to_sql(excel_file_path, engine, read_excel_kwargs=None,\n    to_generic_type_kwargs=None, to_sql_kwargs=None):\n    \"\"\"\"\"\"\n    if read_excel_kwargs is None:\n        read_excel_kwargs = dict()\n    if to_sql_kwargs is None:\n        to_sql_kwargs = dict()\n    if to_generic_type_kwargs is None:\n        to_generic_type_kwargs = dict()\n    xl = pd.ExcelFile(excel_file_path)\n    for sheet_name in xl.sheet_names:\n        df = pd.read_excel(excel_file_path, sheet_name, **read_excel_kwargs\n            .get(sheet_name, dict()))\n        kwargs = to_generic_type_kwargs.get(sheet_name)\n        if kwargs:\n            data = to_dict_list_generic_type(df, **kwargs)\n            smart_insert(data, sheet_name, engine)\n        else:\n            df.to_sql(sheet_name, engine, index=False, **to_sql_kwargs.get(\n                sheet_name, dict(if_exists='replace')))\n", "code_tokens": ["excel", "to", "sql", "excel", "file", "path", "engine", "read", "excel", "kwargs", "none", "to", "generic", "type", "kwargs", "none", "to", "sql", "kwargs", "none", "if", "read", "excel", "kwargs", "is", "none", "read", "excel", "kwargs", "dict", "if", "to", "sql", "kwargs", "is", "none", "to", "sql", "kwargs", "dict", "if", "to", "generic", "type", "kwargs", "is", "none", "to", "generic", "type", "kwargs", "dict", "xl", "pd", "excelfile", "excel", "file", "path", "for", "sheet", "name", "in", "xl", "sheet", "names", "df", "pd", "read", "excel", "excel", "file", "path", "sheet", "name", "read", "excel", "kwargs", "get", "sheet", "name", "dict", "kwargs", "to", "generic", "type", "kwargs", "get", "sheet", "name", "if", "kwargs", "data", "to", "dict", "list", "generic", "type", "df", "kwargs", "smart", "insert", "data", "sheet", "name", "engine", "else", "df", "to", "sql", "sheet", "name", "engine", "index", "false", "to", "sql", "kwargs", "get", "sheet", "name", "dict", "if", "exists", "replace"], "docstring": "export to excel", "docstring_tokens": ["export", "to", "excel"], "idx": 141}
{"url": "https://github.com/APSL/transmanager/blob/79157085840008e146b264521681913090197ed1/transmanager/export.py#L58-L67", "repo": "transmanager", "func_name": "export_translations", "original_string": ["    def export_translations(tasks_ids):\n", "        qs = TransTask.objects.filter(pk__in=tasks_ids)\n", "        export = ExportQueryset(\n", "            qs,\n", "            TransTask,\n", "            ('id', 'object_name', 'object_pk', 'object_field_label', 'object_field_value', 'number_of_words',\n", "             'object_field_value_translation', 'date_modification', 'done')\n", "        )\n", "        excel = export.get_excel()\n", "        return excel\n"], "language": "python", "code": "def export_translations(tasks_ids):\n    qs = TransTask.objects.filter(pk__in=tasks_ids)\n    export = ExportQueryset(qs, TransTask, ('id', 'object_name',\n        'object_pk', 'object_field_label', 'object_field_value',\n        'number_of_words', 'object_field_value_translation',\n        'date_modification', 'done'))\n    excel = export.get_excel()\n    return excel\n", "code_tokens": ["export", "translations", "tasks", "ids", "qs", "transtask", "objects", "filter", "pk", "in", "tasks", "ids", "export", "exportqueryset", "qs", "transtask", "id", "object", "name", "object", "pk", "object", "field", "label", "object", "field", "value", "number", "of", "words", "object", "field", "value", "translation", "date", "modification", "done", "excel", "export", "get", "excel", "return", "excel"], "docstring": "export to excel", "docstring_tokens": ["export", "to", "excel"], "idx": 142}
{"url": "https://github.com/fkarb/xltable/blob/7a592642d27ad5ee90d2aa8c26338abaa9d84bea/xltable/workbook.py#L82-L123", "repo": "xltable", "func_name": "to_excel", "original_string": ["    def to_excel(self, xl_app=None, resize_columns=True):\n", "        from win32com.client import Dispatch, gencache\n", "\n", "        if xl_app is None:\n", "            xl_app = Dispatch(\"Excel.Application\")\n", "        xl_app = gencache.EnsureDispatch(xl_app)\n", "\n", "        # Add a new workbook with the correct number of sheets.\n", "        # We aren't allowed to create an empty one.\n", "        assert self.worksheets, \"Can't export workbook with no worksheets\"\n", "        sheets_in_new_workbook = xl_app.SheetsInNewWorkbook\n", "        try:\n", "            xl_app.SheetsInNewWorkbook = float(len(self.worksheets))\n", "            self.workbook_obj = xl_app.Workbooks.Add()\n", "        finally:\n", "            xl_app.SheetsInNewWorkbook = sheets_in_new_workbook\n", "\n", "        # Rename the worksheets, ensuring that there can never be two sheets with the same\n", "        # name due to the sheets default names conflicting with the new names.\n", "        sheet_names = {s.name for s in self.worksheets}\n", "        assert len(sheet_names) == len(self.worksheets), \"Worksheets must have unique names\"\n", "        for worksheet in self.workbook_obj.Sheets:\n", "            i = 1\n", "            original_name = worksheet.Name\n", "            while worksheet.Name in sheet_names:\n", "                worksheet.Name = \"%s_%d\" % (original_name, i)\n", "                i += 1\n", "\n", "        for worksheet, sheet in zip(self.workbook_obj.Sheets, self.worksheets):\n", "            worksheet.Name = sheet.name\n", "\n", "        # Export each sheet (have to use itersheets for this as it sets the\n", "        # current active sheet before yielding each one).\n", "        for worksheet, sheet in zip(self.workbook_obj.Sheets, self.itersheets()):\n", "            worksheet.Select()\n", "            sheet.to_excel(workbook=self,\n", "                           worksheet=worksheet,\n", "                           xl_app=xl_app,\n", "                           rename=False,\n", "                           resize_columns=resize_columns)\n", "\n", "        return self.workbook_obj\n"], "language": "python", "code": "def to_excel(self, xl_app=None, resize_columns=True):\n    from win32com.client import Dispatch, gencache\n    if xl_app is None:\n        xl_app = Dispatch('Excel.Application')\n    xl_app = gencache.EnsureDispatch(xl_app)\n    assert self.worksheets, \"Can't export workbook with no worksheets\"\n    sheets_in_new_workbook = xl_app.SheetsInNewWorkbook\n    try:\n        xl_app.SheetsInNewWorkbook = float(len(self.worksheets))\n        self.workbook_obj = xl_app.Workbooks.Add()\n    finally:\n        xl_app.SheetsInNewWorkbook = sheets_in_new_workbook\n    sheet_names = {s.name for s in self.worksheets}\n    assert len(sheet_names) == len(self.worksheets\n        ), 'Worksheets must have unique names'\n    for worksheet in self.workbook_obj.Sheets:\n        i = 1\n        original_name = worksheet.Name\n        while worksheet.Name in sheet_names:\n            worksheet.Name = '%s_%d' % (original_name, i)\n            i += 1\n    for worksheet, sheet in zip(self.workbook_obj.Sheets, self.worksheets):\n        worksheet.Name = sheet.name\n    for worksheet, sheet in zip(self.workbook_obj.Sheets, self.itersheets()):\n        worksheet.Select()\n        sheet.to_excel(workbook=self, worksheet=worksheet, xl_app=xl_app,\n            rename=False, resize_columns=resize_columns)\n    return self.workbook_obj\n", "code_tokens": ["to", "excel", "self", "xl", "app", "none", "resize", "columns", "true", "from", "client", "import", "dispatch", "gencache", "if", "xl", "app", "is", "none", "xl", "app", "dispatch", "excel", "application", "xl", "app", "gencache", "ensuredispatch", "xl", "app", "assert", "self", "worksheets", "can", "export", "workbook", "with", "no", "worksheets", "sheets", "in", "new", "workbook", "xl", "app", "sheetsinnewworkbook", "try", "xl", "app", "sheetsinnewworkbook", "float", "len", "self", "worksheets", "self", "workbook", "obj", "xl", "app", "workbooks", "add", "finally", "xl", "app", "sheetsinnewworkbook", "sheets", "in", "new", "workbook", "sheet", "names", "name", "for", "in", "self", "worksheets", "assert", "len", "sheet", "names", "len", "self", "worksheets", "worksheets", "must", "have", "unique", "names", "for", "worksheet", "in", "self", "workbook", "obj", "sheets", "1", "original", "name", "worksheet", "name", "while", "worksheet", "name", "in", "sheet", "names", "worksheet", "name", "original", "name", "1", "for", "worksheet", "sheet", "in", "zip", "self", "workbook", "obj", "sheets", "self", "worksheets", "worksheet", "name", "sheet", "name", "for", "worksheet", "sheet", "in", "zip", "self", "workbook", "obj", "sheets", "self", "itersheets", "worksheet", "select", "sheet", "to", "excel", "workbook", "self", "worksheet", "worksheet", "xl", "app", "xl", "app", "rename", "false", "resize", "columns", "resize", "columns", "return", "self", "workbook", "obj"], "docstring": "export to excel", "docstring_tokens": ["export", "to", "excel"], "idx": 143}
{"url": "https://github.com/mcs07/ChemDataExtractor/blob/349a3bea965f2073141d62043b89319222e46af1/chemdataextractor/text/processors.py#L91-L95", "repo": "ChemDataExtractor", "func_name": "__call__", "original_string": ["    def __call__(self, value):\n", "        for substring in self.substrings:\n", "            if value.startswith(substring):\n", "                return value[len(substring):]\n", "        return value\n"], "language": "python", "code": "def __call__(self, value):\n    for substring in self.substrings:\n        if value.startswith(substring):\n            return value[len(substring):]\n    return value\n", "code_tokens": ["call", "self", "value", "for", "substring", "in", "self", "substrings", "if", "value", "startswith", "substring", "return", "value", "len", "substring", "return", "value"], "docstring": "positions of substrings in string", "docstring_tokens": ["positions", "of", "substrings", "in", "string"], "idx": 144}
{"url": "https://github.com/hollenstein/maspy/blob/f15fcfd24df306d8420540460d902aa3073ec133/maspy/auxiliary.py#L471-L491", "repo": "maspy", "func_name": "findAllSubstrings", "original_string": ["def findAllSubstrings(string, substring):\n", "    \"\"\" Returns a list of all substring starting positions in string or an empty\n", "    list if substring is not present in string.\n", "\n", "    :param string: a template string\n", "    :param substring: a string, which is looked for in the ``string`` parameter.\n", "\n", "    :returns: a list of substring starting positions in the template string\n", "    \"\"\"\n", "    #TODO: solve with regex? what about '.':\n", "    #return [m.start() for m in re.finditer('(?='+substring+')', string)]\n", "    start = 0\n", "    positions = []\n", "    while True:\n", "        start = string.find(substring, start)\n", "        if start == -1:\n", "            break\n", "        positions.append(start)\n", "        #+1 instead of +len(substring) to also find overlapping matches\n", "        start += 1\n", "    return positions\n"], "language": "python", "code": "def findAllSubstrings(string, substring):\n    \"\"\"\"\"\"\n    start = 0\n    positions = []\n    while True:\n        start = string.find(substring, start)\n        if start == -1:\n            break\n        positions.append(start)\n        start += 1\n    return positions\n", "code_tokens": ["findallsubstrings", "string", "substring", "start", "0", "positions", "while", "true", "start", "string", "find", "substring", "start", "if", "start", "1", "break", "positions", "append", "start", "start", "1", "return", "positions"], "docstring": "positions of substrings in string", "docstring_tokens": ["positions", "of", "substrings", "in", "string"], "idx": 145}
{"url": "https://github.com/alvations/lazyme/blob/961a8282198588ff72e15643f725ce895e51d06d/lazyme/string.py#L45-L53", "repo": "lazyme", "func_name": "deduplicate", "original_string": ["def deduplicate(s, ch):\n", "    \"\"\"\n", "    From http://stackoverflow.com/q/42216559/610569\n", "\n", "        s = 'this is   an   irritating string with  random spacing  .'\n", "        deduplicate(s)\n", "        'this is an irritating string with random spacing .'\n", "    \"\"\"\n", "    return ch.join([substring for substring in s.strip().split(ch) if substring])\n"], "language": "python", "code": "def deduplicate(s, ch):\n    \"\"\"\"\"\"\n    return ch.join([substring for substring in s.strip().split(ch) if\n        substring])\n", "code_tokens": ["deduplicate", "ch", "return", "ch", "join", "substring", "for", "substring", "in", "strip", "split", "ch", "if", "substring"], "docstring": "positions of substrings in string", "docstring_tokens": ["positions", "of", "substrings", "in", "string"], "idx": 146}
{"url": "https://github.com/cfusting/fastgp/blob/6cf3c5d14abedaea064feef6ca434ee806a11756/fastgp/algorithms/evolutionary_feature_synthesis.py#L314-L332", "repo": "fastgp", "func_name": "build_operation_stack", "original_string": ["def build_operation_stack(string):\n", "    stack = []\n", "    start = 0\n", "    for i, s in enumerate(string):\n", "        if s == '(':\n", "            substring = string[start:i]\n", "            start = i + 1\n", "            stack.append(substring)\n", "        elif s == ',':\n", "            if i != start:\n", "                substring = string[start:i]\n", "                stack.append(substring)\n", "            start = i + 1\n", "        elif s == ')':\n", "            if i != start:\n", "                substring = string[start:i]\n", "                stack.append(substring)\n", "            start = i + 1\n", "    return stack\n"], "language": "python", "code": "def build_operation_stack(string):\n    stack = []\n    start = 0\n    for i, s in enumerate(string):\n        if s == '(':\n            substring = string[start:i]\n            start = i + 1\n            stack.append(substring)\n        elif s == ',':\n            if i != start:\n                substring = string[start:i]\n                stack.append(substring)\n            start = i + 1\n        elif s == ')':\n            if i != start:\n                substring = string[start:i]\n                stack.append(substring)\n            start = i + 1\n    return stack\n", "code_tokens": ["build", "operation", "stack", "string", "stack", "start", "0", "for", "in", "enumerate", "string", "if", "substring", "string", "start", "start", "1", "stack", "append", "substring", "elif", "if", "start", "substring", "string", "start", "stack", "append", "substring", "start", "1", "elif", "if", "start", "substring", "string", "start", "stack", "append", "substring", "start", "1", "return", "stack"], "docstring": "positions of substrings in string", "docstring_tokens": ["positions", "of", "substrings", "in", "string"], "idx": 147}
{"url": "https://github.com/mcs07/ChemDataExtractor/blob/349a3bea965f2073141d62043b89319222e46af1/chemdataextractor/text/processors.py#L104-L108", "repo": "ChemDataExtractor", "func_name": "__call__", "original_string": ["    def __call__(self, value):\n", "        for substring in self.substrings:\n", "            if value.endswith(substring):\n", "                return value[:-len(substring)]\n", "        return value\n"], "language": "python", "code": "def __call__(self, value):\n    for substring in self.substrings:\n        if value.endswith(substring):\n            return value[:-len(substring)]\n    return value\n", "code_tokens": ["call", "self", "value", "for", "substring", "in", "self", "substrings", "if", "value", "endswith", "substring", "return", "value", "len", "substring", "return", "value"], "docstring": "positions of substrings in string", "docstring_tokens": ["positions", "of", "substrings", "in", "string"], "idx": 148}
{"url": "https://github.com/zeekay/soundcloud-cli/blob/8a83013683e1acf32f093239bbb6d3c02bc50b37/soundcloud_cli/utils.py#L7-L23", "repo": "soundcloud-cli", "func_name": "copy_to_clipboard", "original_string": ["def copy_to_clipboard(text):\n", "    # reliable on mac\n", "    if sys.platform == 'darwin':\n", "        os.system('echo \"{0}\" | pbcopy'.format(text))\n", "        return\n", "\n", "    # okay we'll try cross-platform way\n", "    try:\n", "        from Tkinter import Tk\n", "    except ImportError:\n", "        return\n", "\n", "    r = Tk()\n", "    r.withdraw()\n", "    r.clipboard_clear()\n", "    r.clipboard_append(text.encode('ascii'))\n", "    r.destroy()\n"], "language": "python", "code": "def copy_to_clipboard(text):\n    if sys.platform == 'darwin':\n        os.system('echo \"{0}\" | pbcopy'.format(text))\n        return\n    try:\n        from Tkinter import Tk\n    except ImportError:\n        return\n    r = Tk()\n    r.withdraw()\n    r.clipboard_clear()\n    r.clipboard_append(text.encode('ascii'))\n    r.destroy()\n", "code_tokens": ["copy", "to", "clipboard", "text", "if", "sys", "platform", "darwin", "os", "system", "echo", "0", "pbcopy", "format", "text", "return", "try", "from", "tkinter", "import", "tk", "except", "importerror", "return", "tk", "withdraw", "clipboard", "clear", "clipboard", "append", "text", "encode", "ascii", "destroy"], "docstring": "copy to clipboard", "docstring_tokens": ["copy", "to", "clipboard"], "idx": 149}
{"url": "https://github.com/devassistant/devassistant/blob/2dbfeaa666a64127263664d18969c55d19ecc83e/devassistant/gui/gui_helper.py#L466-L473", "repo": "devassistant", "func_name": "create_clipboard", "original_string": ["    def create_clipboard(self, text, selection=Gdk.SELECTION_CLIPBOARD):\n", "        \"\"\"\n", "        Function creates a clipboard\n", "        \"\"\"\n", "        clipboard = Gtk.Clipboard.get(selection)\n", "        clipboard.set_text('\\n'.join(text), -1)\n", "        clipboard.store()\n", "        return clipboard\n"], "language": "python", "code": "def create_clipboard(self, text, selection=Gdk.SELECTION_CLIPBOARD):\n    \"\"\"\"\"\"\n    clipboard = Gtk.Clipboard.get(selection)\n    clipboard.set_text('\\n'.join(text), -1)\n    clipboard.store()\n    return clipboard\n", "code_tokens": ["create", "clipboard", "self", "text", "selection", "gdk", "selection", "clipboard", "clipboard", "gtk", "clipboard", "get", "selection", "clipboard", "set", "text", "join", "text", "1", "clipboard", "store", "return", "clipboard"], "docstring": "copy to clipboard", "docstring_tokens": ["copy", "to", "clipboard"], "idx": 150}
{"url": "https://github.com/bitesofcode/projexui/blob/f18a73bec84df90b034ca69b9deea118dbedfc4d/projexui/widgets/xfilepathedit.py#L136-L142", "repo": "projexui", "func_name": "copyFilepath", "original_string": ["    def copyFilepath( self ):\n", "        \"\"\"\n", "        Copies the current filepath contents to the current clipboard.\n", "        \"\"\"\n", "        clipboard = QApplication.instance().clipboard()\n", "        clipboard.setText(self.filepath())\n", "        clipboard.setText(self.filepath(), clipboard.Selection)\n"], "language": "python", "code": "def copyFilepath(self):\n    \"\"\"\"\"\"\n    clipboard = QApplication.instance().clipboard()\n    clipboard.setText(self.filepath())\n    clipboard.setText(self.filepath(), clipboard.Selection)\n", "code_tokens": ["copyfilepath", "self", "clipboard", "qapplication", "instance", "clipboard", "clipboard", "settext", "self", "filepath", "clipboard", "settext", "self", "filepath", "clipboard", "selection"], "docstring": "copy to clipboard", "docstring_tokens": ["copy", "to", "clipboard"], "idx": 151}
{"url": "https://github.com/rvswift/EB/blob/341880b79faf8147dc9fa6e90438531cd09fabcc/EB/builder/splitter/splitter.py#L37-L57", "repo": "EB", "func_name": "write_csv_header", "original_string": ["def write_csv_header(mol, csv_writer):\n", "    \"\"\"\n", "\tWrite the csv header\n", "\t\"\"\"\n", "\n", "    # create line list where line elements for writing will be stored\n", "    line = []\n", "\n", "    # ID\n", "    line.append('id')\n", "\n", "    # status\n", "    line.append('status')\n", "\n", "    # query labels\n", "    queryList = mol.properties.keys()\n", "    for queryLabel in queryList:\n", "        line.append(queryLabel)\n", "\n", "    # write line\n", "    csv_writer.writerow(line)\n"], "language": "python", "code": "def write_csv_header(mol, csv_writer):\n    \"\"\"\"\"\"\n    line = []\n    line.append('id')\n    line.append('status')\n    queryList = mol.properties.keys()\n    for queryLabel in queryList:\n        line.append(queryLabel)\n    csv_writer.writerow(line)\n", "code_tokens": ["write", "csv", "header", "mol", "csv", "writer", "line", "line", "append", "id", "line", "append", "status", "querylist", "mol", "properties", "keys", "for", "querylabel", "in", "querylist", "line", "append", "querylabel", "csv", "writer", "writerow", "line"], "docstring": "write csv", "docstring_tokens": ["write", "csv"], "idx": 152}
{"url": "https://github.com/quantopian/zipline/blob/77ad15e6dc4c1cbcdc133653bac8a63fc704f7fe/zipline/data/bcolz_daily_bars.py#L209-L237", "repo": "zipline", "func_name": "write_csvs", "original_string": ["    def write_csvs(self,\n", "                   asset_map,\n", "                   show_progress=False,\n", "                   invalid_data_behavior='warn'):\n", "        \"\"\"Read CSVs as DataFrames from our asset map.\n", "\n", "        Parameters\n", "        ----------\n", "        asset_map : dict[int -> str]\n", "            A mapping from asset id to file path with the CSV data for that\n", "            asset\n", "        show_progress : bool\n", "            Whether or not to show a progress bar while writing.\n", "        invalid_data_behavior : {'warn', 'raise', 'ignore'}\n", "            What to do when data is encountered that is outside the range of\n", "            a uint32.\n", "        \"\"\"\n", "        read = partial(\n", "            read_csv,\n", "            parse_dates=['day'],\n", "            index_col='day',\n", "            dtype=self._csv_dtypes,\n", "        )\n", "        return self.write(\n", "            ((asset, read(path)) for asset, path in iteritems(asset_map)),\n", "            assets=viewkeys(asset_map),\n", "            show_progress=show_progress,\n", "            invalid_data_behavior=invalid_data_behavior,\n", "        )\n"], "language": "python", "code": "def write_csvs(self, asset_map, show_progress=False, invalid_data_behavior=\n    'warn'):\n    \"\"\"\"\"\"\n    read = partial(read_csv, parse_dates=['day'], index_col='day', dtype=\n        self._csv_dtypes)\n    return self.write(((asset, read(path)) for asset, path in iteritems(\n        asset_map)), assets=viewkeys(asset_map), show_progress=\n        show_progress, invalid_data_behavior=invalid_data_behavior)\n", "code_tokens": ["write", "csvs", "self", "asset", "map", "show", "progress", "false", "invalid", "data", "behavior", "warn", "read", "partial", "read", "csv", "parse", "dates", "day", "index", "col", "day", "dtype", "self", "csv", "dtypes", "return", "self", "write", "asset", "read", "path", "for", "asset", "path", "in", "iteritems", "asset", "map", "assets", "viewkeys", "asset", "map", "show", "progress", "show", "progress", "invalid", "data", "behavior", "invalid", "data", "behavior"], "docstring": "write csv", "docstring_tokens": ["write", "csv"], "idx": 153}
{"url": "https://github.com/jordanjoz1/flickr-views-counter/blob/cba89285823ab39afd9f40a19db82371d45bd830/count_views.py#L130-L138", "repo": "flickr-views-counter", "func_name": "write_to_csv", "original_string": ["def write_to_csv(fname, header, rows):\n", "    with open(fname, 'wb') as csvfile:\n", "        csvwriter = csv.writer(csvfile, delimiter=',', quotechar='|',\n", "                               quoting=csv.QUOTE_MINIMAL)\n", "        csvwriter.writerow(header)\n", "        for row in rows:\n", "            csvwriter.writerow(\n", "                [s.encode(\"utf-8\").replace(',', '').replace('\\n', '')\n", "                 for s in row])\n"], "language": "python", "code": "def write_to_csv(fname, header, rows):\n    with open(fname, 'wb') as csvfile:\n        csvwriter = csv.writer(csvfile, delimiter=',', quotechar='|',\n            quoting=csv.QUOTE_MINIMAL)\n        csvwriter.writerow(header)\n        for row in rows:\n            csvwriter.writerow([s.encode('utf-8').replace(',', '').replace(\n                '\\n', '') for s in row])\n", "code_tokens": ["write", "to", "csv", "fname", "header", "rows", "with", "open", "fname", "wb", "as", "csvfile", "csvwriter", "csv", "writer", "csvfile", "delimiter", "quotechar", "quoting", "csv", "quote", "minimal", "csvwriter", "writerow", "header", "for", "row", "in", "rows", "csvwriter", "writerow", "encode", "utf", "8", "replace", "replace", "for", "in", "row"], "docstring": "write csv", "docstring_tokens": ["write", "csv"], "idx": 154}
{"url": "https://github.com/googledatalab/pydatalab/blob/d9031901d5bca22fe0d5925d204e6698df9852e1/solutionbox/structured_data/mltoolbox/_structured_data/prediction/predict.py#L307-L346", "repo": "pydatalab", "func_name": "expand", "original_string": ["  def expand(self, datasets):\n", "    import json\n", "\n", "    tf_graph_predictions, errors = datasets\n", "\n", "    if self._output_format == 'json':\n", "      (tf_graph_predictions |\n", "       'Write Raw JSON' >>\n", "       beam.io.textio.WriteToText(os.path.join(self._output_dir, 'predictions'),\n", "                                  file_name_suffix='.json',\n", "                                  coder=RawJsonCoder(),\n", "                                  shard_name_template=self._shard_name_template))\n", "    elif self._output_format == 'csv':\n", "      # make a csv header file\n", "      header = [col['name'] for col in self._schema]\n", "      csv_coder = CSVCoder(header)\n", "      (tf_graph_predictions.pipeline |\n", "       'Make CSV Header' >>\n", "       beam.Create([json.dumps(self._schema, indent=2)]) |\n", "       'Write CSV Schema File' >>\n", "       beam.io.textio.WriteToText(os.path.join(self._output_dir, 'csv_schema'),\n", "                                  file_name_suffix='.json',\n", "                                  shard_name_template=''))\n", "\n", "      # Write the csv predictions\n", "      (tf_graph_predictions |\n", "       'Write CSV' >>\n", "       beam.io.textio.WriteToText(os.path.join(self._output_dir, 'predictions'),\n", "                                  file_name_suffix='.csv',\n", "                                  coder=csv_coder,\n", "                                  shard_name_template=self._shard_name_template))\n", "    else:\n", "      raise ValueError('FormatAndSave: unknown format %s', self._output_format)\n", "\n", "    # Write the errors to a text file.\n", "    (errors |\n", "     'Write Errors' >>\n", "     beam.io.textio.WriteToText(os.path.join(self._output_dir, 'errors'),\n", "                                file_name_suffix='.txt',\n", "                                shard_name_template=self._shard_name_template))\n"], "language": "python", "code": "def expand(self, datasets):\n    import json\n    tf_graph_predictions, errors = datasets\n    if self._output_format == 'json':\n        tf_graph_predictions | 'Write Raw JSON' >> beam.io.textio.WriteToText(\n            os.path.join(self._output_dir, 'predictions'), file_name_suffix\n            ='.json', coder=RawJsonCoder(), shard_name_template=self.\n            _shard_name_template)\n    elif self._output_format == 'csv':\n        header = [col['name'] for col in self._schema]\n        csv_coder = CSVCoder(header)\n        tf_graph_predictions.pipeline | 'Make CSV Header' >> beam.Create([\n            json.dumps(self._schema, indent=2)]\n            ) | 'Write CSV Schema File' >> beam.io.textio.WriteToText(os.\n            path.join(self._output_dir, 'csv_schema'), file_name_suffix=\n            '.json', shard_name_template='')\n        tf_graph_predictions | 'Write CSV' >> beam.io.textio.WriteToText(os\n            .path.join(self._output_dir, 'predictions'), file_name_suffix=\n            '.csv', coder=csv_coder, shard_name_template=self.\n            _shard_name_template)\n    else:\n        raise ValueError('FormatAndSave: unknown format %s', self.\n            _output_format)\n    errors | 'Write Errors' >> beam.io.textio.WriteToText(os.path.join(self\n        ._output_dir, 'errors'), file_name_suffix='.txt',\n        shard_name_template=self._shard_name_template)\n", "code_tokens": ["expand", "self", "datasets", "import", "json", "tf", "graph", "predictions", "errors", "datasets", "if", "self", "output", "format", "json", "tf", "graph", "predictions", "write", "raw", "json", "beam", "io", "textio", "writetotext", "os", "path", "join", "self", "output", "dir", "predictions", "file", "name", "suffix", "json", "coder", "rawjsoncoder", "shard", "name", "template", "self", "shard", "name", "template", "elif", "self", "output", "format", "csv", "header", "col", "name", "for", "col", "in", "self", "schema", "csv", "coder", "csvcoder", "header", "tf", "graph", "predictions", "pipeline", "make", "csv", "header", "beam", "create", "json", "dumps", "self", "schema", "indent", "2", "write", "csv", "schema", "file", "beam", "io", "textio", "writetotext", "os", "path", "join", "self", "output", "dir", "csv", "schema", "file", "name", "suffix", "json", "shard", "name", "template", "tf", "graph", "predictions", "write", "csv", "beam", "io", "textio", "writetotext", "os", "path", "join", "self", "output", "dir", "predictions", "file", "name", "suffix", "csv", "coder", "csv", "coder", "shard", "name", "template", "self", "shard", "name", "template", "else", "raise", "valueerror", "formatandsave", "unknown", "format", "self", "output", "format", "errors", "write", "errors", "beam", "io", "textio", "writetotext", "os", "path", "join", "self", "output", "dir", "errors", "file", "name", "suffix", "txt", "shard", "name", "template", "self", "shard", "name", "template"], "docstring": "write csv", "docstring_tokens": ["write", "csv"], "idx": 155}
{"url": "https://github.com/rvswift/EB/blob/341880b79faf8147dc9fa6e90438531cd09fabcc/EB/builder/splitter/splitter.py#L90-L113", "repo": "EB", "func_name": "csv_writer", "original_string": ["def csv_writer(molecules, options, prefix):\n", "    \"\"\"\n", "\tWrite a csv file.\n", "\t\"\"\"\n", "\n", "    # output file\n", "    outdir = os.getcwd()\n", "    filename = prefix + '.csv'\n", "    outfile = os.path.join(outdir, filename)\n", "\n", "    # initiate csv writer object\n", "    f = open(outfile, 'w')\n", "    csv_writer = csv.writer(f)\n", "\n", "    # write csv header\n", "    mol = molecules[0]\n", "    write_csv_header(mol, csv_writer)\n", "\n", "    # write csv lines\n", "    for mol in molecules:\n", "        write_csv_line(mol, csv_writer, options)\n", "\n", "    # close file\n", "    f.close()\n"], "language": "python", "code": "def csv_writer(molecules, options, prefix):\n    \"\"\"\"\"\"\n    outdir = os.getcwd()\n    filename = prefix + '.csv'\n    outfile = os.path.join(outdir, filename)\n    f = open(outfile, 'w')\n    csv_writer = csv.writer(f)\n    mol = molecules[0]\n    write_csv_header(mol, csv_writer)\n    for mol in molecules:\n        write_csv_line(mol, csv_writer, options)\n    f.close()\n", "code_tokens": ["csv", "writer", "molecules", "options", "prefix", "outdir", "os", "getcwd", "filename", "prefix", "csv", "outfile", "os", "path", "join", "outdir", "filename", "open", "outfile", "csv", "writer", "csv", "writer", "mol", "molecules", "0", "write", "csv", "header", "mol", "csv", "writer", "for", "mol", "in", "molecules", "write", "csv", "line", "mol", "csv", "writer", "options", "close"], "docstring": "write csv", "docstring_tokens": ["write", "csv"], "idx": 156}
{"url": "https://github.com/gem/oq-engine/blob/8294553a0b8aba33fd96437a35065d03547d0040/openquake/hmtk/seismicity/smoothing/smoothed_seismicity.py#L491-L518", "repo": "oq-engine", "func_name": "write_to_csv", "original_string": ["    def write_to_csv(self, filename):\n", "        '''\n", "        Exports to simple csv\n", "\n", "        :param str filename:\n", "            Path to file for export\n", "        '''\n", "        fid = open(filename, 'wt')\n", "        # Create header list\n", "        header_info = ['Longitude', 'Latitude', 'Depth', 'Observed Count',\n", "                       'Smoothed Rate', 'b-value']\n", "        writer = csv.DictWriter(fid, fieldnames=header_info)\n", "        headers = dict((name0, name0) for name0 in header_info)\n", "        # Write to file\n", "        writer.writerow(headers)\n", "        for row in self.data:\n", "            # institute crude compression by omitting points with no seismicity\n", "            # and taking advantage of the %g format\n", "            if row[4] == 0:\n", "                continue\n", "            row_dict = {'Longitude': '%g' % row[0],\n", "                        'Latitude': '%g' % row[1],\n", "                        'Depth': '%g' % row[2],\n", "                        'Observed Count': '%d' % row[3],\n", "                        'Smoothed Rate': '%.6g' % row[4],\n", "                        'b-value': '%g' % self.bval}\n", "            writer.writerow(row_dict)\n", "        fid.close()\n"], "language": "python", "code": "def write_to_csv(self, filename):\n    \"\"\"\"\"\"\n    fid = open(filename, 'wt')\n    header_info = ['Longitude', 'Latitude', 'Depth', 'Observed Count',\n        'Smoothed Rate', 'b-value']\n    writer = csv.DictWriter(fid, fieldnames=header_info)\n    headers = dict((name0, name0) for name0 in header_info)\n    writer.writerow(headers)\n    for row in self.data:\n        if row[4] == 0:\n            continue\n        row_dict = {'Longitude': '%g' % row[0], 'Latitude': '%g' % row[1],\n            'Depth': '%g' % row[2], 'Observed Count': '%d' % row[3],\n            'Smoothed Rate': '%.6g' % row[4], 'b-value': '%g' % self.bval}\n        writer.writerow(row_dict)\n    fid.close()\n", "code_tokens": ["write", "to", "csv", "self", "filename", "fid", "open", "filename", "wt", "header", "info", "longitude", "latitude", "depth", "observed", "count", "smoothed", "rate", "value", "writer", "csv", "dictwriter", "fid", "fieldnames", "header", "info", "headers", "dict", "for", "in", "header", "info", "writer", "writerow", "headers", "for", "row", "in", "self", "data", "if", "row", "4", "0", "continue", "row", "dict", "longitude", "row", "0", "latitude", "row", "1", "depth", "row", "2", "observed", "count", "row", "3", "smoothed", "rate", "row", "4", "value", "self", "bval", "writer", "writerow", "row", "dict", "fid", "close"], "docstring": "write csv", "docstring_tokens": ["write", "csv"], "idx": 157}
{"url": "https://github.com/probcomp/crosscat/blob/4a05bddb06a45f3b7b3e05e095720f16257d1535/src/utils/data_utils.py#L306-L311", "repo": "crosscat", "func_name": "write_csv", "original_string": ["def write_csv(filename, T, header = None):\n", "    with open(filename,'w') as fh:\n", "        csv_writer = csv.writer(fh, delimiter=',')\n", "        if header != None:\n", "            csv_writer.writerow(header)\n", "        [csv_writer.writerow(T[i]) for i in range(len(T))]\n"], "language": "python", "code": "def write_csv(filename, T, header=None):\n    with open(filename, 'w') as fh:\n        csv_writer = csv.writer(fh, delimiter=',')\n        if header != None:\n            csv_writer.writerow(header)\n        [csv_writer.writerow(T[i]) for i in range(len(T))]\n", "code_tokens": ["write", "csv", "filename", "header", "none", "with", "open", "filename", "as", "fh", "csv", "writer", "csv", "writer", "fh", "delimiter", "if", "header", "none", "csv", "writer", "writerow", "header", "csv", "writer", "writerow", "for", "in", "range", "len"], "docstring": "write csv", "docstring_tokens": ["write", "csv"], "idx": 158}
{"url": "https://github.com/keon/algorithms/blob/4d6569464a62a75c1357acc97e2dd32ee2f9f4a3/algorithms/strings/strip_url_params.py#L85-L95", "repo": "algorithms", "func_name": "strip_url_params3", "original_string": ["def strip_url_params3(url, strip=None):\n", "    if not strip: strip = []\n", "    \n", "    parse = urllib.parse.urlparse(url)\n", "    query = urllib.parse.parse_qs(parse.query)\n", "    \n", "    query = {k: v[0] for k, v in query.items() if k not in strip}\n", "    query = urllib.parse.urlencode(query)\n", "    new = parse._replace(query=query)\n", "    \n", "    return new.geturl()"], "language": "python", "code": "def strip_url_params3(url, strip=None):\n    if not strip:\n        strip = []\n    parse = urllib.parse.urlparse(url)\n    query = urllib.parse.parse_qs(parse.query)\n    query = {k: v[0] for k, v in query.items() if k not in strip}\n    query = urllib.parse.urlencode(query)\n    new = parse._replace(query=query)\n    return new.geturl()\n", "code_tokens": ["strip", "url", "url", "strip", "none", "if", "not", "strip", "strip", "parse", "urllib", "parse", "urlparse", "url", "query", "urllib", "parse", "parse", "qs", "parse", "query", "query", "0", "for", "in", "query", "items", "if", "not", "in", "strip", "query", "urllib", "parse", "urlencode", "query", "new", "parse", "replace", "query", "query", "return", "new", "geturl"], "docstring": "parse query string in url", "docstring_tokens": ["parse", "query", "string", "in", "url"], "idx": 159}
{"url": "https://github.com/wummel/linkchecker/blob/c2ce810c3fb00b895a841a7be6b2e78c64e7b042/linkcheck/url.py#L250-L272", "repo": "linkchecker", "func_name": "url_parse_query", "original_string": ["def url_parse_query (query, encoding=None):\n", "    \"\"\"Parse and re-join the given CGI query.\"\"\"\n", "    if isinstance(query, unicode):\n", "        if encoding is None:\n", "            encoding = url_encoding\n", "        query = query.encode(encoding, 'ignore')\n", "    # if ? is in the query, split it off, seen at msdn.microsoft.com\n", "    append = \"\"\n", "    while '?' in query:\n", "        query, rest = query.rsplit('?', 1)\n", "        append = '?'+url_parse_query(rest)+append\n", "    l = []\n", "    for k, v, sep in parse_qsl(query, keep_blank_values=True):\n", "        k = url_quote_part(k, '/-:,;')\n", "        if v:\n", "            v = url_quote_part(v, '/-:,;')\n", "            l.append(\"%s=%s%s\" % (k, v, sep))\n", "        elif v is None:\n", "            l.append(\"%s%s\" % (k, sep))\n", "        else:\n", "            # some sites do not work when the equal sign is missing\n", "            l.append(\"%s=%s\" % (k, sep))\n", "    return ''.join(l) + append\n"], "language": "python", "code": "def url_parse_query(query, encoding=None):\n    \"\"\"\"\"\"\n    if isinstance(query, unicode):\n        if encoding is None:\n            encoding = url_encoding\n        query = query.encode(encoding, 'ignore')\n    append = ''\n    while '?' in query:\n        query, rest = query.rsplit('?', 1)\n        append = '?' + url_parse_query(rest) + append\n    l = []\n    for k, v, sep in parse_qsl(query, keep_blank_values=True):\n        k = url_quote_part(k, '/-:,;')\n        if v:\n            v = url_quote_part(v, '/-:,;')\n            l.append('%s=%s%s' % (k, v, sep))\n        elif v is None:\n            l.append('%s%s' % (k, sep))\n        else:\n            l.append('%s=%s' % (k, sep))\n    return ''.join(l) + append\n", "code_tokens": ["url", "parse", "query", "query", "encoding", "none", "if", "isinstance", "query", "unicode", "if", "encoding", "is", "none", "encoding", "url", "encoding", "query", "query", "encode", "encoding", "ignore", "append", "while", "in", "query", "query", "rest", "query", "rsplit", "1", "append", "url", "parse", "query", "rest", "append", "for", "sep", "in", "parse", "qsl", "query", "keep", "blank", "values", "true", "url", "quote", "part", "if", "url", "quote", "part", "append", "sep", "elif", "is", "none", "append", "sep", "else", "append", "sep", "return", "join", "append"], "docstring": "parse query string in url", "docstring_tokens": ["parse", "query", "string", "in", "url"], "idx": 160}
{"url": "https://github.com/TeamHG-Memex/MaybeDont/blob/34721f67b69d426adda324a0ed905d3860828af9/maybedont/predict.py#L238-L241", "repo": "MaybeDont", "func_name": "_parse_url", "original_string": ["def _parse_url(url):\n", "    p = urlsplit(url)\n", "    query = {k: v[0] for k, v in parse_qs(p.query).items() if len(v) == 1}\n", "    return ''.join([p.netloc, p.path]), query\n"], "language": "python", "code": "def _parse_url(url):\n    p = urlsplit(url)\n    query = {k: v[0] for k, v in parse_qs(p.query).items() if len(v) == 1}\n    return ''.join([p.netloc, p.path]), query\n", "code_tokens": ["parse", "url", "url", "urlsplit", "url", "query", "0", "for", "in", "parse", "qs", "query", "items", "if", "len", "1", "return", "join", "netloc", "path", "query"], "docstring": "parse query string in url", "docstring_tokens": ["parse", "query", "string", "in", "url"], "idx": 161}
{"url": "https://github.com/thomasjiangcy/django-rest-mock/blob/09e91de20d1a5efd5c47c6e3d7fe979443012e2c/rest_mock_server/core/parser.py#L43-L52", "repo": "django-rest-mock", "func_name": "_parse_url", "original_string": ["    def _parse_url(url_string):\n", "        u = urlparse(url_string)\n", "        url = u.path\n", "        query = parse_qs(u.query)\n", "\n", "        return {\n", "            'full_url': url_string.strip(),\n", "            'url': url.strip(),\n", "            'query': query\n", "        }\n"], "language": "python", "code": "def _parse_url(url_string):\n    u = urlparse(url_string)\n    url = u.path\n    query = parse_qs(u.query)\n    return {'full_url': url_string.strip(), 'url': url.strip(), 'query': query}\n", "code_tokens": ["parse", "url", "url", "string", "urlparse", "url", "string", "url", "path", "query", "parse", "qs", "query", "return", "full", "url", "url", "string", "strip", "url", "url", "strip", "query", "query"], "docstring": "parse query string in url", "docstring_tokens": ["parse", "query", "string", "in", "url"], "idx": 162}
{"url": "https://github.com/vfxetc/sgactions/blob/e2c6ee14a1f8092e97f57a501650581428a6ac6e/sgactions/dispatch.py#L14-L33", "repo": "sgactions", "func_name": "parse_url", "original_string": ["def parse_url(url):\n", "\n", "    # Parse the URL into scheme, path, and query.\n", "    m = re.match(r'^(?:(\\w+):)?(.*?)(?:/(.*?))?(?:\\?(.*))?$', url)\n", "    scheme, netloc, path, query = m.groups()\n", "    \n", "    kwargs = urlparse.parse_qs(query, keep_blank_values=True) if query else {}\n", "    for k, v in kwargs.iteritems():\n", "        if len(v) == 1 and k not in ('cols', 'column_display_names'):\n", "            kwargs[k] = v = v[0]\n", "        if k.endswith('_id') and v.isdigit():\n", "            kwargs[k] = int(v)\n", "\n", "    # Parse the path into an entrypoint.\n", "    m = re.match(r'^([\\w.]+:\\w+)$', netloc)\n", "    if not m:\n", "        raise ValueError('entrypoint must be like \"package.module:function\"; got \"%s\"' % netloc)\n", "        return 1\n", "\n", "    return m.group(1), kwargs\n"], "language": "python", "code": "def parse_url(url):\n    m = re.match('^(?:(\\\\w+):)?(.*?)(?:/(.*?))?(?:\\\\?(.*))?$', url)\n    scheme, netloc, path, query = m.groups()\n    kwargs = urlparse.parse_qs(query, keep_blank_values=True) if query else {}\n    for k, v in kwargs.iteritems():\n        if len(v) == 1 and k not in ('cols', 'column_display_names'):\n            kwargs[k] = v = v[0]\n        if k.endswith('_id') and v.isdigit():\n            kwargs[k] = int(v)\n    m = re.match('^([\\\\w.]+:\\\\w+)$', netloc)\n    if not m:\n        raise ValueError(\n            'entrypoint must be like \"package.module:function\"; got \"%s\"' %\n            netloc)\n        return 1\n    return m.group(1), kwargs\n", "code_tokens": ["parse", "url", "url", "re", "match", "url", "scheme", "netloc", "path", "query", "groups", "kwargs", "urlparse", "parse", "qs", "query", "keep", "blank", "values", "true", "if", "query", "else", "for", "in", "kwargs", "iteritems", "if", "len", "1", "and", "not", "in", "cols", "column", "display", "names", "kwargs", "0", "if", "endswith", "id", "and", "isdigit", "kwargs", "int", "re", "match", "netloc", "if", "not", "raise", "valueerror", "entrypoint", "must", "be", "like", "package", "module", "function", "got", "netloc", "return", "1", "return", "group", "1", "kwargs"], "docstring": "parse query string in url", "docstring_tokens": ["parse", "query", "string", "in", "url"], "idx": 163}
{"url": "https://github.com/zeromake/aiko/blob/53b246fa88652466a9e38ac3d1a99a6198195b0f/aiko/request.py#L329-L336", "repo": "aiko", "func_name": "url", "original_string": ["    def url(self) -> str:\n", "        \"\"\"\n", "        path + query \u7684url\n", "        \"\"\"\n", "        url_str = self.parse_url.path or \"\"\n", "        if self.parse_url.querystring is not None:\n", "            url_str += \"?\" + self.parse_url.querystring\n", "        return url_str\n"], "language": "python", "code": "def url(self) ->str:\n    \"\"\"\"\"\"\n    url_str = self.parse_url.path or ''\n    if self.parse_url.querystring is not None:\n        url_str += '?' + self.parse_url.querystring\n    return url_str\n", "code_tokens": ["url", "self", "str", "url", "str", "self", "parse", "url", "path", "or", "if", "self", "parse", "url", "querystring", "is", "not", "none", "url", "str", "self", "parse", "url", "querystring", "return", "url", "str"], "docstring": "parse query string in url", "docstring_tokens": ["parse", "query", "string", "in", "url"], "idx": 164}
{"url": "https://github.com/knipknap/exscript/blob/72718eee3e87b345d5a5255be9824e867e42927b/Exscript/util/url.py#L72-L103", "repo": "exscript", "func_name": "_urlparse_qs", "original_string": ["def _urlparse_qs(url):\n", "    \"\"\"\n", "    Parse a URL query string and return the components as a dictionary.\n", "\n", "    Based on the cgi.parse_qs method.This is a utility function provided\n", "    with urlparse so that users need not use cgi module for\n", "    parsing the url query string.\n", "\n", "    Arguments:\n", "\n", "    :type url: str\n", "    :param url: URL with query string to be parsed\n", "    \"\"\"\n", "    # Extract the query part from the URL.\n", "    querystring = urlparse(url)[4]\n", "\n", "    # Split the query into name/value pairs.\n", "    pairs = [s2 for s1 in querystring.split('&') for s2 in s1.split(';')]\n", "\n", "    # Split the name/value pairs.\n", "    result = OrderedDefaultDict(list)\n", "    for name_value in pairs:\n", "        pair = name_value.split('=', 1)\n", "        if len(pair) != 2:\n", "            continue\n", "\n", "        if len(pair[1]) > 0:\n", "            name = _unquote(pair[0].replace('+', ' '))\n", "            value = _unquote(pair[1].replace('+', ' '))\n", "            result[name].append(value)\n", "\n", "    return result\n"], "language": "python", "code": "def _urlparse_qs(url):\n    \"\"\"\"\"\"\n    querystring = urlparse(url)[4]\n    pairs = [s2 for s1 in querystring.split('&') for s2 in s1.split(';')]\n    result = OrderedDefaultDict(list)\n    for name_value in pairs:\n        pair = name_value.split('=', 1)\n        if len(pair) != 2:\n            continue\n        if len(pair[1]) > 0:\n            name = _unquote(pair[0].replace('+', ' '))\n            value = _unquote(pair[1].replace('+', ' '))\n            result[name].append(value)\n    return result\n", "code_tokens": ["urlparse", "qs", "url", "querystring", "urlparse", "url", "4", "pairs", "for", "in", "querystring", "split", "for", "in", "split", "result", "ordereddefaultdict", "list", "for", "name", "value", "in", "pairs", "pair", "name", "value", "split", "1", "if", "len", "pair", "2", "continue", "if", "len", "pair", "1", "0", "name", "unquote", "pair", "0", "replace", "value", "unquote", "pair", "1", "replace", "result", "name", "append", "value", "return", "result"], "docstring": "parse query string in url", "docstring_tokens": ["parse", "query", "string", "in", "url"], "idx": 165}
{"url": "https://github.com/jdp/urp/blob/778c16d9a5eae75316ce20aad742af7122be558c/urp.py#L17-L22", "repo": "urp", "func_name": "parse", "original_string": ["def parse(args, data):\n", "    url = urlparse(data)\n", "    query = url.query\n", "    if not args.no_query_params:\n", "        query = parse_qsl(url.query)\n", "    return url, query\n"], "language": "python", "code": "def parse(args, data):\n    url = urlparse(data)\n    query = url.query\n    if not args.no_query_params:\n        query = parse_qsl(url.query)\n    return url, query\n", "code_tokens": ["parse", "args", "data", "url", "urlparse", "data", "query", "url", "query", "if", "not", "args", "no", "query", "params", "query", "parse", "qsl", "url", "query", "return", "url", "query"], "docstring": "parse query string in url", "docstring_tokens": ["parse", "query", "string", "in", "url"], "idx": 166}
{"url": "https://github.com/DenisCarriere/geocoder/blob/39b9999ec70e61da9fa52fe9fe82a261ad70fa8b/geocoder/opencage.py#L409-L415", "repo": "geocoder", "func_name": "_catch_errors", "original_string": ["    def _catch_errors(self, json_response):\n", "        status = json_response.get('status')\n", "        if status and status.get('code') != 200:\n", "            self.status_code = status.get('code')\n", "            self.error = status.get('message')\n", "\n", "        return self.error\n"], "language": "python", "code": "def _catch_errors(self, json_response):\n    status = json_response.get('status')\n    if status and status.get('code') != 200:\n        self.status_code = status.get('code')\n        self.error = status.get('message')\n    return self.error\n", "code_tokens": ["catch", "errors", "self", "json", "response", "status", "json", "response", "get", "status", "if", "status", "and", "status", "get", "code", "200", "self", "status", "code", "status", "get", "code", "self", "error", "status", "get", "message", "return", "self", "error"], "docstring": "get the description of a http status code", "docstring_tokens": ["get", "the", "description", "of", "a", "http", "status", "code"], "idx": 167}
{"url": "https://github.com/hardbyte/python-can/blob/cdc5254d96072df7739263623f3e920628a7d214/can/interfaces/socketcan/utils.py#L70-L90", "repo": "python-can", "func_name": "error_code_to_str", "original_string": ["    return interfaces\n", "\n", "\n", "def error_code_to_str(code: Optional[int]) -> str:\n", "    \"\"\"\n", "    Converts a given error code (errno) to a useful and human readable string.\n", "\n", "    :param code: a possibly invalid/unknown error code\n", "    :returns: a string explaining and containing the given error code, or a string\n", "              explaining that the errorcode is unknown if that is the case\n", "    \"\"\"\n", "    name = errno.errorcode.get(code, \"UNKNOWN\")  # type: ignore\n", "    description = os.strerror(code) if code is not None else \"NO DESCRIPTION AVAILABLE\"\n", "\n", "    return f\"{name} (errno {code}): {description}\"\n"], "language": "python", "code": "def error_code_to_str(code: Optional[int]) ->str:\n    \"\"\"\"\"\"\n    name = errno.errorcode.get(code, 'UNKNOWN')\n    description = os.strerror(code\n        ) if code is not None else 'NO DESCRIPTION AVAILABLE'\n    return f'{name} (errno {code}): {description}'\n", "code_tokens": ["error", "code", "to", "str", "code", "optional", "int", "str", "name", "errno", "errorcode", "get", "code", "unknown", "description", "os", "strerror", "code", "if", "code", "is", "not", "none", "else", "no", "description", "available", "return", "name", "errno", "code", "description"], "docstring": "get the description of a http status code", "docstring_tokens": ["get", "the", "description", "of", "a", "http", "status", "code"], "idx": 168}
{"url": "https://github.com/cloudendpoints/endpoints-python/blob/00dd7c7a52a9ee39d5923191c2604b8eafdb3f24/endpoints/errors.py#L239-L253", "repo": "endpoints-python", "func_name": "_get_status_code", "original_string": ["  def _get_status_code(self, http_status):\n", "    \"\"\"Get the HTTP status code from an HTTP status string.\n", "\n", "    Args:\n", "      http_status: A string containing a HTTP status code and reason.\n", "\n", "    Returns:\n", "      An integer with the status code number from http_status.\n", "    \"\"\"\n", "    try:\n", "      return int(http_status.split(' ', 1)[0])\n", "    except TypeError:\n", "      _logger.warning('Unable to find status code in HTTP status %r.',\n", "                      http_status)\n", "    return 500\n"], "language": "python", "code": "def _get_status_code(self, http_status):\n    \"\"\"\"\"\"\n    try:\n        return int(http_status.split(' ', 1)[0])\n    except TypeError:\n        _logger.warning('Unable to find status code in HTTP status %r.',\n            http_status)\n    return 500\n", "code_tokens": ["get", "status", "code", "self", "http", "status", "try", "return", "int", "http", "status", "split", "1", "0", "except", "typeerror", "logger", "warning", "unable", "to", "find", "status", "code", "in", "http", "status", "http", "status", "return", "500"], "docstring": "get the description of a http status code", "docstring_tokens": ["get", "the", "description", "of", "a", "http", "status", "code"], "idx": 169}
{"url": "https://github.com/vecnet/vecnet.simulation/blob/3a4b3df7b12418c6fa8a7d9cd49656a1c031fc0e/vecnet/simulation/sim_status.py#L47-L54", "repo": "vecnet.simulation", "func_name": "get_description", "original_string": ["def get_description(status_code):\n", "    \"\"\"\n", "    Get the description for a status code.\n", "    \"\"\"\n", "    description = _descriptions.get(status_code)\n", "    if description is None:\n", "        description = 'code = %s (no description)' % str(status_code)\n", "    return description"], "language": "python", "code": "def get_description(status_code):\n    \"\"\"\"\"\"\n    description = _descriptions.get(status_code)\n    if description is None:\n        description = 'code = %s (no description)' % str(status_code)\n    return description\n", "code_tokens": ["get", "description", "status", "code", "description", "descriptions", "get", "status", "code", "if", "description", "is", "none", "description", "code", "no", "description", "str", "status", "code", "return", "description"], "docstring": "get the description of a http status code", "docstring_tokens": ["get", "the", "description", "of", "a", "http", "status", "code"], "idx": 170}
{"url": "https://github.com/ewdurbin/community/blob/f6b80e215a88508e3b07c2f4996ede6edea2c8e3/community/app.py#L36-L42", "repo": "community", "func_name": "compute_status", "original_string": ["def compute_status():\n", "    status_codes = []\n", "    for name, health_url in DEPENDENCIES.items():\n", "        status_codes.append(get_status(health_url)[0])\n", "    if max(status_codes) == 500:\n", "        return ('UNHEALTHY', max(status_codes))\n", "    return ('HEALTHY', max(status_codes))\n"], "language": "python", "code": "def compute_status():\n    status_codes = []\n    for name, health_url in DEPENDENCIES.items():\n        status_codes.append(get_status(health_url)[0])\n    if max(status_codes) == 500:\n        return 'UNHEALTHY', max(status_codes)\n    return 'HEALTHY', max(status_codes)\n", "code_tokens": ["compute", "status", "status", "codes", "for", "name", "health", "url", "in", "dependencies", "items", "status", "codes", "append", "get", "status", "health", "url", "0", "if", "max", "status", "codes", "500", "return", "unhealthy", "max", "status", "codes", "return", "healthy", "max", "status", "codes"], "docstring": "get the description of a http status code", "docstring_tokens": ["get", "the", "description", "of", "a", "http", "status", "code"], "idx": 171}
{"url": "https://github.com/ronaldguillen/wave/blob/20bb979c917f7634d8257992e6d449dc751256a9/wave/renderers.py#L568-L571", "repo": "wave", "func_name": "get_description", "original_string": ["    def get_description(self, view, status_code):\n", "        if status_code in (status.HTTP_401_UNAUTHORIZED, status.HTTP_403_FORBIDDEN):\n", "            return ''\n", "        return view.get_view_description(html=True)\n"], "language": "python", "code": "def get_description(self, view, status_code):\n    if status_code in (status.HTTP_401_UNAUTHORIZED, status.HTTP_403_FORBIDDEN\n        ):\n        return ''\n    return view.get_view_description(html=True)\n", "code_tokens": ["get", "description", "self", "view", "status", "code", "if", "status", "code", "in", "status", "http", "401", "unauthorized", "status", "http", "403", "forbidden", "return", "return", "view", "get", "view", "description", "html", "true"], "docstring": "get the description of a http status code", "docstring_tokens": ["get", "the", "description", "of", "a", "http", "status", "code"], "idx": 172}
{"url": "https://github.com/mar10/wsgidav/blob/cec0d84222fc24bea01be1cea91729001963f172/wsgidav/dav_error.py#L264-L274", "repo": "wsgidav", "func_name": "get_http_status_string", "original_string": ["def get_http_status_string(v):\n", "    \"\"\"Return HTTP response string, e.g. 204 -> ('204 No Content').\n", "    The return string always includes descriptive text, to satisfy Apache mod_dav.\n", "\n", "    `v`: status code or DAVError\n", "    \"\"\"\n", "    code = get_http_status_code(v)\n", "    try:\n", "        return ERROR_DESCRIPTIONS[code]\n", "    except KeyError:\n", "        return \"{} Status\".format(code)\n"], "language": "python", "code": "def get_http_status_string(v):\n    \"\"\"\"\"\"\n    code = get_http_status_code(v)\n    try:\n        return ERROR_DESCRIPTIONS[code]\n    except KeyError:\n        return '{} Status'.format(code)\n", "code_tokens": ["get", "http", "status", "string", "code", "get", "http", "status", "code", "try", "return", "error", "descriptions", "code", "except", "keyerror", "return", "status", "format", "code"], "docstring": "get the description of a http status code", "docstring_tokens": ["get", "the", "description", "of", "a", "http", "status", "code"], "idx": 173}
{"url": "https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/qc/one.py#L133-L144", "repo": "chemlab", "func_name": "binomial_prefactor", "original_string": ["def binomial_prefactor(s,ia,ib,xpa,xpb):\n", "    \"\"\"\n", "    The integral prefactor containing the binomial coefficients from Augspurger and Dykstra.\n", "    >>> binomial_prefactor(0,0,0,0,0)\n", "    1\n", "    \"\"\"\n", "    total= 0\n", "    for t in range(s+1):\n", "        if s-ia <= t <= ib:\n", "            total +=  binomial(ia,s-t)*binomial(ib,t)* \\\n", "                     pow(xpa,ia-s+t)*pow(xpb,ib-t)\n", "    return total\n"], "language": "python", "code": "def binomial_prefactor(s, ia, ib, xpa, xpb):\n    \"\"\"\"\"\"\n    total = 0\n    for t in range(s + 1):\n        if s - ia <= t <= ib:\n            total += binomial(ia, s - t) * binomial(ib, t) * pow(xpa, ia -\n                s + t) * pow(xpb, ib - t)\n    return total\n", "code_tokens": ["binomial", "prefactor", "ia", "ib", "xpa", "xpb", "total", "0", "for", "in", "range", "1", "if", "ia", "ib", "total", "binomial", "ia", "binomial", "ib", "pow", "xpa", "ia", "pow", "xpb", "ib", "return", "total"], "docstring": "binomial distribution", "docstring_tokens": ["binomial", "distribution"], "idx": 174}
{"url": "https://github.com/lpantano/seqcluster/blob/774e23add8cd4fdc83d626cea3bd1f458e7d060d/seqcluster/libs/thinkbayes.py#L1530-L1535", "repo": "seqcluster", "func_name": "EvalBinomialPmf", "original_string": ["def EvalBinomialPmf(k, n, p):\n", "    \"\"\"Evaluates the binomial pmf.\n", "\n", "    Returns the probabily of k successes in n trials with probability p.\n", "    \"\"\"\n", "    return scipy.stats.binom.pmf(k, n, p)\n"], "language": "python", "code": "def EvalBinomialPmf(k, n, p):\n    \"\"\"\"\"\"\n    return scipy.stats.binom.pmf(k, n, p)\n", "code_tokens": ["evalbinomialpmf", "return", "scipy", "stats", "binom", "pmf"], "docstring": "binomial distribution", "docstring_tokens": ["binomial", "distribution"], "idx": 175}
{"url": "https://github.com/TeamHG-Memex/MaybeDont/blob/34721f67b69d426adda324a0ed905d3860828af9/maybedont/predict.py#L275-L283", "repo": "MaybeDont", "func_name": "get_prob", "original_string": ["    def get_prob(self):\n", "        if self.total < 5:\n", "            return 0.\n", "        a, b = self.dup + 1, self.nodup + 1\n", "        n = a + b\n", "        p = a / n\n", "        q = b / n\n", "        # Lower edge of the 95% confidence interval, binomial distribution\n", "        return p - 1.96 * math.sqrt(p * q / n)\n"], "language": "python", "code": "def get_prob(self):\n    if self.total < 5:\n        return 0.0\n    a, b = self.dup + 1, self.nodup + 1\n    n = a + b\n    p = a / n\n    q = b / n\n    return p - 1.96 * math.sqrt(p * q / n)\n", "code_tokens": ["get", "prob", "self", "if", "self", "total", "5", "return", "0", "0", "self", "dup", "1", "self", "nodup", "1", "return", "1", "96", "math", "sqrt"], "docstring": "binomial distribution", "docstring_tokens": ["binomial", "distribution"], "idx": 176}
{"url": "https://github.com/QInfer/python-qinfer/blob/8170c84a0be1723f8c6b09e0d3c7a40a886f1fe3/src/qinfer/distributions.py#L1078-L1082", "repo": "python-qinfer", "func_name": "sample", "original_string": ["    def sample(self, n=1):\n", "        p_vals = self._p_dist.rvs(size=n)[:, np.newaxis]\n", "        # numpy.random.binomial supports sampling using different p values,\n", "        # whereas scipy does not.\n", "        return np.random.binomial(self.n, p_vals)\n"], "language": "python", "code": "def sample(self, n=1):\n    p_vals = self._p_dist.rvs(size=n)[:, np.newaxis]\n    return np.random.binomial(self.n, p_vals)\n", "code_tokens": ["sample", "self", "1", "vals", "self", "dist", "rvs", "size", "np", "newaxis", "return", "np", "random", "binomial", "self", "vals"], "docstring": "binomial distribution", "docstring_tokens": ["binomial", "distribution"], "idx": 177}
{"url": "https://github.com/davidfokkema/artist/blob/26ae7987522622710f2910980770c50012fda47d/demo/demo_histogram_fit.py#L8-L45", "repo": "artist", "func_name": "main", "original_string": ["def main():\n", "    # Draw random numbers from the normal distribution\n", "    np.random.seed(1)\n", "    N = np.random.normal(size=2000)\n", "\n", "    # define bin edges\n", "    edge = 5\n", "    bin_width = .1\n", "    bins = np.arange(-edge, edge + .5 * bin_width, bin_width)\n", "\n", "    # build histogram and x, y values at the center of the bins\n", "    n, bins = np.histogram(N, bins=bins)\n", "    x = (bins[:-1] + bins[1:]) / 2\n", "    y = n\n", "\n", "    # fit normal distribution pdf to data\n", "    f = lambda x, N, mu, sigma: N * scipy.stats.norm.pdf(x, mu, sigma)\n", "    popt, pcov = scipy.optimize.curve_fit(f, x, y)\n", "    print(\"Parameters from fit (N, mu, sigma):\", popt)\n", "\n", "    # make graph\n", "    graph = Plot()\n", "\n", "    # graph histogram\n", "    graph.histogram(n, bins)\n", "\n", "    # graph model with fit parameters\n", "    x = np.linspace(-edge, edge, 100)\n", "    graph.plot(x, f(x, *popt), mark=None)\n", "\n", "    # set labels and limits\n", "    graph.set_xlabel(\"value\")\n", "    graph.set_ylabel(\"count\")\n", "    graph.set_label(\"Fit to data\")\n", "    graph.set_xlimits(-6, 6)\n", "\n", "    # save graph to file\n", "    graph.save('histogram-fit')\n"], "language": "python", "code": "def main():\n    np.random.seed(1)\n    N = np.random.normal(size=2000)\n    edge = 5\n    bin_width = 0.1\n    bins = np.arange(-edge, edge + 0.5 * bin_width, bin_width)\n    n, bins = np.histogram(N, bins=bins)\n    x = (bins[:-1] + bins[1:]) / 2\n    y = n\n    f = lambda x, N, mu, sigma: N * scipy.stats.norm.pdf(x, mu, sigma)\n    popt, pcov = scipy.optimize.curve_fit(f, x, y)\n    print('Parameters from fit (N, mu, sigma):', popt)\n    graph = Plot()\n    graph.histogram(n, bins)\n    x = np.linspace(-edge, edge, 100)\n    graph.plot(x, f(x, *popt), mark=None)\n    graph.set_xlabel('value')\n    graph.set_ylabel('count')\n    graph.set_label('Fit to data')\n    graph.set_xlimits(-6, 6)\n    graph.save('histogram-fit')\n", "code_tokens": ["main", "np", "random", "seed", "1", "np", "random", "normal", "size", "2000", "edge", "5", "bin", "width", "0", "1", "bins", "np", "arange", "edge", "edge", "0", "5", "bin", "width", "bin", "width", "bins", "np", "histogram", "bins", "bins", "bins", "1", "bins", "1", "2", "lambda", "mu", "sigma", "scipy", "stats", "norm", "pdf", "mu", "sigma", "popt", "pcov", "scipy", "optimize", "curve", "fit", "print", "parameters", "from", "fit", "mu", "sigma", "popt", "graph", "plot", "graph", "histogram", "bins", "np", "linspace", "edge", "edge", "100", "graph", "plot", "popt", "mark", "none", "graph", "set", "xlabel", "value", "graph", "set", "ylabel", "count", "graph", "set", "label", "fit", "to", "data", "graph", "set", "xlimits", "6", "6", "graph", "save", "histogram", "fit"], "docstring": "binomial distribution", "docstring_tokens": ["binomial", "distribution"], "idx": 178}
{"url": "https://github.com/tisimst/mcerp/blob/2bb8260c9ad2d58a806847f1b627b6451e407de1/mcerp/__init__.py#L1150-L1167", "repo": "mcerp", "func_name": "Binomial", "original_string": ["def Binomial(n, p, tag=None):\n", "    \"\"\"\n", "    A Binomial random variate\n", "    \n", "    Parameters\n", "    ----------\n", "    n : int\n", "        The number of trials\n", "    p : scalar\n", "        The probability of success\n", "    \"\"\"\n", "    assert (\n", "        int(n) == n and n > 0\n", "    ), 'Binomial number of trials \"n\" must be an integer greater than zero'\n", "    assert (\n", "        0 < p < 1\n", "    ), 'Binomial probability \"p\" must be between zero and one, non-inclusive'\n", "    return uv(ss.binom(n, p), tag=tag)\n"], "language": "python", "code": "def Binomial(n, p, tag=None):\n    \"\"\"\"\"\"\n    assert int(n\n        ) == n and n > 0, 'Binomial number of trials \"n\" must be an integer greater than zero'\n    assert 0 < p < 1, 'Binomial probability \"p\" must be between zero and one, non-inclusive'\n    return uv(ss.binom(n, p), tag=tag)\n", "code_tokens": ["binomial", "tag", "none", "assert", "int", "and", "0", "binomial", "number", "of", "trials", "must", "be", "an", "integer", "greater", "than", "zero", "assert", "0", "1", "binomial", "probability", "must", "be", "between", "zero", "and", "one", "non", "inclusive", "return", "uv", "ss", "binom", "tag", "tag"], "docstring": "binomial distribution", "docstring_tokens": ["binomial", "distribution"], "idx": 179}
{"url": "https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/qc/utils.py#L30-L40", "repo": "chemlab", "func_name": "binomial", "original_string": ["def binomial(n,k):\n", "    \"\"\"\n", "    Binomial coefficient\n", "    >>> binomial(5,2)\n", "    10\n", "    >>> binomial(10,5)\n", "    252\n", "    \"\"\"\n", "    if n==k: return 1\n", "    assert n>k, \"Attempting to call binomial(%d,%d)\" % (n,k)\n", "    return factorial(n)//(factorial(k)*factorial(n-k))\n"], "language": "python", "code": "def binomial(n, k):\n    \"\"\"\"\"\"\n    if n == k:\n        return 1\n    assert n > k, 'Attempting to call binomial(%d,%d)' % (n, k)\n    return factorial(n) // (factorial(k) * factorial(n - k))\n", "code_tokens": ["binomial", "if", "return", "1", "assert", "attempting", "to", "call", "binomial", "return", "factorial", "factorial", "factorial"], "docstring": "binomial distribution", "docstring_tokens": ["binomial", "distribution"], "idx": 180}
{"url": "https://github.com/jopohl/urh/blob/2eb33b125c8407964cd1092843cde5010eb88aae/src/urh/controller/widgets/SignalFrame.py#L31-L33", "repo": "urh", "func_name": "perform_filter", "original_string": ["\n", "def perform_filter(result_array: Array, data, f_low, f_high, filter_bw):\n", "    result_array = np.frombuffer(result_array.get_obj(), dtype=np.complex64)\n"], "language": "python", "code": "def perform_filter(result_array: Array, data, f_low, f_high, filter_bw):\n    result_array = np.frombuffer(result_array.get_obj(), dtype=np.complex64)\n", "code_tokens": ["perform", "filter", "result", "array", "array", "data", "low", "high", "filter", "bw", "result", "array", "np", "frombuffer", "result", "array", "get", "obj", "dtype", "np"], "docstring": "filter array", "docstring_tokens": ["filter", "array"], "idx": 181}
{"url": "https://github.com/Yipit/pyeqs/blob/2e385c0a5d113af0e20be4d9393add2aabdd9565/pyeqs/query_builder.py#L41-L50", "repo": "pyeqs", "func_name": "_build_filtered_query", "original_string": ["    def _build_filtered_query(self, f, operator):\n", "        \"\"\"\n", "        Create the root of the filter tree\n", "        \"\"\"\n", "        self._filtered = True\n", "        if isinstance(f, Filter):\n", "            filter_object = f\n", "        else:\n", "            filter_object = Filter(operator).filter(f)\n", "        self._filter_dsl = filter_object\n"], "language": "python", "code": "def _build_filtered_query(self, f, operator):\n    \"\"\"\"\"\"\n    self._filtered = True\n    if isinstance(f, Filter):\n        filter_object = f\n    else:\n        filter_object = Filter(operator).filter(f)\n    self._filter_dsl = filter_object\n", "code_tokens": ["build", "filtered", "query", "self", "operator", "self", "filtered", "true", "if", "isinstance", "filter", "filter", "object", "else", "filter", "object", "filter", "operator", "filter", "self", "filter", "dsl", "filter", "object"], "docstring": "filter array", "docstring_tokens": ["filter", "array"], "idx": 182}
{"url": "https://github.com/odlgroup/odl/blob/b8443f6aca90e191ba36c91d32253c5a36249a6c/odl/tomo/analytic/filtered_back_projection.py#L313-L474", "repo": "odl", "func_name": "fbp_filter_op", "original_string": ["def fbp_filter_op(ray_trafo, padding=True, filter_type='Ram-Lak',\n", "                  frequency_scaling=1.0):\n", "    \"\"\"Create a filter operator for FBP from a `RayTransform`.\n", "\n", "    Parameters\n", "    ----------\n", "    ray_trafo : `RayTransform`\n", "        The ray transform (forward operator) whose approximate inverse should\n", "        be computed. Its geometry has to be any of the following\n", "\n", "        `Parallel2dGeometry` : Exact reconstruction\n", "\n", "        `Parallel3dAxisGeometry` : Exact reconstruction\n", "\n", "        `FanBeamGeometry` : Approximate reconstruction, correct in limit of\n", "        fan angle = 0.\n", "        Only flat detectors are supported (det_curvature_radius is None).\n", "\n", "        `ConeBeamGeometry`, pitch = 0 (circular) : Approximate reconstruction,\n", "        correct in the limit of fan angle = 0 and cone angle = 0.\n", "\n", "        `ConeBeamGeometry`, pitch > 0 (helical) : Very approximate unless a\n", "        `tam_danielson_window` is used. Accurate with the window.\n", "\n", "        Other geometries: Not supported\n", "\n", "    padding : bool, optional\n", "        If the data space should be zero padded. Without padding, the data may\n", "        be corrupted due to the circular convolution used. Using padding makes\n", "        the algorithm slower.\n", "    filter_type : optional\n", "        The type of filter to be used.\n", "        The predefined options are, in approximate order from most noise\n", "        senstive to least noise sensitive:\n", "        ``'Ram-Lak'``, ``'Shepp-Logan'``, ``'Cosine'``, ``'Hamming'`` and\n", "        ``'Hann'``.\n", "        A callable can also be provided. It must take an array of values in\n", "        [0, 1] and return the filter for these frequencies.\n", "    frequency_scaling : float, optional\n", "        Relative cutoff frequency for the filter.\n", "        The normalized frequencies are rescaled so that they fit into the range\n", "        [0, frequency_scaling]. Any frequency above ``frequency_scaling`` is\n", "        set to zero.\n", "\n", "    Returns\n", "    -------\n", "    filter_op : `Operator`\n", "        Filtering operator for FBP based on ``ray_trafo``.\n", "\n", "    See Also\n", "    --------\n", "    tam_danielson_window : Windowing for helical data\n", "    \"\"\"\n", "    impl = 'pyfftw' if PYFFTW_AVAILABLE else 'numpy'\n", "    alen = ray_trafo.geometry.motion_params.length\n", "\n", "    if ray_trafo.domain.ndim == 2:\n", "        # Define ramp filter\n", "        def fourier_filter(x):\n", "            abs_freq = np.abs(x[1])\n", "            norm_freq = abs_freq / np.max(abs_freq)\n", "            filt = _fbp_filter(norm_freq, filter_type, frequency_scaling)\n", "            scaling = 1 / (2 * alen)\n", "            return filt * np.max(abs_freq) * scaling\n", "\n", "        # Define (padded) fourier transform\n", "        if padding:\n", "            # Define padding operator\n", "            ran_shp = (ray_trafo.range.shape[0],\n", "                       ray_trafo.range.shape[1] * 2 - 1)\n", "            resizing = ResizingOperator(ray_trafo.range, ran_shp=ran_shp)\n", "\n", "            fourier = FourierTransform(resizing.range, axes=1, impl=impl)\n", "            fourier = fourier * resizing\n", "        else:\n", "            fourier = FourierTransform(ray_trafo.range, axes=1, impl=impl)\n", "\n", "    elif ray_trafo.domain.ndim == 3:\n", "        # Find the direction that the filter should be taken in\n", "        rot_dir = _rotation_direction_in_detector(ray_trafo.geometry)\n", "\n", "        # Find what axes should be used in the fourier transform\n", "        used_axes = (rot_dir != 0)\n", "        if used_axes[0] and not used_axes[1]:\n", "            axes = [1]\n", "        elif not used_axes[0] and used_axes[1]:\n", "            axes = [2]\n", "        else:\n", "            axes = [1, 2]\n", "\n", "        # Add scaling for cone-beam case\n", "        if hasattr(ray_trafo.geometry, 'src_radius'):\n", "            scale = (ray_trafo.geometry.src_radius\n", "                     / (ray_trafo.geometry.src_radius\n", "                        + ray_trafo.geometry.det_radius))\n", "\n", "            if ray_trafo.geometry.pitch != 0:\n", "                # In helical geometry the whole volume is not in each\n", "                # projection and we need to use another weighting.\n", "                # Ideally each point in the volume effects only\n", "                # the projections in a half rotation, so we assume that that\n", "                # is the case.\n", "                scale *= alen / (np.pi)\n", "        else:\n", "            scale = 1.0\n", "\n", "        # Define ramp filter\n", "        def fourier_filter(x):\n", "            # If axis is aligned to a coordinate axis, save some memory and\n", "            # time by using broadcasting\n", "            if not used_axes[0]:\n", "                abs_freq = np.abs(rot_dir[1] * x[2])\n", "            elif not used_axes[1]:\n", "                abs_freq = np.abs(rot_dir[0] * x[1])\n", "            else:\n", "                abs_freq = np.abs(rot_dir[0] * x[1] + rot_dir[1] * x[2])\n", "            norm_freq = abs_freq / np.max(abs_freq)\n", "            filt = _fbp_filter(norm_freq, filter_type, frequency_scaling)\n", "            scaling = scale * np.max(abs_freq) / (2 * alen)\n", "            return filt * scaling\n", "\n", "        # Define (padded) fourier transform\n", "        if padding:\n", "            # Define padding operator\n", "            if used_axes[0]:\n", "                padded_shape_u = ray_trafo.range.shape[1] * 2 - 1\n", "            else:\n", "                padded_shape_u = ray_trafo.range.shape[1]\n", "\n", "            if used_axes[1]:\n", "                padded_shape_v = ray_trafo.range.shape[2] * 2 - 1\n", "            else:\n", "                padded_shape_v = ray_trafo.range.shape[2]\n", "\n", "            ran_shp = (ray_trafo.range.shape[0],\n", "                       padded_shape_u,\n", "                       padded_shape_v)\n", "            resizing = ResizingOperator(ray_trafo.range, ran_shp=ran_shp)\n", "\n", "            fourier = FourierTransform(resizing.range, axes=axes, impl=impl)\n", "            fourier = fourier * resizing\n", "        else:\n", "            fourier = FourierTransform(ray_trafo.range, axes=axes, impl=impl)\n", "    else:\n", "        raise NotImplementedError('FBP only implemented in 2d and 3d')\n", "\n", "    # Create ramp in the detector direction\n", "    ramp_function = fourier.range.element(fourier_filter)\n", "\n", "    weight = 1\n", "    if not ray_trafo.range.is_weighted:\n", "        # Compensate for potentially unweighted range of the ray transform\n", "        weight *= ray_trafo.range.cell_volume\n", "\n", "    if not ray_trafo.domain.is_weighted:\n", "        # Compensate for potentially unweighted domain of the ray transform\n", "        weight /= ray_trafo.domain.cell_volume\n", "\n", "    ramp_function *= weight\n", "\n", "    # Create ramp filter via the convolution formula with fourier transforms\n", "    return fourier.inverse * ramp_function * fourier\n"], "language": "python", "code": "def fbp_filter_op(ray_trafo, padding=True, filter_type='Ram-Lak',\n    frequency_scaling=1.0):\n    \"\"\"\"\"\"\n    impl = 'pyfftw' if PYFFTW_AVAILABLE else 'numpy'\n    alen = ray_trafo.geometry.motion_params.length\n    if ray_trafo.domain.ndim == 2:\n\n        def fourier_filter(x):\n            abs_freq = np.abs(x[1])\n            norm_freq = abs_freq / np.max(abs_freq)\n            filt = _fbp_filter(norm_freq, filter_type, frequency_scaling)\n            scaling = 1 / (2 * alen)\n            return filt * np.max(abs_freq) * scaling\n        if padding:\n            ran_shp = ray_trafo.range.shape[0], ray_trafo.range.shape[1\n                ] * 2 - 1\n            resizing = ResizingOperator(ray_trafo.range, ran_shp=ran_shp)\n            fourier = FourierTransform(resizing.range, axes=1, impl=impl)\n            fourier = fourier * resizing\n        else:\n            fourier = FourierTransform(ray_trafo.range, axes=1, impl=impl)\n    elif ray_trafo.domain.ndim == 3:\n        rot_dir = _rotation_direction_in_detector(ray_trafo.geometry)\n        used_axes = rot_dir != 0\n        if used_axes[0] and not used_axes[1]:\n            axes = [1]\n        elif not used_axes[0] and used_axes[1]:\n            axes = [2]\n        else:\n            axes = [1, 2]\n        if hasattr(ray_trafo.geometry, 'src_radius'):\n            scale = ray_trafo.geometry.src_radius / (ray_trafo.geometry.\n                src_radius + ray_trafo.geometry.det_radius)\n            if ray_trafo.geometry.pitch != 0:\n                scale *= alen / np.pi\n        else:\n            scale = 1.0\n\n        def fourier_filter(x):\n            if not used_axes[0]:\n                abs_freq = np.abs(rot_dir[1] * x[2])\n            elif not used_axes[1]:\n                abs_freq = np.abs(rot_dir[0] * x[1])\n            else:\n                abs_freq = np.abs(rot_dir[0] * x[1] + rot_dir[1] * x[2])\n            norm_freq = abs_freq / np.max(abs_freq)\n            filt = _fbp_filter(norm_freq, filter_type, frequency_scaling)\n            scaling = scale * np.max(abs_freq) / (2 * alen)\n            return filt * scaling\n        if padding:\n            if used_axes[0]:\n                padded_shape_u = ray_trafo.range.shape[1] * 2 - 1\n            else:\n                padded_shape_u = ray_trafo.range.shape[1]\n            if used_axes[1]:\n                padded_shape_v = ray_trafo.range.shape[2] * 2 - 1\n            else:\n                padded_shape_v = ray_trafo.range.shape[2]\n            ran_shp = ray_trafo.range.shape[0], padded_shape_u, padded_shape_v\n            resizing = ResizingOperator(ray_trafo.range, ran_shp=ran_shp)\n            fourier = FourierTransform(resizing.range, axes=axes, impl=impl)\n            fourier = fourier * resizing\n        else:\n            fourier = FourierTransform(ray_trafo.range, axes=axes, impl=impl)\n    else:\n        raise NotImplementedError('FBP only implemented in 2d and 3d')\n    ramp_function = fourier.range.element(fourier_filter)\n    weight = 1\n    if not ray_trafo.range.is_weighted:\n        weight *= ray_trafo.range.cell_volume\n    if not ray_trafo.domain.is_weighted:\n        weight /= ray_trafo.domain.cell_volume\n    ramp_function *= weight\n    return fourier.inverse * ramp_function * fourier\n", "code_tokens": ["fbp", "filter", "op", "ray", "trafo", "padding", "true", "filter", "type", "ram", "lak", "frequency", "scaling", "1", "0", "impl", "pyfftw", "if", "pyfftw", "available", "else", "numpy", "alen", "ray", "trafo", "geometry", "motion", "params", "length", "if", "ray", "trafo", "domain", "ndim", "2", "def", "fourier", "filter", "abs", "freq", "np", "abs", "1", "norm", "freq", "abs", "freq", "np", "max", "abs", "freq", "filt", "fbp", "filter", "norm", "freq", "filter", "type", "frequency", "scaling", "scaling", "1", "2", "alen", "return", "filt", "np", "max", "abs", "freq", "scaling", "if", "padding", "ran", "shp", "ray", "trafo", "range", "shape", "0", "ray", "trafo", "range", "shape", "1", "2", "1", "resizing", "resizingoperator", "ray", "trafo", "range", "ran", "shp", "ran", "shp", "fourier", "fouriertransform", "resizing", "range", "axes", "1", "impl", "impl", "fourier", "fourier", "resizing", "else", "fourier", "fouriertransform", "ray", "trafo", "range", "axes", "1", "impl", "impl", "elif", "ray", "trafo", "domain", "ndim", "3", "rot", "dir", "rotation", "direction", "in", "detector", "ray", "trafo", "geometry", "used", "axes", "rot", "dir", "0", "if", "used", "axes", "0", "and", "not", "used", "axes", "1", "axes", "1", "elif", "not", "used", "axes", "0", "and", "used", "axes", "1", "axes", "2", "else", "axes", "1", "2", "if", "hasattr", "ray", "trafo", "geometry", "src", "radius", "scale", "ray", "trafo", "geometry", "src", "radius", "ray", "trafo", "geometry", "src", "radius", "ray", "trafo", "geometry", "det", "radius", "if", "ray", "trafo", "geometry", "pitch", "0", "scale", "alen", "np", "pi", "else", "scale", "1", "0", "def", "fourier", "filter", "if", "not", "used", "axes", "0", "abs", "freq", "np", "abs", "rot", "dir", "1", "2", "elif", "not", "used", "axes", "1", "abs", "freq", "np", "abs", "rot", "dir", "0", "1", "else", "abs", "freq", "np", "abs", "rot", "dir", "0", "1", "rot", "dir", "1", "2", "norm", "freq", "abs", "freq", "np", "max", "abs", "freq", "filt", "fbp", "filter", "norm", "freq", "filter", "type", "frequency", "scaling", "scaling", "scale", "np", "max", "abs", "freq", "2", "alen", "return", "filt", "scaling", "if", "padding", "if", "used", "axes", "0", "padded", "shape", "ray", "trafo", "range", "shape", "1", "2", "1", "else", "padded", "shape", "ray", "trafo", "range", "shape", "1", "if", "used", "axes", "1", "padded", "shape", "ray", "trafo", "range", "shape", "2", "2", "1", "else", "padded", "shape", "ray", "trafo", "range", "shape", "2", "ran", "shp", "ray", "trafo", "range", "shape", "0", "padded", "shape", "padded", "shape", "resizing", "resizingoperator", "ray", "trafo", "range", "ran", "shp", "ran", "shp", "fourier", "fouriertransform", "resizing", "range", "axes", "axes", "impl", "impl", "fourier", "fourier", "resizing", "else", "fourier", "fouriertransform", "ray", "trafo", "range", "axes", "axes", "impl", "impl", "else", "raise", "notimplementederror", "fbp", "only", "implemented", "in", "and", "ramp", "function", "fourier", "range", "element", "fourier", "filter", "weight", "1", "if", "not", "ray", "trafo", "range", "is", "weighted", "weight", "ray", "trafo", "range", "cell", "volume", "if", "not", "ray", "trafo", "domain", "is", "weighted", "weight", "ray", "trafo", "domain", "cell", "volume", "ramp", "function", "weight", "return", "fourier", "inverse", "ramp", "function", "fourier"], "docstring": "filter array", "docstring_tokens": ["filter", "array"], "idx": 183}
{"url": "https://github.com/PiotrDabkowski/Js2Py/blob/c0fa43f5679cf91ca8986c5747fcb07a433dc584/js2py/internals/prototypes/jsarray.py#L382-L398", "repo": "Js2Py", "func_name": "filter", "original_string": ["    def filter(this, args):\n", "        array = to_object(this, args.space)\n", "        callbackfn = get_arg(args, 0)\n", "        arr_len = js_arr_length(array)\n", "        if not is_callable(callbackfn):\n", "            raise MakeError('TypeError', 'callbackfn must be a function')\n", "        _this = get_arg(args, 1)\n", "        k = 0\n", "        res = []\n", "        while k < arr_len:\n", "            if array.has_property(unicode(k)):\n", "                kValue = array.get(unicode(k))\n", "                if to_boolean(\n", "                        callbackfn.call(_this, (kValue, float(k), array))):\n", "                    res.append(kValue)\n", "            k += 1\n", "        return args.space.ConstructArray(res)\n"], "language": "python", "code": "def filter(this, args):\n    array = to_object(this, args.space)\n    callbackfn = get_arg(args, 0)\n    arr_len = js_arr_length(array)\n    if not is_callable(callbackfn):\n        raise MakeError('TypeError', 'callbackfn must be a function')\n    _this = get_arg(args, 1)\n    k = 0\n    res = []\n    while k < arr_len:\n        if array.has_property(unicode(k)):\n            kValue = array.get(unicode(k))\n            if to_boolean(callbackfn.call(_this, (kValue, float(k), array))):\n                res.append(kValue)\n        k += 1\n    return args.space.ConstructArray(res)\n", "code_tokens": ["filter", "this", "args", "array", "to", "object", "this", "args", "space", "callbackfn", "get", "arg", "args", "0", "arr", "len", "js", "arr", "length", "array", "if", "not", "is", "callable", "callbackfn", "raise", "makeerror", "typeerror", "callbackfn", "must", "be", "function", "this", "get", "arg", "args", "1", "0", "res", "while", "arr", "len", "if", "array", "has", "property", "unicode", "kvalue", "array", "get", "unicode", "if", "to", "boolean", "callbackfn", "call", "this", "kvalue", "float", "array", "res", "append", "kvalue", "1", "return", "args", "space", "constructarray", "res"], "docstring": "filter array", "docstring_tokens": ["filter", "array"], "idx": 184}
{"url": "https://github.com/jim-easterbrook/pyctools/blob/2a958665326892f45f249bebe62c2c23f306732b/src/pyctools/components/interp/resize.py#L77-L95", "repo": "pyctools", "func_name": "get_filter", "original_string": ["\n", "    def get_filter(self):\n", "        new_filter = self.input_buffer['filter'].peek()\n", "        if not new_filter:\n", "            return False\n", "        if new_filter == self.filter_frame:\n", "            return True\n", "        self.send('filter', new_filter)\n", "        filter_coefs = new_filter.as_numpy(dtype=numpy.float32)\n", "        if filter_coefs.ndim != 3:\n", "            self.logger.warning('Filter input must be 3 dimensional')\n", "            return False\n", "        ylen, xlen = filter_coefs.shape[:2]\n", "        if (xlen % 2) != 1 or (ylen % 2) != 1:\n", "            self.logger.warning('Filter input must have odd width & height')\n", "            return False\n", "        self.filter_frame = new_filter\n", "        self.filter_coefs = filter_coefs\n", "        self.fil_count = None\n"], "language": "python", "code": "def get_filter(self):\n    new_filter = self.input_buffer['filter'].peek()\n    if not new_filter:\n        return False\n    if new_filter == self.filter_frame:\n        return True\n    self.send('filter', new_filter)\n    filter_coefs = new_filter.as_numpy(dtype=numpy.float32)\n    if filter_coefs.ndim != 3:\n        self.logger.warning('Filter input must be 3 dimensional')\n        return False\n    ylen, xlen = filter_coefs.shape[:2]\n    if xlen % 2 != 1 or ylen % 2 != 1:\n        self.logger.warning('Filter input must have odd width & height')\n        return False\n    self.filter_frame = new_filter\n    self.filter_coefs = filter_coefs\n    self.fil_count = None\n", "code_tokens": ["get", "filter", "self", "new", "filter", "self", "input", "buffer", "filter", "peek", "if", "not", "new", "filter", "return", "false", "if", "new", "filter", "self", "filter", "frame", "return", "true", "self", "send", "filter", "new", "filter", "filter", "coefs", "new", "filter", "as", "numpy", "dtype", "numpy", "if", "filter", "coefs", "ndim", "3", "self", "logger", "warning", "filter", "input", "must", "be", "3", "dimensional", "return", "false", "ylen", "xlen", "filter", "coefs", "shape", "2", "if", "xlen", "2", "1", "or", "ylen", "2", "1", "self", "logger", "warning", "filter", "input", "must", "have", "odd", "width", "height", "return", "false", "self", "filter", "frame", "new", "filter", "self", "filter", "coefs", "filter", "coefs", "self", "fil", "count", "none"], "docstring": "filter array", "docstring_tokens": ["filter", "array"], "idx": 185}
{"url": "https://github.com/brechtm/rinohtype/blob/40a63c4e5ad7550f62b6860f1812cb67cafb9dc7/src/rinoh/backend/pdf/filter.py#L459-L463", "repo": "rinohtype", "func_name": "params", "original_string": ["    def params(self):\n", "        if not any(filter.params for filter in self):\n", "            return None\n", "        else:\n", "            return Array(filter.params or Null() for filter in self)\n"], "language": "python", "code": "def params(self):\n    if not any(filter.params for filter in self):\n        return None\n    else:\n        return Array(filter.params or Null() for filter in self)\n", "code_tokens": ["params", "self", "if", "not", "any", "filter", "params", "for", "filter", "in", "self", "return", "none", "else", "return", "array", "filter", "params", "or", "null", "for", "filter", "in", "self"], "docstring": "filter array", "docstring_tokens": ["filter", "array"], "idx": 186}
{"url": "https://github.com/mosesschwartz/scrypture/blob/d51eb0c9835a5122a655078268185ce8ab9ec86a/scrypture/demo_scripts/Utils/json_to_csv.py#L28-L47", "repo": "scrypture", "func_name": "json_to_csv", "original_string": ["def json_to_csv(json_input):\n", "    '''\n", "    Convert simple JSON to CSV\n", "    Accepts a JSON string or JSON object\n", "    '''\n", "    try:\n", "        json_input = json.loads(json_input)\n", "    except:\n", "        pass # If loads fails, it's probably already parsed\n", "    headers = set()\n", "    for json_row in json_input:\n", "        headers.update(json_row.keys())\n", "\n", "    csv_io = StringIO.StringIO()\n", "    csv_out = csv.DictWriter(csv_io,headers)\n", "    csv_out.writeheader()\n", "    for json_row in json_input:\n", "        csv_out.writerow(json_row)\n", "    csv_io.seek(0)\n", "    return csv_io.read()\n"], "language": "python", "code": "def json_to_csv(json_input):\n    \"\"\"\"\"\"\n    try:\n        json_input = json.loads(json_input)\n    except:\n        pass\n    headers = set()\n    for json_row in json_input:\n        headers.update(json_row.keys())\n    csv_io = StringIO.StringIO()\n    csv_out = csv.DictWriter(csv_io, headers)\n    csv_out.writeheader()\n    for json_row in json_input:\n        csv_out.writerow(json_row)\n    csv_io.seek(0)\n    return csv_io.read()\n", "code_tokens": ["json", "to", "csv", "json", "input", "try", "json", "input", "json", "loads", "json", "input", "except", "pass", "headers", "set", "for", "json", "row", "in", "json", "input", "headers", "update", "json", "row", "keys", "csv", "io", "stringio", "stringio", "csv", "out", "csv", "dictwriter", "csv", "io", "headers", "csv", "out", "writeheader", "for", "json", "row", "in", "json", "input", "csv", "out", "writerow", "json", "row", "csv", "io", "seek", "0", "return", "csv", "io", "read"], "docstring": "convert json to csv", "docstring_tokens": ["convert", "json", "to", "csv"], "idx": 187}
{"url": "https://github.com/linkedin/naarad/blob/261e2c0760fd6a6b0ee59064180bd8e3674311fe/lib/luminol/src/luminol/utils.py#L40-L58", "repo": "naarad", "func_name": "read_csv", "original_string": ["def read_csv(csv_name):\n", "  \"\"\"\n", "  Read data from a csv file into a dictionary.\n", "  :param str csv_name: path to a csv file.\n", "  :return dict: a dictionary represents the data in file.\n", "  \"\"\"\n", "  data = {}\n", "  if not isinstance(csv_name, (str, unicode)):\n", "    raise exceptions.InvalidDataFormat('luminol.utils: csv_name has to be a string!')\n", "  with open(csv_name, 'r') as csv_data:\n", "    reader = csv.reader(csv_data, delimiter=',', quotechar='|')\n", "    for row in reader:\n", "      try:\n", "        key = to_epoch(row[0])\n", "        value = float(row[1])\n", "        data[key] = value\n", "      except ValueError:\n", "        pass\n", "  return data\n"], "language": "python", "code": "def read_csv(csv_name):\n    \"\"\"\"\"\"\n    data = {}\n    if not isinstance(csv_name, (str, unicode)):\n        raise exceptions.InvalidDataFormat(\n            'luminol.utils: csv_name has to be a string!')\n    with open(csv_name, 'r') as csv_data:\n        reader = csv.reader(csv_data, delimiter=',', quotechar='|')\n        for row in reader:\n            try:\n                key = to_epoch(row[0])\n                value = float(row[1])\n                data[key] = value\n            except ValueError:\n                pass\n    return data\n", "code_tokens": ["read", "csv", "csv", "name", "data", "if", "not", "isinstance", "csv", "name", "str", "unicode", "raise", "exceptions", "invaliddataformat", "luminol", "utils", "csv", "name", "has", "to", "be", "string", "with", "open", "csv", "name", "as", "csv", "data", "reader", "csv", "reader", "csv", "data", "delimiter", "quotechar", "for", "row", "in", "reader", "try", "key", "to", "epoch", "row", "0", "value", "float", "row", "1", "data", "key", "value", "except", "valueerror", "pass", "return", "data"], "docstring": "convert json to csv", "docstring_tokens": ["convert", "json", "to", "csv"], "idx": 188}
{"url": "https://github.com/PythonSanSebastian/docstamp/blob/b43808f2e15351b0b2f0b7eade9c7ef319c9e646/docstamp/file_utils.py#L138-L170", "repo": "docstamp", "func_name": "csv_to_json", "original_string": ["def csv_to_json(csv_filepath, json_filepath, fieldnames, ignore_first_line=True):\n", "    \"\"\" Convert a CSV file in `csv_filepath` into a JSON file in `json_filepath`.\n", "\n", "    Parameters\n", "    ----------\n", "    csv_filepath: str\n", "        Path to the input CSV file.\n", "\n", "    json_filepath: str\n", "        Path to the output JSON file. Will be overwritten if exists.\n", "\n", "    fieldnames: List[str]\n", "        Names of the fields in the CSV file.\n", "\n", "    ignore_first_line: bool\n", "    \"\"\"\n", "    import csv\n", "    import json\n", "\n", "    csvfile = open(csv_filepath, 'r')\n", "    jsonfile = open(json_filepath, 'w')\n", "\n", "    reader = csv.DictReader(csvfile, fieldnames)\n", "    rows = []\n", "    if ignore_first_line:\n", "        next(reader)\n", "\n", "    for row in reader:\n", "        rows.append(row)\n", "\n", "    json.dump(rows, jsonfile)\n", "    jsonfile.close()\n", "    csvfile.close()\n"], "language": "python", "code": "def csv_to_json(csv_filepath, json_filepath, fieldnames, ignore_first_line=True\n    ):\n    \"\"\"\"\"\"\n    import csv\n    import json\n    csvfile = open(csv_filepath, 'r')\n    jsonfile = open(json_filepath, 'w')\n    reader = csv.DictReader(csvfile, fieldnames)\n    rows = []\n    if ignore_first_line:\n        next(reader)\n    for row in reader:\n        rows.append(row)\n    json.dump(rows, jsonfile)\n    jsonfile.close()\n    csvfile.close()\n", "code_tokens": ["csv", "to", "json", "csv", "filepath", "json", "filepath", "fieldnames", "ignore", "first", "line", "true", "import", "csv", "import", "json", "csvfile", "open", "csv", "filepath", "jsonfile", "open", "json", "filepath", "reader", "csv", "dictreader", "csvfile", "fieldnames", "rows", "if", "ignore", "first", "line", "next", "reader", "for", "row", "in", "reader", "rows", "append", "row", "json", "dump", "rows", "jsonfile", "jsonfile", "close", "csvfile", "close"], "docstring": "convert json to csv", "docstring_tokens": ["convert", "json", "to", "csv"], "idx": 189}
{"url": "https://github.com/oplatek/csv2json/blob/f2f95db71ba2ce683fd6d0d3e2f13c9d0a77ceb6/csv2json/__init__.py#L20-L51", "repo": "csv2json", "func_name": "convert", "original_string": ["def convert(csv, json, **kwargs):\n", "    '''Convert csv to json.\n", "\n", "    csv:  filename or file-like object\n", "    json: filename  or file-like object\n", "\n", "\n", "    if csv is '-' or None:\n", "        stdin is used for input\n", "    if json is '-' or None:\n", "        stdout is used for output\n", "    '''\n", "\n", "    csv_local, json_local = None, None\n", "    try:\n", "        if csv == '-' or csv is None:\n", "            csv = sys.stdin\n", "        elif isinstance(csv, str):\n", "            csv = csv_local = open(csv, 'r')\n", "\n", "        if json == '-' or json is None:\n", "            json = sys.stdout\n", "        elif isinstance(json, str):\n", "            json = json_local = open(json, 'w')\n", "\n", "        data = load_csv(csv, **kwargs)\n", "        save_json(data, json, **kwargs)\n", "    finally:\n", "        if csv_local is not None:\n", "            csv_local.close()\n", "        if json_local is not None:\n", "            json_local.close()\n"], "language": "python", "code": "def convert(csv, json, **kwargs):\n    \"\"\"\"\"\"\n    csv_local, json_local = None, None\n    try:\n        if csv == '-' or csv is None:\n            csv = sys.stdin\n        elif isinstance(csv, str):\n            csv = csv_local = open(csv, 'r')\n        if json == '-' or json is None:\n            json = sys.stdout\n        elif isinstance(json, str):\n            json = json_local = open(json, 'w')\n        data = load_csv(csv, **kwargs)\n        save_json(data, json, **kwargs)\n    finally:\n        if csv_local is not None:\n            csv_local.close()\n        if json_local is not None:\n            json_local.close()\n", "code_tokens": ["convert", "csv", "json", "kwargs", "csv", "local", "json", "local", "none", "none", "try", "if", "csv", "or", "csv", "is", "none", "csv", "sys", "stdin", "elif", "isinstance", "csv", "str", "csv", "csv", "local", "open", "csv", "if", "json", "or", "json", "is", "none", "json", "sys", "stdout", "elif", "isinstance", "json", "str", "json", "json", "local", "open", "json", "data", "load", "csv", "csv", "kwargs", "save", "json", "data", "json", "kwargs", "finally", "if", "csv", "local", "is", "not", "none", "csv", "local", "close", "if", "json", "local", "is", "not", "none", "json", "local", "close"], "docstring": "convert json to csv", "docstring_tokens": ["convert", "json", "to", "csv"], "idx": 190}
{"url": "https://github.com/flatironinstitute/kbucket/blob/867915ebb0ea153a399c3e392698f89bf43c7903/kbucket/kbucketclient.py#L639-L645", "repo": "kbucket", "func_name": "_read_json_file", "original_string": ["def _read_json_file(path):\n", "  try:\n", "    with open(path) as f:\n", "      return json.load(f)\n", "  except:\n", "    print ('Warning: Unable to read or parse json file: '+path)\n", "    return None\n"], "language": "python", "code": "def _read_json_file(path):\n    try:\n        with open(path) as f:\n            return json.load(f)\n    except:\n        print('Warning: Unable to read or parse json file: ' + path)\n        return None\n", "code_tokens": ["read", "json", "file", "path", "try", "with", "open", "path", "as", "return", "json", "load", "except", "print", "warning", "unable", "to", "read", "or", "parse", "json", "file", "path", "return", "none"], "docstring": "parse json file", "docstring_tokens": ["parse", "json", "file"], "idx": 191}
{"url": "https://github.com/s1s1ty/py-jsonq/blob/9625597a2578bddcbed4e540174d5253b1fc3b75/pyjsonq/query.py#L46-L60", "repo": "py-jsonq", "func_name": "__parse_json_file", "original_string": ["    def __parse_json_file(self, file_path):\n", "        \"\"\"Process Json file data\n", "\n", "        :@param file_path\n", "        :@type file_path: string\n", "\n", "        :@throws IOError\n", "        \"\"\"\n", "        if file_path == '' or os.path.splitext(file_path)[1] != '.json':\n", "            raise IOError('Invalid Json file')\n", "\n", "        with open(file_path) as json_file:\n", "            self._raw_data = json.load(json_file)\n", "\n", "        self._json_data = copy.deepcopy(self._raw_data)\n"], "language": "python", "code": "def __parse_json_file(self, file_path):\n    \"\"\"\"\"\"\n    if file_path == '' or os.path.splitext(file_path)[1] != '.json':\n        raise IOError('Invalid Json file')\n    with open(file_path) as json_file:\n        self._raw_data = json.load(json_file)\n    self._json_data = copy.deepcopy(self._raw_data)\n", "code_tokens": ["parse", "json", "file", "self", "file", "path", "if", "file", "path", "or", "os", "path", "splitext", "file", "path", "1", "json", "raise", "ioerror", "invalid", "json", "file", "with", "open", "file", "path", "as", "json", "file", "self", "raw", "data", "json", "load", "json", "file", "self", "json", "data", "copy", "deepcopy", "self", "raw", "data"], "docstring": "parse json file", "docstring_tokens": ["parse", "json", "file"], "idx": 192}
{"url": "https://github.com/danhper/python-i18n/blob/bbba4b7ec091997ea8df2067acd7af316ee00b31/i18n/loaders/json_loader.py#L10-L14", "repo": "python-i18n", "func_name": "parse_file", "original_string": ["    def parse_file(self, file_content):\n", "        try:\n", "            return json.loads(file_content)\n", "        except ValueError as e:\n", "            raise I18nFileLoadError(\"invalid JSON: {0}\".format(e.strerror))\n"], "language": "python", "code": "def parse_file(self, file_content):\n    try:\n        return json.loads(file_content)\n    except ValueError as e:\n        raise I18nFileLoadError('invalid JSON: {0}'.format(e.strerror))\n", "code_tokens": ["parse", "file", "self", "file", "content", "try", "return", "json", "loads", "file", "content", "except", "valueerror", "as", "raise", "invalid", "json", "0", "format", "strerror"], "docstring": "parse json file", "docstring_tokens": ["parse", "json", "file"], "idx": 193}
{"url": "https://github.com/release-engineering/productmd/blob/49256bf2e8c84124f42346241140b986ad7bfc38/productmd/common.py#L302-L315", "repo": "productmd", "func_name": "parse_file", "original_string": ["    def parse_file(self, f):\n", "        # parse file, return parser or dict with data\n", "        if hasattr(f, \"seekable\"):\n", "            if f.seekable():\n", "                f.seek(0)\n", "        elif hasattr(f, \"seek\"):\n", "            f.seek(0)\n", "        if six.PY3 and isinstance(f, six.moves.http_client.HTTPResponse):\n", "            # HTTPResponse needs special handling in py3\n", "            reader = codecs.getreader(\"utf-8\")\n", "            parser = json.load(reader(f))\n", "        else:\n", "            parser = json.load(f)\n", "        return parser\n"], "language": "python", "code": "def parse_file(self, f):\n    if hasattr(f, 'seekable'):\n        if f.seekable():\n            f.seek(0)\n    elif hasattr(f, 'seek'):\n        f.seek(0)\n    if six.PY3 and isinstance(f, six.moves.http_client.HTTPResponse):\n        reader = codecs.getreader('utf-8')\n        parser = json.load(reader(f))\n    else:\n        parser = json.load(f)\n    return parser\n", "code_tokens": ["parse", "file", "self", "if", "hasattr", "seekable", "if", "seekable", "seek", "0", "elif", "hasattr", "seek", "seek", "0", "if", "six", "and", "isinstance", "six", "moves", "http", "client", "httpresponse", "reader", "codecs", "getreader", "utf", "8", "parser", "json", "load", "reader", "else", "parser", "json", "load", "return", "parser"], "docstring": "parse json file", "docstring_tokens": ["parse", "json", "file"], "idx": 194}
{"url": "https://github.com/nkmathew/yasi-sexp-indenter/blob/6ec2a4675e79606c555bcb67494a0ba994b05805/yasi.py#L551-L568", "repo": "yasi-sexp-indenter", "func_name": "parse_rc_json", "original_string": ["    ['while', 'if', 'case', 'dotimes', 'define', 'dolist', 'catch',\n", "     'throw', 'lambda', 'lambda-macro', 'when', 'unless', 'letex', 'begin',\n", "     'dostring', 'let', 'letn', 'doargs', 'define-macro', 'until', 'do-until',\n", "     'do-while', 'for-all', 'find-all', 'for'\n", "     ]\n", "\n", "# The 'if' and 'else' part of an if block should have different indent levels so\n", "# that they can stand out since there's no else Keyword in Lisp/Scheme to make\n", "# this explicit.  list IF_LIKE helps us track these keywords.\n", "IF_LIKE = ['if']\n", "\n", "\n", "@lru_cache(maxsize=None)\n", "def parse_rc_json():\n", "    \"\"\" Reads the json configuration file(.yasirc.json), parses it and returns the\n", "    dictionary\n", "    \"\"\"\n", "    fname = '.yasirc.json'\n"], "language": "python", "code": "@lru_cache(maxsize=None)\ndef parse_rc_json():\n    \"\"\"\"\"\"\n    fname = '.yasirc.json'\n", "code_tokens": ["parse", "rc", "json", "fname", "yasirc", "json"], "docstring": "parse json file", "docstring_tokens": ["parse", "json", "file"], "idx": 195}
{"url": "https://github.com/djtaylor/python-lsbinit/blob/a41fc551226f61ac2bf1b8b0f3f5395db85e75a2/lsbinit/pid.py#L43-L78", "repo": "python-lsbinit", "func_name": "ps", "original_string": ["    def ps(self):\n", "        \"\"\"\n", "        Get the process information from the system PS command.\n", "        \"\"\"\n", "        \n", "        # Get the process ID\n", "        pid = self.get()\n", "        \n", "        # Parent / child processes\n", "        parent   = None\n", "        children = []\n", "        \n", "        # If the process is running\n", "        if pid:\n", "            proc   = Popen(['ps', '-ef'], stdout=PIPE)\n", "            for _line in proc.stdout.readlines():\n", "                line = self.unicode(_line.rstrip())\n", "                \n", "                # Get the current PID / parent PID\n", "                this_pid, this_parent = self._ps_extract_pid(line)\n", "                try:\n", "                    \n", "                    # If scanning a child process\n", "                    if int(pid) == int(this_parent):\n", "                        children.append('{}; [{}]'.format(this_pid.rstrip(), re.sub(' +', ' ', line)))\n", "                    \n", "                    # If scanning the parent process\n", "                    if int(pid) == int(this_pid):\n", "                        parent = re.sub(' +', ' ', line)\n", "                        \n", "                # Ignore value errors\n", "                except ValueError:\n", "                    continue\n", "                \n", "        # Return the parent PID and any children processes\n", "        return (parent, children)\n"], "language": "python", "code": "def ps(self):\n    \"\"\"\"\"\"\n    pid = self.get()\n    parent = None\n    children = []\n    if pid:\n        proc = Popen(['ps', '-ef'], stdout=PIPE)\n        for _line in proc.stdout.readlines():\n            line = self.unicode(_line.rstrip())\n            this_pid, this_parent = self._ps_extract_pid(line)\n            try:\n                if int(pid) == int(this_parent):\n                    children.append('{}; [{}]'.format(this_pid.rstrip(), re\n                        .sub(' +', ' ', line)))\n                if int(pid) == int(this_pid):\n                    parent = re.sub(' +', ' ', line)\n            except ValueError:\n                continue\n    return parent, children\n", "code_tokens": ["ps", "self", "pid", "self", "get", "parent", "none", "children", "if", "pid", "proc", "popen", "ps", "ef", "stdout", "pipe", "for", "line", "in", "proc", "stdout", "readlines", "line", "self", "unicode", "line", "rstrip", "this", "pid", "this", "parent", "self", "ps", "extract", "pid", "line", "try", "if", "int", "pid", "int", "this", "parent", "children", "append", "format", "this", "pid", "rstrip", "re", "sub", "line", "if", "int", "pid", "int", "this", "pid", "parent", "re", "sub", "line", "except", "valueerror", "continue", "return", "parent", "children"], "docstring": "get current process id", "docstring_tokens": ["get", "current", "process", "id"], "idx": 196}
{"url": "https://github.com/cloud9ers/gurumate/blob/075dc74d1ee62a8c6b7a8bf2b271364f01629d1e/environment/lib/python2.7/site-packages/gurumate-2.8.6-py2.7.egg/gurumate/linux2/procs.py#L43-L46", "repo": "gurumate", "func_name": "get_pid", "original_string": ["def get_pid(PROCNAME):\n", "    for proc in psutil.process_iter():\n", "        if proc.name == PROCNAME:\n", "            return proc.pid\n"], "language": "python", "code": "def get_pid(PROCNAME):\n    for proc in psutil.process_iter():\n        if proc.name == PROCNAME:\n            return proc.pid\n", "code_tokens": ["get", "pid", "procname", "for", "proc", "in", "psutil", "process", "iter", "if", "proc", "name", "procname", "return", "proc", "pid"], "docstring": "get current process id", "docstring_tokens": ["get", "current", "process", "id"], "idx": 197}
{"url": "https://github.com/helixyte/everest/blob/70c9b93c3061db5cb62428349d18b8fb8566411b/everest/representers/xml.py#L440-L450", "repo": "everest", "func_name": "get_id", "original_string": ["    def get_id(self):\n", "        # FIXME: This will not work with ID strings that happen to be\n", "        #        convertible to an int.\n", "        id_str = self.get('id')\n", "        try:\n", "            id_val = int(id_str)\n", "        except ValueError:\n", "            id_val = id_str\n", "        except TypeError: # Happens if the id string is None.\n", "            id_val = id_str\n", "        return id_val\n"], "language": "python", "code": "def get_id(self):\n    id_str = self.get('id')\n    try:\n        id_val = int(id_str)\n    except ValueError:\n        id_val = id_str\n    except TypeError:\n        id_val = id_str\n    return id_val\n", "code_tokens": ["get", "id", "self", "id", "str", "self", "get", "id", "try", "id", "val", "int", "id", "str", "except", "valueerror", "id", "val", "id", "str", "except", "typeerror", "id", "val", "id", "str", "return", "id", "val"], "docstring": "get current process id", "docstring_tokens": ["get", "current", "process", "id"], "idx": 198}
{"url": "https://github.com/bcbio/bcbio-nextgen/blob/6a9348c0054ccd5baffd22f1bb7d0422f6978b20/bcbio/cwl/workflow.py#L421-L437", "repo": "bcbio-nextgen", "func_name": "_create_variable", "original_string": ["def _create_variable(orig_v, step, variables):\n", "    \"\"\"Create a new output variable, potentially over-writing existing or creating new.\n", "    \"\"\"\n", "    # get current variable, and convert to be the output of our process step\n", "    try:\n", "        v = _get_variable(orig_v[\"id\"], variables)\n", "    except ValueError:\n", "        v = copy.deepcopy(orig_v)\n", "        if not isinstance(v[\"id\"], six.string_types):\n", "            v[\"id\"] = _get_string_vid(v[\"id\"])\n", "    for key, val in orig_v.items():\n", "        if key not in [\"id\", \"type\"]:\n", "            v[key] = val\n", "    if orig_v.get(\"type\") != \"null\":\n", "        v[\"type\"] = orig_v[\"type\"]\n", "    v[\"id\"] = \"%s/%s\" % (step.name, get_base_id(v[\"id\"]))\n", "    return v\n"], "language": "python", "code": "def _create_variable(orig_v, step, variables):\n    \"\"\"\"\"\"\n    try:\n        v = _get_variable(orig_v['id'], variables)\n    except ValueError:\n        v = copy.deepcopy(orig_v)\n        if not isinstance(v['id'], six.string_types):\n            v['id'] = _get_string_vid(v['id'])\n    for key, val in orig_v.items():\n        if key not in ['id', 'type']:\n            v[key] = val\n    if orig_v.get('type') != 'null':\n        v['type'] = orig_v['type']\n    v['id'] = '%s/%s' % (step.name, get_base_id(v['id']))\n    return v\n", "code_tokens": ["create", "variable", "orig", "step", "variables", "try", "get", "variable", "orig", "id", "variables", "except", "valueerror", "copy", "deepcopy", "orig", "if", "not", "isinstance", "id", "six", "string", "types", "id", "get", "string", "vid", "id", "for", "key", "val", "in", "orig", "items", "if", "key", "not", "in", "id", "type", "key", "val", "if", "orig", "get", "type", "null", "type", "orig", "type", "id", "step", "name", "get", "base", "id", "id", "return"], "docstring": "get current process id", "docstring_tokens": ["get", "current", "process", "id"], "idx": 199}
{"url": "https://github.com/jobec/rfc5424-logging-handler/blob/9c4f669c5e54cf382936cd950e2204caeb6d05f0/rfc5424logging/handler.py#L277-L281", "repo": "rfc5424-logging-handler", "func_name": "get_procid", "original_string": ["\n", "    def get_procid(self, record):\n", "        procid = getattr(record, 'procid', self.procid)\n", "        if procid is None or procid == '':\n", "            procid = getattr(record, 'process', NILVALUE)\n"], "language": "python", "code": "def get_procid(self, record):\n    procid = getattr(record, 'procid', self.procid)\n    if procid is None or procid == '':\n        procid = getattr(record, 'process', NILVALUE)\n", "code_tokens": ["get", "procid", "self", "record", "procid", "getattr", "record", "procid", "self", "procid", "if", "procid", "is", "none", "or", "procid", "procid", "getattr", "record", "process", "nilvalue"], "docstring": "get current process id", "docstring_tokens": ["get", "current", "process", "id"], "idx": 200}
{"url": "https://github.com/fabioz/PyDev.Debugger/blob/ed9c4307662a5593b8a7f1f3389ecd0e79b8c503/pydevd_attach_to_process/winappdbg/thread.py#L204-L226", "repo": "PyDev.Debugger", "func_name": "get_pid", "original_string": ["    def get_pid(self):\n", "        \"\"\"\n", "        @rtype:  int\n", "        @return: Parent process global ID.\n", "\n", "        @raise WindowsError: An error occured when calling a Win32 API function.\n", "        @raise RuntimeError: The parent process ID can't be found.\n", "        \"\"\"\n", "        if self.dwProcessId is None:\n", "            if self.__process is not None:\n", "                # Infinite loop if self.__process is None\n", "                self.dwProcessId = self.get_process().get_pid()\n", "            else:\n", "                try:\n", "                    # I wish this had been implemented before Vista...\n", "                    # XXX TODO find the real ntdll call under this api\n", "                    hThread = self.get_handle(\n", "                                        win32.THREAD_QUERY_LIMITED_INFORMATION)\n", "                    self.dwProcessId = win32.GetProcessIdOfThread(hThread)\n", "                except AttributeError:\n", "                    # This method is really bad :P\n", "                    self.dwProcessId = self.__get_pid_by_scanning()\n", "        return self.dwProcessId\n"], "language": "python", "code": "def get_pid(self):\n    \"\"\"\"\"\"\n    if self.dwProcessId is None:\n        if self.__process is not None:\n            self.dwProcessId = self.get_process().get_pid()\n        else:\n            try:\n                hThread = self.get_handle(win32.\n                    THREAD_QUERY_LIMITED_INFORMATION)\n                self.dwProcessId = win32.GetProcessIdOfThread(hThread)\n            except AttributeError:\n                self.dwProcessId = self.__get_pid_by_scanning()\n    return self.dwProcessId\n", "code_tokens": ["get", "pid", "self", "if", "self", "dwprocessid", "is", "none", "if", "self", "process", "is", "not", "none", "self", "dwprocessid", "self", "get", "process", "get", "pid", "else", "try", "hthread", "self", "get", "handle", "thread", "query", "limited", "information", "self", "dwprocessid", "getprocessidofthread", "hthread", "except", "attributeerror", "self", "dwprocessid", "self", "get", "pid", "by", "scanning", "return", "self", "dwprocessid"], "docstring": "get current process id", "docstring_tokens": ["get", "current", "process", "id"], "idx": 201}
{"url": "https://github.com/lionheart/django-pyodbc/blob/46adda7b0bfabfa2640f72592c6f6f407f78b363/django_pyodbc/compiler.py#L107-L111", "repo": "django-pyodbc", "func_name": "_break", "original_string": ["def _break(s, find):\n", "    \"\"\"Break a string s into the part before the substring to find,\n", "    and the part including and after the substring.\"\"\"\n", "    i = s.find(find)\n", "    return s[:i], s[i:]\n"], "language": "python", "code": "def _break(s, find):\n    \"\"\"\"\"\"\n    i = s.find(find)\n    return s[:i], s[i:]\n", "code_tokens": ["break", "find", "find", "find", "return"], "docstring": "find int in string", "docstring_tokens": ["find", "int", "in", "string"], "idx": 202}
{"url": "https://github.com/enkore/i3pystatus/blob/14cfde967cecf79b40e223e35a04600f4c875af7/i3pystatus/core/util.py#L80-L84", "repo": "i3pystatus", "func_name": "get", "original_string": ["    def get(self, find_id):\n", "        find_id = int(find_id)\n", "        for module in self:\n", "            if id(module) == find_id:\n", "                return module\n"], "language": "python", "code": "def get(self, find_id):\n    find_id = int(find_id)\n    for module in self:\n        if id(module) == find_id:\n            return module\n", "code_tokens": ["get", "self", "find", "id", "find", "id", "int", "find", "id", "for", "module", "in", "self", "if", "id", "module", "find", "id", "return", "module"], "docstring": "find int in string", "docstring_tokens": ["find", "int", "in", "string"], "idx": 203}
{"url": "https://github.com/acutesoftware/AIKIF/blob/fcf1582dc5f884b9a4fa7c6e20e9de9d94d21d03/aikif/core_data.py#L311-L318", "repo": "AIKIF", "func_name": "find", "original_string": ["    def find(self, txt):\n", "        result = []\n", "        for e in self.table:\n", "            print('find(self, txt) e = ', e)\n", "            if txt in str(e):\n", "                result.append(e)\n", "                #print(e)\n", "        return result\n"], "language": "python", "code": "def find(self, txt):\n    result = []\n    for e in self.table:\n        print('find(self, txt) e = ', e)\n        if txt in str(e):\n            result.append(e)\n    return result\n", "code_tokens": ["find", "self", "txt", "result", "for", "in", "self", "table", "print", "find", "self", "txt", "if", "txt", "in", "str", "result", "append", "return", "result"], "docstring": "find int in string", "docstring_tokens": ["find", "int", "in", "string"], "idx": 204}
{"url": "https://github.com/fdb/aufmachen/blob/f2986a0cf087ac53969f82b84d872e3f1c6986f4/aufmachen/websites/immoweb.py#L115-L130", "repo": "aufmachen", "func_name": "find_number", "original_string": ["def find_number(regex, s):\n", "    \"\"\"Find a number using a given regular expression.\n", "    If the string cannot be found, returns None.\n", "    The regex should contain one matching group, \n", "    as only the result of the first group is returned.\n", "    The group should only contain numeric characters ([0-9]+).\n", "    \n", "    s - The string to search.\n", "    regex - A string containing the regular expression.\n", "    \n", "    Returns an integer or None.\n", "    \"\"\"\n", "    result = find_string(regex, s)\n", "    if result is None:\n", "        return None\n", "    return int(result)\n"], "language": "python", "code": "def find_number(regex, s):\n    \"\"\"\"\"\"\n    result = find_string(regex, s)\n    if result is None:\n        return None\n    return int(result)\n", "code_tokens": ["find", "number", "regex", "result", "find", "string", "regex", "if", "result", "is", "none", "return", "none", "return", "int", "result"], "docstring": "find int in string", "docstring_tokens": ["find", "int", "in", "string"], "idx": 205}
{"url": "https://github.com/hubo1016/vlcp/blob/239055229ec93a99cc7e15208075724ccf543bd1/vlcp/service/sdn/plugins/networkvxlandriver.py#L217-L233", "repo": "vlcp", "func_name": "_isavaliablevni", "original_string": ["def _isavaliablevni(vnirange,allocated,vni):\n", "    \n", "    find = False\n", "    for start,end in vnirange:\n", "        if start <= int(vni) <= end:\n", "            find = True\n", "            break\n", "\n", "    if find:\n", "        if str(vni) not in allocated:\n", "            find = True\n", "        else:\n", "            find = False\n", "    else:\n", "        find = False\n", "\n", "    return find\n"], "language": "python", "code": "def _isavaliablevni(vnirange, allocated, vni):\n    find = False\n    for start, end in vnirange:\n        if start <= int(vni) <= end:\n            find = True\n            break\n    if find:\n        if str(vni) not in allocated:\n            find = True\n        else:\n            find = False\n    else:\n        find = False\n    return find\n", "code_tokens": ["isavaliablevni", "vnirange", "allocated", "vni", "find", "false", "for", "start", "end", "in", "vnirange", "if", "start", "int", "vni", "end", "find", "true", "break", "if", "find", "if", "str", "vni", "not", "in", "allocated", "find", "true", "else", "find", "false", "else", "find", "false", "return", "find"], "docstring": "find int in string", "docstring_tokens": ["find", "int", "in", "string"], "idx": 206}
{"url": "https://github.com/jwkvam/plotlywrapper/blob/762b42912e824fecb1212c186900f2ebdd0ab12b/plotlywrapper.py#L832-L850", "repo": "plotlywrapper", "func_name": "heatmap", "original_string": ["def heatmap(z, x=None, y=None, colorscale='Viridis'):\n", "    \"\"\"Create a heatmap.\n", "\n", "    Parameters\n", "    ----------\n", "    z : TODO\n", "    x : TODO, optional\n", "    y : TODO, optional\n", "    colorscale : TODO, optional\n", "\n", "    Returns\n", "    -------\n", "    Chart\n", "\n", "\n", "    \"\"\"\n", "    z = np.atleast_1d(z)\n", "    data = [go.Heatmap(z=z, x=x, y=y, colorscale=colorscale)]\n", "    return Chart(data=data)\n"], "language": "python", "code": "def heatmap(z, x=None, y=None, colorscale='Viridis'):\n    \"\"\"\"\"\"\n    z = np.atleast_1d(z)\n    data = [go.Heatmap(z=z, x=x, y=y, colorscale=colorscale)]\n    return Chart(data=data)\n", "code_tokens": ["heatmap", "none", "none", "colorscale", "viridis", "np", "atleast", "data", "go", "heatmap", "colorscale", "colorscale", "return", "chart", "data", "data"], "docstring": "heatmap from 3d coordinates", "docstring_tokens": ["heatmap", "from", "3d", "coordinates"], "idx": 207}
{"url": "https://github.com/tensorpack/tensorpack/blob/d7a13cb74c9066bc791d7aafc3b744b60ee79a9f/examples/CaffeModels/load-cpm.py#L27-L32", "repo": "tensorpack", "func_name": "colorize", "original_string": ["def colorize(img, heatmap):\n", "    \"\"\" img: bgr, [0,255]\n", "        heatmap: [0,1]\n", "    \"\"\"\n", "    heatmap = viz.intensity_to_rgb(heatmap, cmap='jet')[:, :, ::-1]\n", "    return img * 0.5 + heatmap * 0.5\n"], "language": "python", "code": "def colorize(img, heatmap):\n    \"\"\"\"\"\"\n    heatmap = viz.intensity_to_rgb(heatmap, cmap='jet')[:, :, ::-1]\n    return img * 0.5 + heatmap * 0.5\n", "code_tokens": ["colorize", "img", "heatmap", "heatmap", "viz", "intensity", "to", "rgb", "heatmap", "cmap", "jet", "1", "return", "img", "0", "5", "heatmap", "0", "5"], "docstring": "heatmap from 3d coordinates", "docstring_tokens": ["heatmap", "from", "3d", "coordinates"], "idx": 208}
{"url": "https://github.com/flo-compbio/genometools/blob/dd962bb26d60a0f14ca14d8c9a4dd75768962c7d/genometools/expression/matrix.py#L295-L319", "repo": "genometools", "func_name": "get_figure", "original_string": ["    def get_figure(self, heatmap_kw=None, **kwargs):\n", "        \"\"\"Generate a plotly figure showing the matrix as a heatmap.\n", "\n", "        This is a shortcut for ``ExpMatrix.get_heatmap(...).get_figure(...)``.\n", "\n", "        See :func:`ExpHeatmap.get_figure` for keyword arguments.\n", "\n", "        Parameters\n", "        ----------\n", "        heatmap_kw : dict or None\n", "            If not None, dictionary containing keyword arguments to be passed\n", "            to the `ExpHeatmap` constructor.\n", "\n", "        Returns\n", "        -------\n", "        `plotly.graph_objs.Figure`\n", "            The plotly figure.\n", "        \"\"\"\n", "        if heatmap_kw is not None:\n", "            assert isinstance(heatmap_kw, dict)\n", "\n", "        if heatmap_kw is None:\n", "            heatmap_kw = {}\n", "\n", "        return self.get_heatmap(**heatmap_kw).get_figure(**kwargs)\n"], "language": "python", "code": "def get_figure(self, heatmap_kw=None, **kwargs):\n    \"\"\"\"\"\"\n    if heatmap_kw is not None:\n        assert isinstance(heatmap_kw, dict)\n    if heatmap_kw is None:\n        heatmap_kw = {}\n    return self.get_heatmap(**heatmap_kw).get_figure(**kwargs)\n", "code_tokens": ["get", "figure", "self", "heatmap", "kw", "none", "kwargs", "if", "heatmap", "kw", "is", "not", "none", "assert", "isinstance", "heatmap", "kw", "dict", "if", "heatmap", "kw", "is", "none", "heatmap", "kw", "return", "self", "get", "heatmap", "heatmap", "kw", "get", "figure", "kwargs"], "docstring": "heatmap from 3d coordinates", "docstring_tokens": ["heatmap", "from", "3d", "coordinates"], "idx": 209}
{"url": "https://github.com/kevinsprong23/aperture/blob/d0420fef3b25d8afc0e5ddcfb6fe5f0ff42b9799/aperture/heatmaps.py#L123-L144", "repo": "aperture", "func_name": "heatmap", "original_string": ["def heatmap(x, y, step=None, min_pt=None, max_pt=None,\n", "                 colormap='Blues', alpha=1, grid=False,\n", "                 colorbar=True, scale='lin',\n", "                 vmax='auto', vmin='auto', crop=True):\n", "    \"\"\"\n", "    function to take vectors x and y and hist them\n", "    \"\"\"\n", "    (x_vec, y_vec, hist_matrix) = calc_2d_hist(x, y, step, min_pt, max_pt)\n", "\n", "    # simple in this case because it is positive counts\n", "    if scale == 'log':\n", "        for row in hist_matrix:\n", "            for i, el in enumerate(row):\n", "                row[i] = 0 if row[i] == 0 else log10(row[i])\n", "\n", "    # plot\n", "    fig = plt.figure()\n", "    init_heatmap(x_vec, y_vec, hist_matrix, fig, colormap=colormap,\n", "                 alpha=alpha, grid=grid, colorbar=colorbar,\n", "                 vmax=vmax, vmin=vmin, crop=crop)\n", "\n", "    return fig\n"], "language": "python", "code": "def heatmap(x, y, step=None, min_pt=None, max_pt=None, colormap='Blues',\n    alpha=1, grid=False, colorbar=True, scale='lin', vmax='auto', vmin=\n    'auto', crop=True):\n    \"\"\"\"\"\"\n    x_vec, y_vec, hist_matrix = calc_2d_hist(x, y, step, min_pt, max_pt)\n    if scale == 'log':\n        for row in hist_matrix:\n            for i, el in enumerate(row):\n                row[i] = 0 if row[i] == 0 else log10(row[i])\n    fig = plt.figure()\n    init_heatmap(x_vec, y_vec, hist_matrix, fig, colormap=colormap, alpha=\n        alpha, grid=grid, colorbar=colorbar, vmax=vmax, vmin=vmin, crop=crop)\n    return fig\n", "code_tokens": ["heatmap", "step", "none", "min", "pt", "none", "max", "pt", "none", "colormap", "blues", "alpha", "1", "grid", "false", "colorbar", "true", "scale", "lin", "vmax", "auto", "vmin", "auto", "crop", "true", "vec", "vec", "hist", "matrix", "calc", "hist", "step", "min", "pt", "max", "pt", "if", "scale", "log", "for", "row", "in", "hist", "matrix", "for", "el", "in", "enumerate", "row", "row", "0", "if", "row", "0", "else", "row", "fig", "plt", "figure", "init", "heatmap", "vec", "vec", "hist", "matrix", "fig", "colormap", "colormap", "alpha", "alpha", "grid", "grid", "colorbar", "colorbar", "vmax", "vmax", "vmin", "vmin", "crop", "crop", "return", "fig"], "docstring": "heatmap from 3d coordinates", "docstring_tokens": ["heatmap", "from", "3d", "coordinates"], "idx": 210}
{"url": "https://github.com/chemlab/chemlab/blob/c8730966316d101e24f39ac3b96b51282aba0abe/chemlab/graphics/camera.py#L181-L194", "repo": "chemlab", "func_name": "_get_projection_matrix", "original_string": ["    def _get_projection_matrix(self):\n", "        # Convert from homogeneous 3d coordinates to \n", "        # 2D coordinates\n", "        \n", "        fov = self.fov*np.pi/180.0\n", "        \n", "        top = np.tan(fov * 0.5)*self.z_near\n", "        bottom = -top\n", "        \n", "        left = self.aspectratio * bottom\n", "        right = self.aspectratio * top\n", "        \n", "        return clip_matrix(left, right, bottom, top,\n", "                           self.z_near, self.z_far, perspective=True)\n"], "language": "python", "code": "def _get_projection_matrix(self):\n    fov = self.fov * np.pi / 180.0\n    top = np.tan(fov * 0.5) * self.z_near\n    bottom = -top\n    left = self.aspectratio * bottom\n    right = self.aspectratio * top\n    return clip_matrix(left, right, bottom, top, self.z_near, self.z_far,\n        perspective=True)\n", "code_tokens": ["get", "projection", "matrix", "self", "fov", "self", "fov", "np", "pi", "180", "0", "top", "np", "tan", "fov", "0", "5", "self", "near", "bottom", "top", "left", "self", "aspectratio", "bottom", "right", "self", "aspectratio", "top", "return", "clip", "matrix", "left", "right", "bottom", "top", "self", "near", "self", "far", "perspective", "true"], "docstring": "heatmap from 3d coordinates", "docstring_tokens": ["heatmap", "from", "3d", "coordinates"], "idx": 211}
{"url": "https://github.com/jwkvam/plotlywrapper/blob/762b42912e824fecb1212c186900f2ebdd0ab12b/doc/figures.py#L43-L45", "repo": "plotlywrapper", "func_name": "heatmap2", "original_string": ["def heatmap2():\n", "    x = np.arange(5)\n", "    pw.heatmap(z=np.arange(25), x=np.tile(x, 5), y=x.repeat(5)).save('fig_heatmap2.html', **options)\n"], "language": "python", "code": "def heatmap2():\n    x = np.arange(5)\n    pw.heatmap(z=np.arange(25), x=np.tile(x, 5), y=x.repeat(5)).save(\n        'fig_heatmap2.html', **options)\n", "code_tokens": ["np", "arange", "5", "pw", "heatmap", "np", "arange", "25", "np", "tile", "5", "repeat", "5", "save", "fig", "html", "options"], "docstring": "heatmap from 3d coordinates", "docstring_tokens": ["heatmap", "from", "3d", "coordinates"], "idx": 212}
{"url": "https://github.com/frictionlessdata/datapackage-pipelines/blob/3a34bbdf042d13c3bec5eef46ff360ee41403874/datapackage_pipelines/lib/dump/to_path.py#L16-L25", "repo": "datapackage-pipelines", "func_name": "write_file_to_output", "original_string": ["    def write_file_to_output(self, filename, path):\n", "        path = os.path.join(self.out_path, path)\n", "        # Avoid rewriting existing files\n", "        if self.add_filehash_to_path and os.path.exists(path):\n", "            return\n", "        path_part = os.path.dirname(path)\n", "        PathDumper.__makedirs(path_part)\n", "        shutil.copy(filename, path)\n", "        os.chmod(path, 0o666)\n", "        return path\n"], "language": "python", "code": "def write_file_to_output(self, filename, path):\n    path = os.path.join(self.out_path, path)\n    if self.add_filehash_to_path and os.path.exists(path):\n        return\n    path_part = os.path.dirname(path)\n    PathDumper.__makedirs(path_part)\n    shutil.copy(filename, path)\n    os.chmod(path, 438)\n    return path\n", "code_tokens": ["write", "file", "to", "output", "self", "filename", "path", "path", "os", "path", "join", "self", "out", "path", "path", "if", "self", "add", "filehash", "to", "path", "and", "os", "path", "exists", "path", "return", "path", "part", "os", "path", "dirname", "path", "pathdumper", "makedirs", "path", "part", "shutil", "copy", "filename", "path", "os", "chmod", "path", "438", "return", "path"], "docstring": "copying a file to a path", "docstring_tokens": ["copying", "a", "file", "to", "a", "path"], "idx": 213}
{"url": "https://github.com/RedHatInsights/insights-core/blob/b57cbf8ed7c089672426ede0441e0a4f789ef4a1/insights/client/archive.py#L93-L104", "repo": "insights-core", "func_name": "create_command_dir", "original_string": ["    def create_command_dir(self):\n", "        \"\"\"\n", "        Create the \"insights_commands\" dir\n", "        \"\"\"\n", "        self.create_archive_dir()\n", "        cmd_dir = os.path.join(self.archive_dir, \"insights_commands\")\n", "        logger.debug('Creating command directory %s...', cmd_dir)\n", "        if not os.path.exists(cmd_dir):\n", "            os.makedirs(cmd_dir, 0o700)\n", "        self.cmd_dir = cmd_dir\n", "        return self.cmd_dir\n", "\n"], "language": "python", "code": "def create_command_dir(self):\n    \"\"\"\"\"\"\n    self.create_archive_dir()\n    cmd_dir = os.path.join(self.archive_dir, 'insights_commands')\n    logger.debug('Creating command directory %s...', cmd_dir)\n    if not os.path.exists(cmd_dir):\n        os.makedirs(cmd_dir, 448)\n    self.cmd_dir = cmd_dir\n    return self.cmd_dir\n", "code_tokens": ["create", "command", "dir", "self", "self", "create", "archive", "dir", "cmd", "dir", "os", "path", "join", "self", "archive", "dir", "insights", "commands", "logger", "debug", "creating", "command", "directory", "cmd", "dir", "if", "not", "os", "path", "exists", "cmd", "dir", "os", "makedirs", "cmd", "dir", "448", "self", "cmd", "dir", "cmd", "dir", "return", "self", "cmd", "dir"], "docstring": "copying a file to a path", "docstring_tokens": ["copying", "a", "file", "to", "a", "path"], "idx": 214}
{"url": "https://github.com/dsoprea/PathManifest/blob/0f5cfd4925a61cc0eac150ff354200392d07ec74/pm/manifest.py#L239-L271", "repo": "PathManifest", "func_name": "__inject_files_to_staging", "original_string": ["    def __inject_files_to_staging(self, rel_filepaths, temp_path):\n", "        patch_files = {}\n", "        for rel_filepath in rel_filepaths:\n", "            from_filepath = os.path.join(self.__root_path, rel_filepath)\n", "            to_filepath = os.path.join(temp_path, rel_filepath)\n", "\n", "            _LOGGER.debug(\"Copying file to patch path: [%s] => [%s]\", \n", "                          from_filepath, to_filepath)\n", "\n", "            to_path = os.path.dirname(to_filepath)\n", "            if os.path.exists(to_path) is False:\n", "                os.makedirs(to_path)\n", "\n", "            with open(from_filepath, 'rb') as f:\n", "                with open(to_filepath, 'wb') as g:\n", "                    shutil.copyfileobj(f, g)\n", "\n", "            s = os.stat(from_filepath)\n", "            mtime_epoch = int(s.st_mtime)\n", "            filesize_b = s.st_size\n", "\n", "            # Set patch mtime.\n", "            os.utime(to_filepath, (mtime_epoch, mtime_epoch))\n", "\n", "            hash_ = self.__get_md5_for_rel_filepath(rel_filepath)\n", "\n", "            patch_files[rel_filepath] = {\n", "                'mtime_epoch': mtime_epoch,\n", "                'filesize_b': filesize_b,\n", "                'hash_md5': hash_,\n", "            }\n", "\n", "        return patch_files\n"], "language": "python", "code": "def __inject_files_to_staging(self, rel_filepaths, temp_path):\n    patch_files = {}\n    for rel_filepath in rel_filepaths:\n        from_filepath = os.path.join(self.__root_path, rel_filepath)\n        to_filepath = os.path.join(temp_path, rel_filepath)\n        _LOGGER.debug('Copying file to patch path: [%s] => [%s]',\n            from_filepath, to_filepath)\n        to_path = os.path.dirname(to_filepath)\n        if os.path.exists(to_path) is False:\n            os.makedirs(to_path)\n        with open(from_filepath, 'rb') as f:\n            with open(to_filepath, 'wb') as g:\n                shutil.copyfileobj(f, g)\n        s = os.stat(from_filepath)\n        mtime_epoch = int(s.st_mtime)\n        filesize_b = s.st_size\n        os.utime(to_filepath, (mtime_epoch, mtime_epoch))\n        hash_ = self.__get_md5_for_rel_filepath(rel_filepath)\n        patch_files[rel_filepath] = {'mtime_epoch': mtime_epoch,\n            'filesize_b': filesize_b, 'hash_md5': hash_}\n    return patch_files\n", "code_tokens": ["inject", "files", "to", "staging", "self", "rel", "filepaths", "temp", "path", "patch", "files", "for", "rel", "filepath", "in", "rel", "filepaths", "from", "filepath", "os", "path", "join", "self", "root", "path", "rel", "filepath", "to", "filepath", "os", "path", "join", "temp", "path", "rel", "filepath", "logger", "debug", "copying", "file", "to", "patch", "path", "from", "filepath", "to", "filepath", "to", "path", "os", "path", "dirname", "to", "filepath", "if", "os", "path", "exists", "to", "path", "is", "false", "os", "makedirs", "to", "path", "with", "open", "from", "filepath", "rb", "as", "with", "open", "to", "filepath", "wb", "as", "shutil", "copyfileobj", "os", "stat", "from", "filepath", "mtime", "epoch", "int", "st", "mtime", "filesize", "st", "size", "os", "utime", "to", "filepath", "mtime", "epoch", "mtime", "epoch", "hash", "self", "get", "for", "rel", "filepath", "rel", "filepath", "patch", "files", "rel", "filepath", "mtime", "epoch", "mtime", "epoch", "filesize", "filesize", "hash", "hash", "return", "patch", "files"], "docstring": "copying a file to a path", "docstring_tokens": ["copying", "a", "file", "to", "a", "path"], "idx": 215}
{"url": "https://github.com/klen/starter/blob/24a65c10d4ac5a9ca8fc1d8b3d54b3fb13603f5f/starter/core.py#L52-L58", "repo": "starter", "func_name": "copy_file", "original_string": ["    def copy_file(self, from_path, to_path):\n", "        \"\"\" Copy file. \"\"\"\n", "        if not op.exists(op.dirname(to_path)):\n", "            self.make_directory(op.dirname(to_path))\n", "\n", "        shutil.copy(from_path, to_path)\n", "        logging.debug('File copied: {0}'.format(to_path))\n"], "language": "python", "code": "def copy_file(self, from_path, to_path):\n    \"\"\"\"\"\"\n    if not op.exists(op.dirname(to_path)):\n        self.make_directory(op.dirname(to_path))\n    shutil.copy(from_path, to_path)\n    logging.debug('File copied: {0}'.format(to_path))\n", "code_tokens": ["copy", "file", "self", "from", "path", "to", "path", "if", "not", "op", "exists", "op", "dirname", "to", "path", "self", "make", "directory", "op", "dirname", "to", "path", "shutil", "copy", "from", "path", "to", "path", "logging", "debug", "file", "copied", "0", "format", "to", "path"], "docstring": "copying a file to a path", "docstring_tokens": ["copying", "a", "file", "to", "a", "path"], "idx": 216}
{"url": "https://github.com/tjguk/winshell/blob/1509d211ab3403dd1cff6113e4e13462d6dec35b/winshell.py#L266-L294", "repo": "winshell", "func_name": "copy_file", "original_string": ["def copy_file(\n", "    source_path,\n", "    target_path,\n", "    allow_undo=True,\n", "    no_confirm=False,\n", "    rename_on_collision=True,\n", "    silent=False,\n", "    extra_flags=0,\n", "    hWnd=None\n", "):\n", "    \"\"\"Perform a shell-based file copy. Copying in\n", "    this way allows the possibility of undo, auto-renaming,\n", "    and showing the \"flying file\" animation during the copy.\n", "\n", "    The default options allow for undo, don't automatically\n", "    clobber on a name clash, automatically rename on collision\n", "    and display the animation.\n", "    \"\"\"\n", "    return _file_operation(\n", "        shellcon.FO_COPY,\n", "        source_path,\n", "        target_path,\n", "        allow_undo,\n", "        no_confirm,\n", "        rename_on_collision,\n", "        silent,\n", "        extra_flags,\n", "        hWnd\n", "    )\n"], "language": "python", "code": "def copy_file(source_path, target_path, allow_undo=True, no_confirm=False,\n    rename_on_collision=True, silent=False, extra_flags=0, hWnd=None):\n    \"\"\"\"\"\"\n    return _file_operation(shellcon.FO_COPY, source_path, target_path,\n        allow_undo, no_confirm, rename_on_collision, silent, extra_flags, hWnd)\n", "code_tokens": ["copy", "file", "source", "path", "target", "path", "allow", "undo", "true", "no", "confirm", "false", "rename", "on", "collision", "true", "silent", "false", "extra", "flags", "0", "hwnd", "none", "return", "file", "operation", "shellcon", "fo", "copy", "source", "path", "target", "path", "allow", "undo", "no", "confirm", "rename", "on", "collision", "silent", "extra", "flags", "hwnd"], "docstring": "copying a file to a path", "docstring_tokens": ["copying", "a", "file", "to", "a", "path"], "idx": 217}
{"url": "https://github.com/Neurita/boyle/blob/2dae7199849395a209c887d5f30506e1de8a9ad9/scripts/filetree.py#L20-L72", "repo": "boyle", "func_name": "copy", "original_string": ["def copy(configfile='', destpath='', overwrite=False, sub_node=''):\n", "    \"\"\"Copies the files in the built file tree map\n", "    to despath.\n", "\n", "    :param configfile: string\n", "     Path to the FileTreeMap config file\n", "\n", "    :param destpath: string\n", "     Path to the files destination\n", "\n", "    :param overwrite: bool\n", "     Overwrite files if they already exist.\n", "\n", "    :param sub_node: string\n", "     Tree map configuration sub path.\n", "     Will copy only the contents within this sub-node\n", "\n", "    \"\"\"\n", "    log.info('Running {0} {1} {2}'.format(os.path.basename(__file__),\n", "                                          whoami(),\n", "                                          locals()))\n", "\n", "    assert(os.path.isfile(configfile))\n", "\n", "    if os.path.exists(destpath):\n", "        if os.listdir(destpath):\n", "            raise FolderAlreadyExists('Folder {0} already exists. Please clean '\n", "                                      'it or change destpath.'.format(destpath))\n", "    else:\n", "        log.info('Creating folder {0}'.format(destpath))\n", "        path(destpath).makedirs_p()\n", "\n", "    from boyle.files.file_tree_map import FileTreeMap\n", "    file_map = FileTreeMap()\n", "\n", "    try:\n", "        file_map.from_config_file(configfile)\n", "    except Exception as e:\n", "        raise FileTreeMapError(str(e))\n", "\n", "    if sub_node:\n", "        sub_map = file_map.get_node(sub_node)\n", "        if not sub_map:\n", "            raise FileTreeMapError('Could not find sub node '\n", "                                   '{0}'.format(sub_node))\n", "\n", "        file_map._filetree = {}\n", "        file_map._filetree[sub_node] = sub_map\n", "\n", "    try:\n", "        file_map.copy_to(destpath, overwrite=overwrite)\n", "    except Exception as e:\n", "        raise FileTreeMapError(str(e))\n"], "language": "python", "code": "def copy(configfile='', destpath='', overwrite=False, sub_node=''):\n    \"\"\"\"\"\"\n    log.info('Running {0} {1} {2}'.format(os.path.basename(__file__),\n        whoami(), locals()))\n    assert os.path.isfile(configfile)\n    if os.path.exists(destpath):\n        if os.listdir(destpath):\n            raise FolderAlreadyExists(\n                'Folder {0} already exists. Please clean it or change destpath.'\n                .format(destpath))\n    else:\n        log.info('Creating folder {0}'.format(destpath))\n        path(destpath).makedirs_p()\n    from boyle.files.file_tree_map import FileTreeMap\n    file_map = FileTreeMap()\n    try:\n        file_map.from_config_file(configfile)\n    except Exception as e:\n        raise FileTreeMapError(str(e))\n    if sub_node:\n        sub_map = file_map.get_node(sub_node)\n        if not sub_map:\n            raise FileTreeMapError('Could not find sub node {0}'.format(\n                sub_node))\n        file_map._filetree = {}\n        file_map._filetree[sub_node] = sub_map\n    try:\n        file_map.copy_to(destpath, overwrite=overwrite)\n    except Exception as e:\n        raise FileTreeMapError(str(e))\n", "code_tokens": ["copy", "configfile", "destpath", "overwrite", "false", "sub", "node", "log", "info", "running", "0", "1", "2", "format", "os", "path", "basename", "file", "whoami", "locals", "assert", "os", "path", "isfile", "configfile", "if", "os", "path", "exists", "destpath", "if", "os", "listdir", "destpath", "raise", "folderalreadyexists", "folder", "0", "already", "exists", "please", "clean", "it", "or", "change", "destpath", "format", "destpath", "else", "log", "info", "creating", "folder", "0", "format", "destpath", "path", "destpath", "makedirs", "from", "boyle", "files", "file", "tree", "map", "import", "filetreemap", "file", "map", "filetreemap", "try", "file", "map", "from", "config", "file", "configfile", "except", "exception", "as", "raise", "filetreemaperror", "str", "if", "sub", "node", "sub", "map", "file", "map", "get", "node", "sub", "node", "if", "not", "sub", "map", "raise", "filetreemaperror", "could", "not", "find", "sub", "node", "0", "format", "sub", "node", "file", "map", "filetree", "file", "map", "filetree", "sub", "node", "sub", "map", "try", "file", "map", "copy", "to", "destpath", "overwrite", "overwrite", "except", "exception", "as", "raise", "filetreemaperror", "str"], "docstring": "copying a file to a path", "docstring_tokens": ["copying", "a", "file", "to", "a", "path"], "idx": 218}
{"url": "https://github.com/joelfrederico/SciSalt/blob/7bf57c49c7dde0a8b0aa337fbd2fbd527ce7a67f/scisalt/facettools/print2elog.py#L13-L23", "repo": "SciSalt", "func_name": "_copy_file", "original_string": ["def _copy_file(filepath, fulltime):\n", "    if filepath is None:\n", "        filepath_out = ''\n", "    else:\n", "        filename  = _os.path.basename(filepath)\n", "        root, ext = _os.path.splitext(filename)\n", "        filepath_out = fulltime + ext\n", "        copypath = _os.path.join(basedir, filepath_out)\n", "        _shutil.copyfile(filepath, copypath)\n", "\n", "    return filepath_out\n"], "language": "python", "code": "def _copy_file(filepath, fulltime):\n    if filepath is None:\n        filepath_out = ''\n    else:\n        filename = _os.path.basename(filepath)\n        root, ext = _os.path.splitext(filename)\n        filepath_out = fulltime + ext\n        copypath = _os.path.join(basedir, filepath_out)\n        _shutil.copyfile(filepath, copypath)\n    return filepath_out\n", "code_tokens": ["copy", "file", "filepath", "fulltime", "if", "filepath", "is", "none", "filepath", "out", "else", "filename", "os", "path", "basename", "filepath", "root", "ext", "os", "path", "splitext", "filename", "filepath", "out", "fulltime", "ext", "copypath", "os", "path", "join", "basedir", "filepath", "out", "shutil", "copyfile", "filepath", "copypath", "return", "filepath", "out"], "docstring": "copying a file to a path", "docstring_tokens": ["copying", "a", "file", "to", "a", "path"], "idx": 219}
{"url": "https://github.com/fredericklussier/ObservablePy/blob/fd7926a0568621f80b1d567d18f199976f1fa4e8/observablePy/ObservableStore.py#L72-L84", "repo": "ObservablePy", "func_name": "add", "original_string": ["    def add(self, observableElement):\n", "        \"\"\"\n", "        add an observable element\n", "\n", "        :param str observableElement: the name of the observable element\n", "        :raises RuntimeError: if element name already exist in the store\n", "        \"\"\"\n", "        if observableElement not in self._observables:\n", "            self._observables.append(observableElement)\n", "        else:\n", "            raise RuntimeError(\n", "                \"{0} is already an observable element\"\n", "                .format(observableElement))\n"], "language": "python", "code": "def add(self, observableElement):\n    \"\"\"\"\"\"\n    if observableElement not in self._observables:\n        self._observables.append(observableElement)\n    else:\n        raise RuntimeError('{0} is already an observable element'.format(\n            observableElement))\n", "code_tokens": ["add", "self", "observableelement", "if", "observableelement", "not", "in", "self", "observables", "self", "observables", "append", "observableelement", "else", "raise", "runtimeerror", "0", "is", "already", "an", "observable", "element", "format", "observableelement"], "docstring": "get current observable value", "docstring_tokens": ["get", "current", "observable", "value"], "idx": 220}
{"url": "https://github.com/timofurrer/observable/blob/a6a764efaf9408a334bdb1ddf4327d9dbc4b8eaa/observable/property.py#L102-L108", "repo": "observable", "func_name": "create_with", "original_string": ["    def create_with(\n", "            cls, event: str = None, observable: T.Union[str, Observable] = None\n", "    ) -> T.Callable[..., \"ObservableProperty\"]:\n", "        \"\"\"Creates a partial application of ObservableProperty with\n", "        event and observable preset.\"\"\"\n", "\n", "        return functools.partial(cls, event=event, observable=observable)\n"], "language": "python", "code": "def create_with(cls, event: str=None, observable: T.Union[str, Observable]=None\n    ) ->T.Callable[..., 'ObservableProperty']:\n    \"\"\"\"\"\"\n    return functools.partial(cls, event=event, observable=observable)\n", "code_tokens": ["create", "with", "cls", "event", "str", "none", "observable", "union", "str", "observable", "none", "callable", "observableproperty", "return", "functools", "partial", "cls", "event", "event", "observable", "observable"], "docstring": "get current observable value", "docstring_tokens": ["get", "current", "observable", "value"], "idx": 221}
{"url": "https://github.com/jor-/util/blob/0eb0be84430f88885f4d48335596ca8881f85587/util/observable/decorator.py#L114-L143", "repo": "util", "func_name": "_set_observable", "original_string": ["    def _set_observable(self, observable_name, new_value):\n", "        \n", "        # check old value\n", "        if self._has_value(observable_name):\n", "            old_value = self.new_value(observable_name)\n", "            \n", "            # set only if different value\n", "            must_set = np.any(new_value != old_value)\n", "            if must_set:\n", "                \n", "                # if values are observable_names with observers call associated observers of sub observable_names\n", "                if isinstance(old_value, Observable) and isinstance(new_value, Observable):\n", "                    # copy observer\n", "                    old_value.copy_observers_to(new_value)\n", "                    \n", "                    # notify old observer\n", "                    old_value._notify_observers(observable_name=None, include_everything_observers=True)\n", "                    for observable_name in old_value._observers.keys():\n", "                        old_has_value = old_value._has_value(observable_name)\n", "                        new_has_value = new_value._has_value(observable_name)\n", "                        if old_has_value != new_has_value or (old_has_value and new_has_value and np.any(old_value._get_value(observable_name) != new_value._get_value(observable_name))):\n", "                            old_value._notify_observers(observable_name=observable_name, include_everything_observers=False)\n", "        else:\n", "            must_set = True\n", "        \n", "            \n", "        # set new observable_name value and call observer\n", "        if must_set:\n", "            self._set_value(observable_name, new_value)\n", "            self._notify_observers(observable_name)\n"], "language": "python", "code": "def _set_observable(self, observable_name, new_value):\n    if self._has_value(observable_name):\n        old_value = self.new_value(observable_name)\n        must_set = np.any(new_value != old_value)\n        if must_set:\n            if isinstance(old_value, Observable) and isinstance(new_value,\n                Observable):\n                old_value.copy_observers_to(new_value)\n                old_value._notify_observers(observable_name=None,\n                    include_everything_observers=True)\n                for observable_name in old_value._observers.keys():\n                    old_has_value = old_value._has_value(observable_name)\n                    new_has_value = new_value._has_value(observable_name)\n                    if (old_has_value != new_has_value or old_has_value and\n                        new_has_value and np.any(old_value._get_value(\n                        observable_name) != new_value._get_value(\n                        observable_name))):\n                        old_value._notify_observers(observable_name=\n                            observable_name, include_everything_observers=False\n                            )\n    else:\n        must_set = True\n    if must_set:\n        self._set_value(observable_name, new_value)\n        self._notify_observers(observable_name)\n", "code_tokens": ["set", "observable", "self", "observable", "name", "new", "value", "if", "self", "has", "value", "observable", "name", "old", "value", "self", "new", "value", "observable", "name", "must", "set", "np", "any", "new", "value", "old", "value", "if", "must", "set", "if", "isinstance", "old", "value", "observable", "and", "isinstance", "new", "value", "observable", "old", "value", "copy", "observers", "to", "new", "value", "old", "value", "notify", "observers", "observable", "name", "none", "include", "everything", "observers", "true", "for", "observable", "name", "in", "old", "value", "observers", "keys", "old", "has", "value", "old", "value", "has", "value", "observable", "name", "new", "has", "value", "new", "value", "has", "value", "observable", "name", "if", "old", "has", "value", "new", "has", "value", "or", "old", "has", "value", "and", "new", "has", "value", "and", "np", "any", "old", "value", "get", "value", "observable", "name", "new", "value", "get", "value", "observable", "name", "old", "value", "notify", "observers", "observable", "name", "observable", "name", "include", "everything", "observers", "false", "else", "must", "set", "true", "if", "must", "set", "self", "set", "value", "observable", "name", "new", "value", "self", "notify", "observers", "observable", "name"], "docstring": "get current observable value", "docstring_tokens": ["get", "current", "observable", "value"], "idx": 222}
{"url": "https://github.com/timofurrer/observable/blob/a6a764efaf9408a334bdb1ddf4327d9dbc4b8eaa/observable/property.py#L70-L95", "repo": "observable", "func_name": "_trigger_event", "original_string": ["    def _trigger_event(\n", "            self, holder: T.Any, alt_name: str, action: str, *event_args: T.Any\n", "    ) -> None:\n", "        \"\"\"Triggers an event on the associated Observable object.\n", "        The Holder is the object this property is a member of, alt_name\n", "        is used as the event name when self.event is not set, action is\n", "        prepended to the event name and event_args are passed through\n", "        to the registered event handlers.\"\"\"\n", "\n", "        if isinstance(self.observable, Observable):\n", "            observable = self.observable\n", "        elif isinstance(self.observable, str):\n", "            observable = getattr(holder, self.observable)\n", "        elif isinstance(holder, Observable):\n", "            observable = holder\n", "        else:\n", "            raise TypeError(\n", "                \"This ObservableProperty is no member of an Observable \"\n", "                \"object. Specify where to find the Observable object for \"\n", "                \"triggering events with the observable keyword argument \"\n", "                \"when initializing the ObservableProperty.\"\n", "            )\n", "\n", "        name = alt_name if self.event is None else self.event\n", "        event = \"{}_{}\".format(action, name)\n", "        observable.trigger(event, *event_args)\n"], "language": "python", "code": "def _trigger_event(self, holder: T.Any, alt_name: str, action: str, *\n    event_args: T.Any) ->None:\n    \"\"\"\"\"\"\n    if isinstance(self.observable, Observable):\n        observable = self.observable\n    elif isinstance(self.observable, str):\n        observable = getattr(holder, self.observable)\n    elif isinstance(holder, Observable):\n        observable = holder\n    else:\n        raise TypeError(\n            'This ObservableProperty is no member of an Observable object. Specify where to find the Observable object for triggering events with the observable keyword argument when initializing the ObservableProperty.'\n            )\n    name = alt_name if self.event is None else self.event\n    event = '{}_{}'.format(action, name)\n    observable.trigger(event, *event_args)\n", "code_tokens": ["trigger", "event", "self", "holder", "any", "alt", "name", "str", "action", "str", "event", "args", "any", "none", "if", "isinstance", "self", "observable", "observable", "observable", "self", "observable", "elif", "isinstance", "self", "observable", "str", "observable", "getattr", "holder", "self", "observable", "elif", "isinstance", "holder", "observable", "observable", "holder", "else", "raise", "typeerror", "this", "observableproperty", "is", "no", "member", "of", "an", "observable", "object", "specify", "where", "to", "find", "the", "observable", "object", "for", "triggering", "events", "with", "the", "observable", "keyword", "argument", "when", "initializing", "the", "observableproperty", "name", "alt", "name", "if", "self", "event", "is", "none", "else", "self", "event", "event", "format", "action", "name", "observable", "trigger", "event", "event", "args"], "docstring": "get current observable value", "docstring_tokens": ["get", "current", "observable", "value"], "idx": 223}
{"url": "https://github.com/Nekmo/simple-monitor-alert/blob/11d6dbd3c0b3b9a210d6435208066f5636f1f44e/monitors/web.py#L91-L101", "repo": "simple-monitor-alert", "func_name": "_sma_observable", "original_string": ["def _sma_observable(observable_name, expected, function=None, param=None, value=None):\n", "    param = param or os.environ.get(observable_name)\n", "    if not param:\n", "        return\n", "    groups = parse_param(param) if not isinstance(param, list) else param\n", "    function = function or globals()['test_' + observable_name]\n", "    value = value or function(*groups)\n", "    name = function.__name__.replace('test_', '').replace('_', ' ').capitalize()\n", "    print('{}.name = \"{} {}\"'.format(observable_name, name, groups[1]))\n", "    print('{}.value = {}'.format(observable_name, value))\n", "    print('{}.expected = {}'.format(observable_name, expected))\n"], "language": "python", "code": "def _sma_observable(observable_name, expected, function=None, param=None,\n    value=None):\n    param = param or os.environ.get(observable_name)\n    if not param:\n        return\n    groups = parse_param(param) if not isinstance(param, list) else param\n    function = function or globals()['test_' + observable_name]\n    value = value or function(*groups)\n    name = function.__name__.replace('test_', '').replace('_', ' ').capitalize(\n        )\n    print('{}.name = \"{} {}\"'.format(observable_name, name, groups[1]))\n    print('{}.value = {}'.format(observable_name, value))\n    print('{}.expected = {}'.format(observable_name, expected))\n", "code_tokens": ["sma", "observable", "observable", "name", "expected", "function", "none", "param", "none", "value", "none", "param", "param", "or", "os", "environ", "get", "observable", "name", "if", "not", "param", "return", "groups", "parse", "param", "param", "if", "not", "isinstance", "param", "list", "else", "param", "function", "function", "or", "globals", "test", "observable", "name", "value", "value", "or", "function", "groups", "name", "function", "name", "replace", "test", "replace", "capitalize", "print", "name", "format", "observable", "name", "name", "groups", "1", "print", "value", "format", "observable", "name", "value", "print", "expected", "format", "observable", "name", "expected"], "docstring": "get current observable value", "docstring_tokens": ["get", "current", "observable", "value"], "idx": 224}
{"url": "https://github.com/ARMmbed/mbed-cloud-sdk-python/blob/c0af86fb2cdd4dc7ed26f236139241067d293509/examples/connect/set-resource.py#L23-L42", "repo": "mbed-cloud-sdk-python", "func_name": "_main", "original_string": ["def _main():\n", "    api = ConnectAPI()\n", "    # calling start_notifications is required for getting/setting resource synchronously\n", "    api.start_notifications()\n", "    devices = api.list_connected_devices().data\n", "    if not devices:\n", "        raise Exception(\"No connected devices registered. Aborting\")\n", "\n", "    # Synchronously get the initial/current value of the resource\n", "    value = api.get_resource_value(devices[0].id, WRITEABLE_RESOURCE)\n", "    print(\"Current value: %r\" % (value,))\n", "\n", "    # Set Resource value. Resource needs to have type == \"writable_resource\"\n", "    api.set_resource_value(device_id=devices[0].id,\n", "                           resource_path=WRITEABLE_RESOURCE,\n", "                           resource_value='10')\n", "\n", "    # Synchronously get the current value of the resource\n", "    value = api.get_resource_value(devices[0].id, WRITEABLE_RESOURCE)\n", "    print(\"Current value: %r\" % (value,))\n"], "language": "python", "code": "def _main():\n    api = ConnectAPI()\n    api.start_notifications()\n    devices = api.list_connected_devices().data\n    if not devices:\n        raise Exception('No connected devices registered. Aborting')\n    value = api.get_resource_value(devices[0].id, WRITEABLE_RESOURCE)\n    print('Current value: %r' % (value,))\n    api.set_resource_value(device_id=devices[0].id, resource_path=\n        WRITEABLE_RESOURCE, resource_value='10')\n    value = api.get_resource_value(devices[0].id, WRITEABLE_RESOURCE)\n    print('Current value: %r' % (value,))\n", "code_tokens": ["main", "api", "connectapi", "api", "start", "notifications", "devices", "api", "list", "connected", "devices", "data", "if", "not", "devices", "raise", "exception", "no", "connected", "devices", "registered", "aborting", "value", "api", "get", "resource", "value", "devices", "0", "id", "writeable", "resource", "print", "current", "value", "value", "api", "set", "resource", "value", "device", "id", "devices", "0", "id", "resource", "path", "writeable", "resource", "resource", "value", "10", "value", "api", "get", "resource", "value", "devices", "0", "id", "writeable", "resource", "print", "current", "value", "value"], "docstring": "get current observable value", "docstring_tokens": ["get", "current", "observable", "value"], "idx": 225}
{"url": "https://github.com/Nekmo/simple-monitor-alert/blob/11d6dbd3c0b3b9a210d6435208066f5636f1f44e/simple_monitor_alert/sma.py#L69-L75", "repo": "simple-monitor-alert", "func_name": "get_monitor_observables", "original_string": ["    def get_monitor_observables(self, name):\n", "        try:\n", "            lines = self.items(name)\n", "        except NoSectionError:\n", "            return []\n", "        lines = [ItemLine(key, value) for key, value in lines]\n", "        return get_observables_from_lines(lines)\n"], "language": "python", "code": "def get_monitor_observables(self, name):\n    try:\n        lines = self.items(name)\n    except NoSectionError:\n        return []\n    lines = [ItemLine(key, value) for key, value in lines]\n    return get_observables_from_lines(lines)\n", "code_tokens": ["get", "monitor", "observables", "self", "name", "try", "lines", "self", "items", "name", "except", "nosectionerror", "return", "lines", "itemline", "key", "value", "for", "key", "value", "in", "lines", "return", "get", "observables", "from", "lines", "lines"], "docstring": "get current observable value", "docstring_tokens": ["get", "current", "observable", "value"], "idx": 226}
{"url": "https://github.com/eddiejessup/spatious/blob/b7ae91bec029e85a45a7f303ee184076433723cd/spatious/vector.py#L185-L216", "repo": "spatious", "func_name": "sphere_pick_polar", "original_string": ["def sphere_pick_polar(d, n=1, rng=None):\n", "    \"\"\"Return vectors uniformly picked on the unit sphere.\n", "    Vectors are in a polar representation.\n", "\n", "    Parameters\n", "    ----------\n", "    d: float\n", "        The number of dimensions of the space in which the sphere lives.\n", "    n: integer\n", "        Number of samples to pick.\n", "\n", "    Returns\n", "    -------\n", "    r: array, shape (n, d)\n", "        Sample vectors.\n", "    \"\"\"\n", "    if rng is None:\n", "        rng = np.random\n", "    a = np.empty([n, d])\n", "    if d == 1:\n", "        a[:, 0] = rng.randint(2, size=n) * 2 - 1\n", "    elif d == 2:\n", "        a[:, 0] = 1.0\n", "        a[:, 1] = rng.uniform(-np.pi, +np.pi, n)\n", "    elif d == 3:\n", "        u, v = rng.uniform(0.0, 1.0, (2, n))\n", "        a[:, 0] = 1.0\n", "        a[:, 1] = np.arccos(2.0 * v - 1.0)\n", "        a[:, 2] = 2.0 * np.pi * u\n", "    else:\n", "        raise Exception('Invalid vector for polar representation')\n", "    return a\n"], "language": "python", "code": "def sphere_pick_polar(d, n=1, rng=None):\n    \"\"\"\"\"\"\n    if rng is None:\n        rng = np.random\n    a = np.empty([n, d])\n    if d == 1:\n        a[:, 0] = rng.randint(2, size=n) * 2 - 1\n    elif d == 2:\n        a[:, 0] = 1.0\n        a[:, 1] = rng.uniform(-np.pi, +np.pi, n)\n    elif d == 3:\n        u, v = rng.uniform(0.0, 1.0, (2, n))\n        a[:, 0] = 1.0\n        a[:, 1] = np.arccos(2.0 * v - 1.0)\n        a[:, 2] = 2.0 * np.pi * u\n    else:\n        raise Exception('Invalid vector for polar representation')\n    return a\n", "code_tokens": ["sphere", "pick", "polar", "1", "rng", "none", "if", "rng", "is", "none", "rng", "np", "random", "np", "empty", "if", "1", "0", "rng", "randint", "2", "size", "2", "1", "elif", "2", "0", "1", "0", "1", "rng", "uniform", "np", "pi", "np", "pi", "elif", "3", "rng", "uniform", "0", "0", "1", "0", "2", "0", "1", "0", "1", "np", "arccos", "2", "0", "1", "0", "2", "2", "0", "np", "pi", "else", "raise", "exception", "invalid", "vector", "for", "polar", "representation", "return"], "docstring": "how to randomly pick a number", "docstring_tokens": ["how", "to", "randomly", "pick", "a", "number"], "idx": 227}
{"url": "https://github.com/HacKanCuBa/passphrase-py/blob/219d6374338ed9a1475b4f09b0d85212376f11e0/passphrase/passphrase.py#L71-L76", "repo": "passphrase-py", "func_name": "randnum_min", "original_string": ["    def randnum_min(self, randnum: int) -> None:\n", "        if not isinstance(randnum, int):\n", "            raise TypeError('randnum_min can only be int')\n", "        if randnum < 0:\n", "            raise ValueError('randnum_min should be greater than 0')\n", "        self._randnum_min = randnum\n"], "language": "python", "code": "def randnum_min(self, randnum: int) ->None:\n    if not isinstance(randnum, int):\n        raise TypeError('randnum_min can only be int')\n    if randnum < 0:\n        raise ValueError('randnum_min should be greater than 0')\n    self._randnum_min = randnum\n", "code_tokens": ["randnum", "min", "self", "randnum", "int", "none", "if", "not", "isinstance", "randnum", "int", "raise", "typeerror", "randnum", "min", "can", "only", "be", "int", "if", "randnum", "0", "raise", "valueerror", "randnum", "min", "should", "be", "greater", "than", "0", "self", "randnum", "min", "randnum"], "docstring": "how to randomly pick a number", "docstring_tokens": ["how", "to", "randomly", "pick", "a", "number"], "idx": 228}
{"url": "https://github.com/bwesterb/sarah/blob/a9e46e875dfff1dc11255d714bb736e5eb697809/src/drv.py#L62-L67", "repo": "sarah", "func_name": "pick", "original_string": ["    def pick(self):\n", "        while True:\n", "            idx = random.randint(0, len(self.values) - 1)\n", "            v, p = self.values[idx]\n", "            if p >= random.uniform(0, 1):\n", "                return v\n"], "language": "python", "code": "def pick(self):\n    while True:\n        idx = random.randint(0, len(self.values) - 1)\n        v, p = self.values[idx]\n        if p >= random.uniform(0, 1):\n            return v\n", "code_tokens": ["pick", "self", "while", "true", "idx", "random", "randint", "0", "len", "self", "values", "1", "self", "values", "idx", "if", "random", "uniform", "0", "1", "return"], "docstring": "how to randomly pick a number", "docstring_tokens": ["how", "to", "randomly", "pick", "a", "number"], "idx": 229}
{"url": "https://github.com/HacKanCuBa/passphrase-py/blob/219d6374338ed9a1475b4f09b0d85212376f11e0/passphrase/secrets.py#L63-L84", "repo": "passphrase-py", "func_name": "randbelow", "original_string": ["def randbelow(num: int) -> int:\n", "    \"\"\"Return a random int in the range [0,num).\n", "\n", "    Raises ValueError if num <= 0, and TypeError if it's not an integer.\n", "\n", "    >>> randbelow(16)  #doctest:+SKIP\n", "    13\n", "\n", "    \"\"\"\n", "    if not isinstance(num, int):\n", "        raise TypeError('number must be an integer')\n", "    if num <= 0:\n", "        raise ValueError('number must be greater than zero')\n", "    if num == 1:\n", "        return 0\n", "\n", "    # https://github.com/python/cpython/blob/3.6/Lib/random.py#L223\n", "    nbits = num.bit_length()    # don't use (n-1) here because n can be 1\n", "    randnum = random_randint(nbits)    # 0 <= randnum < 2**nbits\n", "    while randnum >= num:\n", "        randnum = random_randint(nbits)\n", "    return randnum\n"], "language": "python", "code": "def randbelow(num: int) ->int:\n    \"\"\"\"\"\"\n    if not isinstance(num, int):\n        raise TypeError('number must be an integer')\n    if num <= 0:\n        raise ValueError('number must be greater than zero')\n    if num == 1:\n        return 0\n    nbits = num.bit_length()\n    randnum = random_randint(nbits)\n    while randnum >= num:\n        randnum = random_randint(nbits)\n    return randnum\n", "code_tokens": ["randbelow", "num", "int", "int", "if", "not", "isinstance", "num", "int", "raise", "typeerror", "number", "must", "be", "an", "integer", "if", "num", "0", "raise", "valueerror", "number", "must", "be", "greater", "than", "zero", "if", "num", "1", "return", "0", "nbits", "num", "bit", "length", "randnum", "random", "randint", "nbits", "while", "randnum", "num", "randnum", "random", "randint", "nbits", "return", "randnum"], "docstring": "how to randomly pick a number", "docstring_tokens": ["how", "to", "randomly", "pick", "a", "number"], "idx": 230}
{"url": "https://github.com/alvarogzp/telegram-bot-framework/blob/7b597a415c1901901c677976cb13100fc3083107/bot/action/extra/random.py#L35-L47", "repo": "telegram-bot-framework", "func_name": "get_help", "original_string": ["    def get_help(event):\n", "        args = [\n", "            \"\",\n", "            \"start end\",\n", "            \"option1 {line-break} option2 [{line-break} option 3 ...]\"\n", "        ]\n", "        description = (\n", "            \"Without arguments, display a random float number in the range \\[0, 1).\\n\\n\"\n", "            \"Add two integers separated by a space to get a random number in that range,\"\n", "            \" including both endpoints: \\[start, end].\\n\\n\"\n", "            \"Put various options, each one in a different line to get one chosen randomly.\"\n", "        )\n", "        return CommandUsageMessage.get_usage_message(event.command, args, description)\n"], "language": "python", "code": "def get_help(event):\n    args = ['', 'start end',\n        'option1 {line-break} option2 [{line-break} option 3 ...]']\n    description = \"\"\"Without arguments, display a random float number in the range \\\\[0, 1).\n\nAdd two integers separated by a space to get a random number in that range, including both endpoints: \\\\[start, end].\n\nPut various options, each one in a different line to get one chosen randomly.\"\"\"\n    return CommandUsageMessage.get_usage_message(event.command, args,\n        description)\n", "code_tokens": ["get", "help", "event", "args", "start", "end", "line", "break", "line", "break", "option", "3", "description", "without", "arguments", "display", "random", "float", "number", "in", "the", "range", "0", "1", "add", "two", "integers", "separated", "by", "space", "to", "get", "random", "number", "in", "that", "range", "including", "both", "endpoints", "start", "end", "put", "various", "options", "each", "one", "in", "different", "line", "to", "get", "one", "chosen", "randomly", "return", "commandusagemessage", "get", "usage", "message", "event", "command", "args", "description"], "docstring": "how to randomly pick a number", "docstring_tokens": ["how", "to", "randomly", "pick", "a", "number"], "idx": 231}
{"url": "https://github.com/bwesterb/sarah/blob/a9e46e875dfff1dc11255d714bb736e5eb697809/src/drv.py#L24-L47", "repo": "sarah", "func_name": "pick", "original_string": ["    def pick(self):\n", "        \"\"\" picks a value accoriding to the given density \"\"\"\n", "        v = random.uniform(0, self.ub)\n", "        d = self.dist\n", "        c = self.vc - 1\n", "        s = self.vc\n", "        while True:\n", "            s = s / 2\n", "            if s == 0:\n", "                break\n", "            if v <= d[c][1]:\n", "                c -= s\n", "            else:\n", "                c += s\n", "                # we only need this logic when increasing c\n", "                while len(d) <= c:\n", "                    s = s / 2\n", "                    c -= s\n", "                    if s == 0:\n", "                        break\n", "        # we may have converged from the left, instead of the right\n", "        if c == len(d) or v <= d[c][1]:\n", "            c -= 1\n", "        return d[c][0]\n"], "language": "python", "code": "def pick(self):\n    \"\"\"\"\"\"\n    v = random.uniform(0, self.ub)\n    d = self.dist\n    c = self.vc - 1\n    s = self.vc\n    while True:\n        s = s / 2\n        if s == 0:\n            break\n        if v <= d[c][1]:\n            c -= s\n        else:\n            c += s\n            while len(d) <= c:\n                s = s / 2\n                c -= s\n                if s == 0:\n                    break\n    if c == len(d) or v <= d[c][1]:\n        c -= 1\n    return d[c][0]\n", "code_tokens": ["pick", "self", "random", "uniform", "0", "self", "ub", "self", "dist", "self", "vc", "1", "self", "vc", "while", "true", "2", "if", "0", "break", "if", "1", "else", "while", "len", "2", "if", "0", "break", "if", "len", "or", "1", "1", "return", "0"], "docstring": "how to randomly pick a number", "docstring_tokens": ["how", "to", "randomly", "pick", "a", "number"], "idx": 232}
{"url": "https://github.com/ManiacalLabs/BiblioPixelAnimations/blob/fba81f6b94f5265272a53f462ef013df1ccdb426/BiblioPixelAnimations/strip/WhiteTwinkle.py#L68-L77", "repo": "BiblioPixelAnimations", "func_name": "pick_led", "original_string": ["    def pick_led(self, inc):\n", "        # Pick a random led, if it's off bump it up an even number so it gets brighter\n", "        idx = random.randrange(0, self.layout.numLEDs)\n", "        this_led = self.layout.get(idx)\n", "        r = this_led[0]\n", "\n", "        if random.randrange(0, self._maxLed) < self.density:\n", "            if r == 0:\n", "                r += inc\n", "                self.layout.set(idx, (2, 2, 2))\n"], "language": "python", "code": "def pick_led(self, inc):\n    idx = random.randrange(0, self.layout.numLEDs)\n    this_led = self.layout.get(idx)\n    r = this_led[0]\n    if random.randrange(0, self._maxLed) < self.density:\n        if r == 0:\n            r += inc\n            self.layout.set(idx, (2, 2, 2))\n", "code_tokens": ["pick", "led", "self", "inc", "idx", "random", "randrange", "0", "self", "layout", "numleds", "this", "led", "self", "layout", "get", "idx", "this", "led", "0", "if", "random", "randrange", "0", "self", "maxled", "self", "density", "if", "0", "inc", "self", "layout", "set", "idx", "2", "2", "2"], "docstring": "how to randomly pick a number", "docstring_tokens": ["how", "to", "randomly", "pick", "a", "number"], "idx": 233}
{"url": "https://github.com/hcpl/xkbgroup/blob/fcf4709a3c8221e0cdf62c09e5cccda232b0104c/xkbgroup/core.py#L242-L258", "repo": "xkbgroup", "func_name": "groups_count", "original_string": ["    def groups_count(self):\n", "        \"\"\"Number of all groups (get-only).\n", "\n", "        :getter: Returns number of all groups\n", "        :type: int\n", "        \"\"\"\n", "        if self._keyboard_description.contents.ctrls is not None:\n", "            return self._keyboard_description.contents.ctrls.contents.num_groups\n", "        else:\n", "            groups_source = self._groups_source\n", "\n", "            groups_count = 0\n", "            while (groups_count < XkbNumKbdGroups and\n", "                   groups_source[groups_count] != None_):\n", "                groups_count += 1\n", "\n", "            return groups_count\n"], "language": "python", "code": "def groups_count(self):\n    \"\"\"\"\"\"\n    if self._keyboard_description.contents.ctrls is not None:\n        return self._keyboard_description.contents.ctrls.contents.num_groups\n    else:\n        groups_source = self._groups_source\n        groups_count = 0\n        while groups_count < XkbNumKbdGroups and groups_source[groups_count\n            ] != None_:\n            groups_count += 1\n        return groups_count\n", "code_tokens": ["groups", "count", "self", "if", "self", "keyboard", "description", "contents", "ctrls", "is", "not", "none", "return", "self", "keyboard", "description", "contents", "ctrls", "contents", "num", "groups", "else", "groups", "source", "self", "groups", "source", "groups", "count", "0", "while", "groups", "count", "xkbnumkbdgroups", "and", "groups", "source", "groups", "count", "none", "groups", "count", "1", "return", "groups", "count"], "docstring": "group by count", "docstring_tokens": ["group", "by", "count"], "idx": 234}
{"url": "https://github.com/shazow/unstdlib.py/blob/e0632fe165cfbfdb5a7e4bc7b412c9d6f2ebad83/unstdlib/standard/list_.py#L16-L36", "repo": "unstdlib.py", "func_name": "groupby_count", "original_string": ["def groupby_count(i, key=None, force_keys=None):\n", "    \"\"\" Aggregate iterator values into buckets based on how frequently the\n", "    values appear.\n", "\n", "    Example::\n", "\n", "        >>> list(groupby_count([1, 1, 1, 2, 3]))\n", "        [(1, 3), (2, 1), (3, 1)]\n", "    \"\"\"\n", "    counter = defaultdict(lambda: 0)\n", "    if not key:\n", "        key = lambda o: o\n", "\n", "    for k in i:\n", "        counter[key(k)] += 1\n", "\n", "    if force_keys:\n", "        for k in force_keys:\n", "            counter[k] += 0\n", "\n", "    return counter.items()\n"], "language": "python", "code": "def groupby_count(i, key=None, force_keys=None):\n    \"\"\"\"\"\"\n    counter = defaultdict(lambda : 0)\n    if not key:\n        key = lambda o: o\n    for k in i:\n        counter[key(k)] += 1\n    if force_keys:\n        for k in force_keys:\n            counter[k] += 0\n    return counter.items()\n", "code_tokens": ["groupby", "count", "key", "none", "force", "keys", "none", "counter", "defaultdict", "lambda", "0", "if", "not", "key", "key", "lambda", "for", "in", "counter", "key", "1", "if", "force", "keys", "for", "in", "force", "keys", "counter", "0", "return", "counter", "items"], "docstring": "group by count", "docstring_tokens": ["group", "by", "count"], "idx": 235}
{"url": "https://github.com/adamjaso/pyauto/blob/b11da69fb21a49241f5ad75dac48d9d369c6279b/ouidb/pyauto/ouidb/config.py#L43-L50", "repo": "pyauto", "func_name": "get_top_entries", "original_string": ["    def get_top_entries(self):\n", "        query = 'select count(vendor_name) count, vendor_name from mac_vendor group by vendor_name having count > 10 order by count;'\n", "        try:\n", "            db = self.get_db()\n", "            results = db.execute(query)\n", "            return [i for i in results]\n", "        finally:\n", "            db.close()\n"], "language": "python", "code": "def get_top_entries(self):\n    query = (\n        'select count(vendor_name) count, vendor_name from mac_vendor group by vendor_name having count > 10 order by count;'\n        )\n    try:\n        db = self.get_db()\n        results = db.execute(query)\n        return [i for i in results]\n    finally:\n        db.close()\n", "code_tokens": ["get", "top", "entries", "self", "query", "select", "count", "vendor", "name", "count", "vendor", "name", "from", "mac", "vendor", "group", "by", "vendor", "name", "having", "count", "10", "order", "by", "count", "try", "db", "self", "get", "db", "results", "db", "execute", "query", "return", "for", "in", "results", "finally", "db", "close"], "docstring": "group by count", "docstring_tokens": ["group", "by", "count"], "idx": 236}
{"url": "https://github.com/rueckstiess/mtools/blob/a6a22910c3569c0c8a3908660ca218a4557e4249/mtools/mplotqueries/plottypes/base_type.py#L79-L91", "repo": "mtools", "func_name": "group", "original_string": ["    def group(self):\n", "        \"\"\"(re-)group all logevents by the given group.\"\"\"\n", "        if hasattr(self, 'group_by'):\n", "            group_by = self.group_by\n", "        else:\n", "            group_by = self.default_group_by\n", "            if self.args['group'] is not None:\n", "                group_by = self.args['group']\n", "\n", "        self.groups = Grouping(self.logevents, group_by)\n", "        self.groups.move_items(None, 'others')\n", "        self.groups.sort_by_size(group_limit=self.args['group_limit'],\n", "                                 discard_others=self.args['no_others'])\n"], "language": "python", "code": "def group(self):\n    \"\"\"\"\"\"\n    if hasattr(self, 'group_by'):\n        group_by = self.group_by\n    else:\n        group_by = self.default_group_by\n        if self.args['group'] is not None:\n            group_by = self.args['group']\n    self.groups = Grouping(self.logevents, group_by)\n    self.groups.move_items(None, 'others')\n    self.groups.sort_by_size(group_limit=self.args['group_limit'],\n        discard_others=self.args['no_others'])\n", "code_tokens": ["group", "self", "if", "hasattr", "self", "group", "by", "group", "by", "self", "group", "by", "else", "group", "by", "self", "default", "group", "by", "if", "self", "args", "group", "is", "not", "none", "group", "by", "self", "args", "group", "self", "groups", "grouping", "self", "logevents", "group", "by", "self", "groups", "move", "items", "none", "others", "self", "groups", "sort", "by", "size", "group", "limit", "self", "args", "group", "limit", "discard", "others", "self", "args", "no", "others"], "docstring": "group by count", "docstring_tokens": ["group", "by", "count"], "idx": 237}
{"url": "https://github.com/codeforamerica/three/blob/67b4a4b233a57aa7995d01f6b0f69c2e85aea6c0/three/core.py#L158-L170", "repo": "three", "func_name": "convert", "original_string": ["    def convert(self, content, conversion):\n", "        \"\"\"Convert content to Python data structures.\"\"\"\n", "        if not conversion:\n", "            data = content\n", "        elif self.format == 'json':\n", "            data = json.loads(content)\n", "        elif self.format == 'xml':\n", "            content = xml(content)\n", "            first = list(content.keys())[0]\n", "            data = content[first]\n", "        else:\n", "            data = content\n", "        return data\n"], "language": "python", "code": "def convert(self, content, conversion):\n    \"\"\"\"\"\"\n    if not conversion:\n        data = content\n    elif self.format == 'json':\n        data = json.loads(content)\n    elif self.format == 'xml':\n        content = xml(content)\n        first = list(content.keys())[0]\n        data = content[first]\n    else:\n        data = content\n    return data\n", "code_tokens": ["convert", "self", "content", "conversion", "if", "not", "conversion", "data", "content", "elif", "self", "format", "json", "data", "json", "loads", "content", "elif", "self", "format", "xml", "content", "xml", "content", "first", "list", "content", "keys", "0", "data", "content", "first", "else", "data", "content", "return", "data"], "docstring": "json to xml conversion", "docstring_tokens": ["json", "to", "xml", "conversion"], "idx": 238}
{"url": "https://github.com/open511/open511/blob/3d573f59d7efa06ff1b5419ea5ff4d90a90b3cf8/open511/converter/tmdd.py#L9-L15", "repo": "open511", "func_name": "tmdd_to_json", "original_string": ["def tmdd_to_json(doc):\n", "    converters = TMDDEventConverter.list_from_document(doc)\n", "    events = [converter.to_json() for converter in converters]\n", "    return {\n", "        \"meta\": dict(version='v1'),\n", "        \"events\": events\n", "    }\n"], "language": "python", "code": "def tmdd_to_json(doc):\n    converters = TMDDEventConverter.list_from_document(doc)\n    events = [converter.to_json() for converter in converters]\n    return {'meta': dict(version='v1'), 'events': events}\n", "code_tokens": ["tmdd", "to", "json", "doc", "converters", "tmddeventconverter", "list", "from", "document", "doc", "events", "converter", "to", "json", "for", "converter", "in", "converters", "return", "meta", "dict", "version", "events", "events"], "docstring": "json to xml conversion", "docstring_tokens": ["json", "to", "xml", "conversion"], "idx": 239}
{"url": "https://github.com/jfear/sramongo/blob/82a9a157e44bda4100be385c644b3ac21be66038/sramongo/xml_helpers.py#L75-L95", "repo": "sramongo", "func_name": "xml_to_root", "original_string": ["\n", "def xml_to_root(xml: Union[str, IO]) -> ElementTree.Element:\n", "    \"\"\"Parse XML into an ElemeTree object.\n", "\n", "    Parameters\n", "    ----------\n", "    xml : str or file-like object\n", "        A filename, file object or string version of xml can be passed.\n", "\n", "    Returns\n", "    -------\n", "    Elementree.Element\n", "\n", "    \"\"\"\n", "    if isinstance(xml, str):\n", "        if \"<\" in xml:\n", "            return ElementTree.fromstring(xml)\n", "        else:\n", "            with open(xml) as fh:\n", "                xml_to_root(fh)\n", "    tree = ElementTree.parse(xml)\n"], "language": "python", "code": "def xml_to_root(xml: Union[str, IO]) ->ElementTree.Element:\n    \"\"\"\"\"\"\n    if isinstance(xml, str):\n        if '<' in xml:\n            return ElementTree.fromstring(xml)\n        else:\n            with open(xml) as fh:\n                xml_to_root(fh)\n    tree = ElementTree.parse(xml)\n", "code_tokens": ["xml", "to", "root", "xml", "union", "str", "io", "elementtree", "element", "if", "isinstance", "xml", "str", "if", "in", "xml", "return", "elementtree", "fromstring", "xml", "else", "with", "open", "xml", "as", "fh", "xml", "to", "root", "fh", "tree", "elementtree", "parse", "xml"], "docstring": "json to xml conversion", "docstring_tokens": ["json", "to", "xml", "conversion"], "idx": 240}
{"url": "https://github.com/erinxocon/requests-xml/blob/923571ceae4ddd4f2f57a2fc8780d89b50f3e7a1/requests_xml.py#L177-L203", "repo": "requests-xml", "func_name": "json", "original_string": ["    def json(self, conversion: _Text = 'badgerfish') -> Mapping:\n", "        \"\"\"A JSON Representation of the XML.  Default is badgerfish.\n", "        :param conversion: Which conversion method to use. (`learn more <https://github.com/sanand0/xmljson#conventions>`_)\n", "        \"\"\"\n", "        if not self._json:\n", "\n", "            if conversion is 'badgerfish':\n", "                from xmljson import badgerfish as serializer\n", "\n", "            elif conversion is 'abdera':\n", "                from xmljson import abdera as serializer\n", "\n", "            elif conversion is 'cobra':\n", "                from xmljson import cobra as serializer\n", "\n", "            elif conversion is 'gdata':\n", "                from xmljson import gdata as serializer\n", "\n", "            elif conversion is 'parker':\n", "                from xmljson import parker as serializer\n", "\n", "            elif conversion is 'yahoo':\n", "                from xmljson import yahoo as serializer\n", "\n", "            self._json = json.dumps(serializer.data(etree.fromstring(self.xml)))\n", "\n", "        return self._json\n"], "language": "python", "code": "def json(self, conversion: _Text='badgerfish') ->Mapping:\n    \"\"\"\"\"\"\n    if not self._json:\n        if conversion is 'badgerfish':\n            from xmljson import badgerfish as serializer\n        elif conversion is 'abdera':\n            from xmljson import abdera as serializer\n        elif conversion is 'cobra':\n            from xmljson import cobra as serializer\n        elif conversion is 'gdata':\n            from xmljson import gdata as serializer\n        elif conversion is 'parker':\n            from xmljson import parker as serializer\n        elif conversion is 'yahoo':\n            from xmljson import yahoo as serializer\n        self._json = json.dumps(serializer.data(etree.fromstring(self.xml)))\n    return self._json\n", "code_tokens": ["json", "self", "conversion", "text", "badgerfish", "mapping", "if", "not", "self", "json", "if", "conversion", "is", "badgerfish", "from", "xmljson", "import", "badgerfish", "as", "serializer", "elif", "conversion", "is", "abdera", "from", "xmljson", "import", "abdera", "as", "serializer", "elif", "conversion", "is", "cobra", "from", "xmljson", "import", "cobra", "as", "serializer", "elif", "conversion", "is", "gdata", "from", "xmljson", "import", "gdata", "as", "serializer", "elif", "conversion", "is", "parker", "from", "xmljson", "import", "parker", "as", "serializer", "elif", "conversion", "is", "yahoo", "from", "xmljson", "import", "yahoo", "as", "serializer", "self", "json", "json", "dumps", "serializer", "data", "etree", "fromstring", "self", "xml", "return", "self", "json"], "docstring": "json to xml conversion", "docstring_tokens": ["json", "to", "xml", "conversion"], "idx": 241}
{"url": "https://github.com/amaas-fintech/amaas-core-sdk-python/blob/347b71f8e776b2dde582b015e31b4802d91e8040/amaascore/core/amaas_model.py#L30-L31", "repo": "amaas-core-sdk-python", "func_name": "to_json_string", "original_string": ["def to_json_string(dict_to_convert):\n", "    return json.dumps(dict_to_convert, ensure_ascii=False, default=json_handler, indent=4, separators=(',', ': '))\n"], "language": "python", "code": "def to_json_string(dict_to_convert):\n    return json.dumps(dict_to_convert, ensure_ascii=False, default=\n        json_handler, indent=4, separators=(',', ': '))\n", "code_tokens": ["to", "json", "string", "dict", "to", "convert", "return", "json", "dumps", "dict", "to", "convert", "ensure", "ascii", "false", "default", "json", "handler", "indent", "4", "separators"], "docstring": "json to xml conversion", "docstring_tokens": ["json", "to", "xml", "conversion"], "idx": 242}
{"url": "https://github.com/open511/open511/blob/3d573f59d7efa06ff1b5419ea5ff4d90a90b3cf8/open511/converter/o5json.py#L9-L63", "repo": "open511", "func_name": "xml_to_json", "original_string": ["def xml_to_json(root):\n", "    \"\"\"Convert an Open511 XML document or document fragment to JSON.\n", "\n", "    Takes an lxml Element object. Returns a dict ready to be JSON-serialized.\"\"\"\n", "    j = {}\n", "\n", "    if len(root) == 0:  # Tag with no children, return str/int\n", "        return _maybe_intify(root.text)\n", "\n", "    if len(root) == 1 and root[0].tag.startswith('{' + NS_GML):  # GML\n", "        return gml_to_geojson(root[0])\n", "\n", "    if root.tag == 'open511':\n", "        j['meta'] = {'version': root.get('version')}\n", "\n", "    for elem in root:\n", "        name = elem.tag\n", "        if name == 'link' and elem.get('rel'):\n", "            name = elem.get('rel') + '_url'\n", "            if name == 'self_url':\n", "                name = 'url'\n", "            if root.tag == 'open511':\n", "                j['meta'][name] = elem.get('href')\n", "                continue\n", "        elif name.startswith('{' + NS_PROTECTED):\n", "            name = '!' + name[name.index('}') + 1:] \n", "        elif name[0] == '{':\n", "            # Namespace!\n", "            name = '+' + name[name.index('}') + 1:]\n", "\n", "        if name in j:\n", "            continue  # duplicate\n", "        elif elem.tag == 'link' and not elem.text:\n", "            j[name] = elem.get('href')\n", "        elif len(elem):\n", "            if name == 'grouped_events':\n", "                # An array of URLs\n", "                j[name] = [xml_link_to_json(child, to_dict=False) for child in elem]\n", "            elif name in ('attachments', 'media_files'):\n", "                # An array of JSON objects\n", "                j[name] = [xml_link_to_json(child, to_dict=True) for child in elem]\n", "            elif all((name == pluralize(child.tag) for child in elem)):\n", "                # <something><somethings> serializes to a JSON array\n", "                j[name] = [xml_to_json(child) for child in elem]\n", "            else:\n", "                j[name] = xml_to_json(elem)\n", "        else:\n", "            if root.tag == 'open511' and name.endswith('s') and not elem.text:\n", "                # Special case: an empty e.g. <events /> container at the root level\n", "                # should be serialized to [], not null\n", "                j[name] = []\n", "            else:\n", "                j[name] = _maybe_intify(elem.text)\n", "\n", "    return j\n"], "language": "python", "code": "def xml_to_json(root):\n    \"\"\"\"\"\"\n    j = {}\n    if len(root) == 0:\n        return _maybe_intify(root.text)\n    if len(root) == 1 and root[0].tag.startswith('{' + NS_GML):\n        return gml_to_geojson(root[0])\n    if root.tag == 'open511':\n        j['meta'] = {'version': root.get('version')}\n    for elem in root:\n        name = elem.tag\n        if name == 'link' and elem.get('rel'):\n            name = elem.get('rel') + '_url'\n            if name == 'self_url':\n                name = 'url'\n            if root.tag == 'open511':\n                j['meta'][name] = elem.get('href')\n                continue\n        elif name.startswith('{' + NS_PROTECTED):\n            name = '!' + name[name.index('}') + 1:]\n        elif name[0] == '{':\n            name = '+' + name[name.index('}') + 1:]\n        if name in j:\n            continue\n        elif elem.tag == 'link' and not elem.text:\n            j[name] = elem.get('href')\n        elif len(elem):\n            if name == 'grouped_events':\n                j[name] = [xml_link_to_json(child, to_dict=False) for child in\n                    elem]\n            elif name in ('attachments', 'media_files'):\n                j[name] = [xml_link_to_json(child, to_dict=True) for child in\n                    elem]\n            elif all(name == pluralize(child.tag) for child in elem):\n                j[name] = [xml_to_json(child) for child in elem]\n            else:\n                j[name] = xml_to_json(elem)\n        elif root.tag == 'open511' and name.endswith('s') and not elem.text:\n            j[name] = []\n        else:\n            j[name] = _maybe_intify(elem.text)\n    return j\n", "code_tokens": ["xml", "to", "json", "root", "if", "len", "root", "0", "return", "maybe", "intify", "root", "text", "if", "len", "root", "1", "and", "root", "0", "tag", "startswith", "ns", "gml", "return", "gml", "to", "geojson", "root", "0", "if", "root", "tag", "meta", "version", "root", "get", "version", "for", "elem", "in", "root", "name", "elem", "tag", "if", "name", "link", "and", "elem", "get", "rel", "name", "elem", "get", "rel", "url", "if", "name", "self", "url", "name", "url", "if", "root", "tag", "meta", "name", "elem", "get", "href", "continue", "elif", "name", "startswith", "ns", "protected", "name", "name", "name", "index", "1", "elif", "name", "0", "name", "name", "name", "index", "1", "if", "name", "in", "continue", "elif", "elem", "tag", "link", "and", "not", "elem", "text", "name", "elem", "get", "href", "elif", "len", "elem", "if", "name", "grouped", "events", "name", "xml", "link", "to", "json", "child", "to", "dict", "false", "for", "child", "in", "elem", "elif", "name", "in", "attachments", "media", "files", "name", "xml", "link", "to", "json", "child", "to", "dict", "true", "for", "child", "in", "elem", "elif", "all", "name", "pluralize", "child", "tag", "for", "child", "in", "elem", "name", "xml", "to", "json", "child", "for", "child", "in", "elem", "else", "name", "xml", "to", "json", "elem", "elif", "root", "tag", "and", "name", "endswith", "and", "not", "elem", "text", "name", "else", "name", "maybe", "intify", "elem", "text", "return"], "docstring": "json to xml conversion", "docstring_tokens": ["json", "to", "xml", "conversion"], "idx": 243}
{"url": "https://github.com/yyuu/botornado/blob/fffb056f5ff2324d1d5c1304014cfb1d899f602e/boto/cloudfront/identity.py#L77-L84", "repo": "botornado", "func_name": "to_xml", "original_string": ["    def to_xml(self):\n", "        s = '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n'\n", "        s += '<CloudFrontOriginAccessIdentityConfig xmlns=\"http://cloudfront.amazonaws.com/doc/2009-09-09/\">\\n'\n", "        s += '  <CallerReference>%s</CallerReference>\\n' % self.caller_reference\n", "        if self.comment:\n", "            s += '  <Comment>%s</Comment>\\n' % self.comment\n", "        s += '</CloudFrontOriginAccessIdentityConfig>\\n'\n", "        return s\n"], "language": "python", "code": "def to_xml(self):\n    s = '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n'\n    s += \"\"\"<CloudFrontOriginAccessIdentityConfig xmlns=\"http://cloudfront.amazonaws.com/doc/2009-09-09/\">\n\"\"\"\n    s += '  <CallerReference>%s</CallerReference>\\n' % self.caller_reference\n    if self.comment:\n        s += '  <Comment>%s</Comment>\\n' % self.comment\n    s += '</CloudFrontOriginAccessIdentityConfig>\\n'\n    return s\n", "code_tokens": ["to", "xml", "self", "xml", "version", "1", "0", "encoding", "utf", "8", "cloudfrontoriginaccessidentityconfig", "xmlns", "http", "cloudfront", "amazonaws", "com", "doc", "2009", "09", "09", "callerreference", "callerreference", "self", "caller", "reference", "if", "self", "comment", "comment", "comment", "self", "comment", "cloudfrontoriginaccessidentityconfig", "return"], "docstring": "json to xml conversion", "docstring_tokens": ["json", "to", "xml", "conversion"], "idx": 244}
{"url": "https://github.com/noahmorrison/chevron/blob/78f1a384eddef16906732d8db66deea6d37049b7/chevron/renderer.py#L34-L47", "repo": "chevron", "func_name": "_html_escape", "original_string": ["\n", "def _html_escape(string):\n", "    \"\"\"HTML escape all of these \" & < >\"\"\"\n", "\n", "    html_codes = {\n", "        '\"': '&quot;',\n", "        '<': '&lt;',\n", "        '>': '&gt;',\n", "    }\n", "\n", "    # & must be handled first\n", "    string = string.replace('&', '&amp;')\n", "    for char in html_codes:\n", "        string = string.replace(char, html_codes[char])\n"], "language": "python", "code": "def _html_escape(string):\n    \"\"\"\"\"\"\n    html_codes = {'\"': '&quot;', '<': '&lt;', '>': '&gt;'}\n    string = string.replace('&', '&amp;')\n    for char in html_codes:\n        string = string.replace(char, html_codes[char])\n", "code_tokens": ["html", "escape", "string", "html", "codes", "quot", "lt", "gt", "string", "string", "replace", "amp", "for", "char", "in", "html", "codes", "string", "string", "replace", "char", "html", "codes", "char"], "docstring": "html encode string", "docstring_tokens": ["html", "encode", "string"], "idx": 245}
{"url": "https://github.com/hit9/rux/blob/d7f60722658a3b83ac6d7bb3ca2790ac9c926b59/rux/parser.py#L33-L42", "repo": "rux", "func_name": "_code_no_lexer", "original_string": ["    def _code_no_lexer(self, text):\n", "        # encode to utf8 string\n", "        text = text.encode(charset).strip()\n", "        return(\n", "            \"\"\"\n", "            <div class=\"highlight\">\n", "              <pre><code>%s</code></pre>\n", "            </div>\n", "            \"\"\" % houdini.escape_html(text)\n", "        )\n"], "language": "python", "code": "def _code_no_lexer(self, text):\n    text = text.encode(charset).strip()\n    return (\n        \"\"\"\n            <div class=\"highlight\">\n              <pre><code>%s</code></pre>\n            </div>\n            \"\"\"\n         % houdini.escape_html(text))\n", "code_tokens": ["code", "no", "lexer", "self", "text", "text", "text", "encode", "charset", "strip", "return", "div", "class", "highlight", "pre", "code", "code", "pre", "div", "houdini", "escape", "html", "text"], "docstring": "html encode string", "docstring_tokens": ["html", "encode", "string"], "idx": 246}
{"url": "https://github.com/sprymix/metamagic.json/blob/c95d3cacd641d433af44f0774f51a085cb4888e6/metamagic/json/encoder.py#L156-L178", "repo": "metamagic.json", "func_name": "_encode_str", "original_string": ["    def _encode_str(self, obj, escape_quotes=True):\n", "        \"\"\"Return an ASCII-only JSON representation of a Python string\"\"\"\n", "        def replace(match):\n", "            s = match.group(0)\n", "            try:\n", "                if escape_quotes:\n", "                    return ESCAPE_DCT[s]\n", "                else:\n", "                    return BASE_ESCAPE_DCT[s]\n", "            except KeyError:\n", "                n = ord(s)\n", "                if n < 0x10000:\n", "                    return '\\\\u{0:04x}'.format(n)\n", "                else:\n", "                    # surrogate pair\n", "                    n -= 0x10000\n", "                    s1 = 0xd800 | ((n >> 10) & 0x3ff)\n", "                    s2 = 0xdc00 | (n & 0x3ff)\n", "                    return '\\\\u{0:04x}\\\\u{1:04x}'.format(s1, s2)\n", "        if escape_quotes:\n", "            return '\"' + ESCAPE_ASCII.sub(replace, obj) + '\"'\n", "        else:\n", "            return BASE_ESCAPE_ASCII.sub(replace, obj)\n"], "language": "python", "code": "def _encode_str(self, obj, escape_quotes=True):\n    \"\"\"\"\"\"\n\n    def replace(match):\n        s = match.group(0)\n        try:\n            if escape_quotes:\n                return ESCAPE_DCT[s]\n            else:\n                return BASE_ESCAPE_DCT[s]\n        except KeyError:\n            n = ord(s)\n            if n < 65536:\n                return '\\\\u{0:04x}'.format(n)\n            else:\n                n -= 65536\n                s1 = 55296 | n >> 10 & 1023\n                s2 = 56320 | n & 1023\n                return '\\\\u{0:04x}\\\\u{1:04x}'.format(s1, s2)\n    if escape_quotes:\n        return '\"' + ESCAPE_ASCII.sub(replace, obj) + '\"'\n    else:\n        return BASE_ESCAPE_ASCII.sub(replace, obj)\n", "code_tokens": ["encode", "str", "self", "obj", "escape", "quotes", "true", "def", "replace", "match", "match", "group", "0", "try", "if", "escape", "quotes", "return", "escape", "dct", "else", "return", "base", "escape", "dct", "except", "keyerror", "ord", "if", "65536", "return", "0", "format", "else", "65536", "55296", "10", "1023", "56320", "1023", "return", "0", "1", "format", "if", "escape", "quotes", "return", "escape", "ascii", "sub", "replace", "obj", "else", "return", "base", "escape", "ascii", "sub", "replace", "obj"], "docstring": "html encode string", "docstring_tokens": ["html", "encode", "string"], "idx": 247}
{"url": "https://github.com/adsabs/adsutils/blob/fb9d6b4f6ed5e6ca19c552efc3cdd6466c587fdb/adsutils/Unicode.py#L153-L159", "repo": "adsutils", "func_name": "encode", "original_string": ["    def encode(self,str):\n", "        data = self.u2ent(str)\n", "        data = data.replace(\"&\", \"&amp;\")\n", "        data = data.replace(\"<\", \"&lt;\")\n", "        data = data.replace(\"\\\"\", \"&quot;\")\n", "        data = data.replace(\">\", \"&gt;\")\n", "        return data\n"], "language": "python", "code": "def encode(self, str):\n    data = self.u2ent(str)\n    data = data.replace('&', '&amp;')\n    data = data.replace('<', '&lt;')\n    data = data.replace('\"', '&quot;')\n    data = data.replace('>', '&gt;')\n    return data\n", "code_tokens": ["encode", "self", "str", "data", "self", "str", "data", "data", "replace", "amp", "data", "data", "replace", "lt", "data", "data", "replace", "quot", "data", "data", "replace", "gt", "return", "data"], "docstring": "html encode string", "docstring_tokens": ["html", "encode", "string"], "idx": 248}
{"url": "https://github.com/wdecoster/nanoplotter/blob/80908dd1be585f450da5a66989de9de4d544ec85/nanoplotter/plot.py#L18-L24", "repo": "nanoplotter", "func_name": "encode", "original_string": ["    def encode(self):\n", "        if self.html:\n", "            return self.html\n", "        elif self.fig:\n", "            return self.encode2()\n", "        else:\n", "            return self.encode1()\n"], "language": "python", "code": "def encode(self):\n    if self.html:\n        return self.html\n    elif self.fig:\n        return self.encode2()\n    else:\n        return self.encode1()\n", "code_tokens": ["encode", "self", "if", "self", "html", "return", "self", "html", "elif", "self", "fig", "return", "self", "else", "return", "self"], "docstring": "html encode string", "docstring_tokens": ["html", "encode", "string"], "idx": 249}
{"url": "https://github.com/jeffa/HTML-Auto-python/blob/e0d24335d86c21b6278f776f3d3416c90e438ab9/t/05-encode.py#L6-L70", "repo": "HTML-Auto-python", "func_name": "test_lower", "original_string": ["    def test_lower(self):\n", "\n", "        encoder = Encoder()\n", "\n", "        self.assertEqual(\n", "            '&amp;&lt;&gt;&quot;&apos;',\n", "            encoder.encode( '&<>\"\\'' ),\n", "            'default chars encoded when chars is nil'\n", "        )\n", "\n", "        self.assertEqual(\n", "            '&amp;&lt;&gt;&quot;&apos;',\n", "            encoder.encode( '&<>\"\\'', '' ),\n", "            'encodes when chars is empty'\n", "        )\n", "\n", "        self.assertEqual(\n", "            'h&#101;llo',\n", "            encoder.encode( 'hello', 'e' ),\n", "            'requested chars encoded correctly'\n", "        )\n", "\n", "        self.assertEqual(\n", "            'hell&#48;',\n", "            encoder.encode( 'hell0', 0 ),\n", "            'zero encodes correctly'\n", "        )\n", "\n", "        self.assertEqual(\n", "            '&amp;b&#97;r',\n", "            encoder.encode( '&bar', 'a&' ),\n", "            'ampersand is not double encoded'\n", "        )\n", "\n", "        self.assertEqual(\n", "            'hello',\n", "            encoder.encode( 'hello' ),\n", "            'no encodes when default chars is nil'\n", "        )\n", "\n", "        self.assertEqual(\n", "            'hello',\n", "            encoder.encode( 'hello', '' ),\n", "            'no encodes when default chars is empty'\n", "        )\n", "\n", "        deadbeef = chr(222) + chr(173) + chr(190) + chr(239)\n", "\n", "        self.assertEqual(\n", "            '&THORN;&shy;&frac34;&iuml;',\n", "            encoder.encode( deadbeef, deadbeef ),\n", "            'hex codes encoded correctly'\n", "        )\n", "\n", "        self.assertEqual(\n", "            '&THORN;&shy;&frac34;&iuml;',\n", "            encoder.encode( deadbeef ),\n", "            'hex codes encoded correctly when chars is nil'\n", "        )\n", "\n", "        self.assertEqual(\n", "            '&THORN;&shy;&frac34;&iuml;',\n", "            encoder.encode( deadbeef, '' ),\n", "            'hex codes encoded correctly when chars is empty'\n", "        )\n"], "language": "python", "code": "def test_lower(self):\n    encoder = Encoder()\n    self.assertEqual('&amp;&lt;&gt;&quot;&apos;', encoder.encode('&<>\"\\''),\n        'default chars encoded when chars is nil')\n    self.assertEqual('&amp;&lt;&gt;&quot;&apos;', encoder.encode('&<>\"\\'',\n        ''), 'encodes when chars is empty')\n    self.assertEqual('h&#101;llo', encoder.encode('hello', 'e'),\n        'requested chars encoded correctly')\n    self.assertEqual('hell&#48;', encoder.encode('hell0', 0),\n        'zero encodes correctly')\n    self.assertEqual('&amp;b&#97;r', encoder.encode('&bar', 'a&'),\n        'ampersand is not double encoded')\n    self.assertEqual('hello', encoder.encode('hello'),\n        'no encodes when default chars is nil')\n    self.assertEqual('hello', encoder.encode('hello', ''),\n        'no encodes when default chars is empty')\n    deadbeef = chr(222) + chr(173) + chr(190) + chr(239)\n    self.assertEqual('&THORN;&shy;&frac34;&iuml;', encoder.encode(deadbeef,\n        deadbeef), 'hex codes encoded correctly')\n    self.assertEqual('&THORN;&shy;&frac34;&iuml;', encoder.encode(deadbeef),\n        'hex codes encoded correctly when chars is nil')\n    self.assertEqual('&THORN;&shy;&frac34;&iuml;', encoder.encode(deadbeef,\n        ''), 'hex codes encoded correctly when chars is empty')\n", "code_tokens": ["test", "lower", "self", "encoder", "encoder", "self", "assertequal", "amp", "lt", "gt", "quot", "apos", "encoder", "encode", "default", "chars", "encoded", "when", "chars", "is", "nil", "self", "assertequal", "amp", "lt", "gt", "quot", "apos", "encoder", "encode", "encodes", "when", "chars", "is", "empty", "self", "assertequal", "101", "llo", "encoder", "encode", "hello", "requested", "chars", "encoded", "correctly", "self", "assertequal", "hell", "48", "encoder", "encode", "0", "zero", "encodes", "correctly", "self", "assertequal", "amp", "97", "encoder", "encode", "bar", "ampersand", "is", "not", "double", "encoded", "self", "assertequal", "hello", "encoder", "encode", "hello", "no", "encodes", "when", "default", "chars", "is", "nil", "self", "assertequal", "hello", "encoder", "encode", "hello", "no", "encodes", "when", "default", "chars", "is", "empty", "deadbeef", "chr", "222", "chr", "173", "chr", "190", "chr", "239", "self", "assertequal", "thorn", "shy", "iuml", "encoder", "encode", "deadbeef", "deadbeef", "hex", "codes", "encoded", "correctly", "self", "assertequal", "thorn", "shy", "iuml", "encoder", "encode", "deadbeef", "hex", "codes", "encoded", "correctly", "when", "chars", "is", "nil", "self", "assertequal", "thorn", "shy", "iuml", "encoder", "encode", "deadbeef", "hex", "codes", "encoded", "correctly", "when", "chars", "is", "empty"], "docstring": "html encode string", "docstring_tokens": ["html", "encode", "string"], "idx": 250}
{"url": "https://github.com/honzajavorek/danube-delta/blob/d0a72f0704d52b888e7fb2b68c4fdc696d370018/danube_delta/plugins/utils.py#L8-L17", "repo": "danube-delta", "func_name": "modify_html", "original_string": ["def modify_html(content, prop='_content'):\n", "    html_string = getattr(content, prop)\n", "    html_tree = html.fromstring(html_string)\n", "\n", "    yield html_tree\n", "\n", "    html_string = html.tostring(html_tree, encoding='unicode')\n", "    html_string = re.sub(r'%7B(\\w+)%7D', r'{\\1}', html_string)\n", "    html_string = re.sub(r'%7C(\\w+)%7C', r'|\\1|', html_string)\n", "    setattr(content, prop, html_string)\n"], "language": "python", "code": "def modify_html(content, prop='_content'):\n    html_string = getattr(content, prop)\n    html_tree = html.fromstring(html_string)\n    yield html_tree\n    html_string = html.tostring(html_tree, encoding='unicode')\n    html_string = re.sub('%7B(\\\\w+)%7D', '{\\\\1}', html_string)\n    html_string = re.sub('%7C(\\\\w+)%7C', '|\\\\1|', html_string)\n    setattr(content, prop, html_string)\n", "code_tokens": ["modify", "html", "content", "prop", "content", "html", "string", "getattr", "content", "prop", "html", "tree", "html", "fromstring", "html", "string", "yield", "html", "tree", "html", "string", "html", "tostring", "html", "tree", "encoding", "unicode", "html", "string", "re", "sub", "1", "html", "string", "html", "string", "re", "sub", "1", "html", "string", "setattr", "content", "prop", "html", "string"], "docstring": "html encode string", "docstring_tokens": ["html", "encode", "string"], "idx": 251}
{"url": "https://github.com/romanorac/discomll/blob/a4703daffb2ba3c9f614bc3dbe45ae55884aea00/discomll/utils/model_view.py#L87-L93", "repo": "discomll", "func_name": "_linreg_model", "original_string": ["def _linreg_model(fitmodel):\n", "    output = \"Linear regression model\\n\\n\"\n", "    for k, v in result_iterator(fitmodel):\n", "        if k == \"thetas\":\n", "            output += \"Thetas\\n\"\n", "            output += \", \".join(map(str, v)) + \"\\n\\n\"\n", "    return output\n"], "language": "python", "code": "def _linreg_model(fitmodel):\n    output = 'Linear regression model\\n\\n'\n    for k, v in result_iterator(fitmodel):\n        if k == 'thetas':\n            output += 'Thetas\\n'\n            output += ', '.join(map(str, v)) + '\\n\\n'\n    return output\n", "code_tokens": ["linreg", "model", "fitmodel", "output", "linear", "regression", "model", "for", "in", "result", "iterator", "fitmodel", "if", "thetas", "output", "thetas", "output", "join", "map", "str", "return", "output"], "docstring": "linear regression", "docstring_tokens": ["linear", "regression"], "idx": 252}
{"url": "https://github.com/SoftwareDefinedBuildings/XBOS/blob/c12d4fb14518ea3ae98c471c28e0710fdf74dd25/apps/Data_quality_analysis/Main.py#L13-L18", "repo": "XBOS", "func_name": "func", "original_string": ["def func(X, y):\n", "    from sklearn.linear_model import LinearRegression\n", "    from sklearn.model_selection import cross_val_score\n", "    model = LinearRegression()\n", "    model.fit(X, y)\n", "    return model.predict(X)\n"], "language": "python", "code": "def func(X, y):\n    from sklearn.linear_model import LinearRegression\n    from sklearn.model_selection import cross_val_score\n    model = LinearRegression()\n    model.fit(X, y)\n    return model.predict(X)\n", "code_tokens": ["func", "from", "sklearn", "linear", "model", "import", "linearregression", "from", "sklearn", "model", "selection", "import", "cross", "val", "score", "model", "linearregression", "model", "fit", "return", "model", "predict"], "docstring": "linear regression", "docstring_tokens": ["linear", "regression"], "idx": 253}
{"url": "https://github.com/synw/dataswim/blob/4a4a53f80daa7cd8e8409d76a19ce07296269da2/dataswim/data/stats.py#L8-L21", "repo": "dataswim", "func_name": "lreg", "original_string": ["    def lreg(self, xcol, ycol, name=\"Regression\"):\n", "        \"\"\"\n", "        Add a column to the main dataframe populted with\n", "        the model's linear regression for a column\n", "        \"\"\"\n", "        try:\n", "            x = self.df[xcol].values.reshape(-1, 1)\n", "            y = self.df[ycol]\n", "            lm = linear_model.LinearRegression()\n", "            lm.fit(x, y)\n", "            predictions = lm.predict(x)\n", "            self.df[name] = predictions\n", "        except Exception as e:\n", "            self.err(e, \"Can not calculate linear regression\")\n"], "language": "python", "code": "def lreg(self, xcol, ycol, name='Regression'):\n    \"\"\"\"\"\"\n    try:\n        x = self.df[xcol].values.reshape(-1, 1)\n        y = self.df[ycol]\n        lm = linear_model.LinearRegression()\n        lm.fit(x, y)\n        predictions = lm.predict(x)\n        self.df[name] = predictions\n    except Exception as e:\n        self.err(e, 'Can not calculate linear regression')\n", "code_tokens": ["lreg", "self", "xcol", "ycol", "name", "regression", "try", "self", "df", "xcol", "values", "reshape", "1", "1", "self", "df", "ycol", "lm", "linear", "model", "linearregression", "lm", "fit", "predictions", "lm", "predict", "self", "df", "name", "predictions", "except", "exception", "as", "self", "err", "can", "not", "calculate", "linear", "regression"], "docstring": "linear regression", "docstring_tokens": ["linear", "regression"], "idx": 254}
{"url": "https://github.com/tmoerman/arboreto/blob/3ff7b6f987b32e5774771751dea646fa6feaaa52/arboreto/core.py#L105-L141", "repo": "arboreto", "func_name": "fit_model", "original_string": ["\n", "def fit_model(regressor_type,\n", "              regressor_kwargs,\n", "              tf_matrix,\n", "              target_gene_expression,\n", "              early_stop_window_length=EARLY_STOP_WINDOW_LENGTH,\n", "              seed=DEMON_SEED):\n", "    \"\"\"\n", "    :param regressor_type: string. Case insensitive.\n", "    :param regressor_kwargs: a dictionary of key-value pairs that configures the regressor.\n", "    :param tf_matrix: the predictor matrix (transcription factor matrix) as a numpy array.\n", "    :param target_gene_expression: the target (y) gene expression to predict in function of the tf_matrix (X).\n", "    :param early_stop_window_length: window length of the early stopping monitor.\n", "    :param seed: (optional) random seed for the regressors.\n", "    :return: a trained regression model.\n", "    \"\"\"\n", "    regressor_type = regressor_type.upper()\n", "\n", "\n", "    if isinstance(target_gene_expression, scipy.sparse.spmatrix):\n", "        target_gene_expression = target_gene_expression.A.flatten()\n", "\n", "    assert tf_matrix.shape[0] == target_gene_expression.shape[0]\n", "\n", "\n", "    def do_sklearn_regression():\n", "        regressor = SKLEARN_REGRESSOR_FACTORY[regressor_type](random_state=seed, **regressor_kwargs)\n", "\n", "        with_early_stopping = is_oob_heuristic_supported(regressor_type, regressor_kwargs)\n", "\n", "        if with_early_stopping:\n", "            regressor.fit(tf_matrix, target_gene_expression, monitor=EarlyStopMonitor(early_stop_window_length))\n", "        else:\n", "            regressor.fit(tf_matrix, target_gene_expression)\n", "\n", "        return regressor\n", "\n"], "language": "python", "code": "def fit_model(regressor_type, regressor_kwargs, tf_matrix,\n    target_gene_expression, early_stop_window_length=\n    EARLY_STOP_WINDOW_LENGTH, seed=DEMON_SEED):\n    \"\"\"\"\"\"\n    regressor_type = regressor_type.upper()\n    if isinstance(target_gene_expression, scipy.sparse.spmatrix):\n        target_gene_expression = target_gene_expression.A.flatten()\n    assert tf_matrix.shape[0] == target_gene_expression.shape[0]\n\n    def do_sklearn_regression():\n        regressor = SKLEARN_REGRESSOR_FACTORY[regressor_type](random_state=\n            seed, **regressor_kwargs)\n        with_early_stopping = is_oob_heuristic_supported(regressor_type,\n            regressor_kwargs)\n        if with_early_stopping:\n            regressor.fit(tf_matrix, target_gene_expression, monitor=\n                EarlyStopMonitor(early_stop_window_length))\n        else:\n            regressor.fit(tf_matrix, target_gene_expression)\n        return regressor\n", "code_tokens": ["fit", "model", "regressor", "type", "regressor", "kwargs", "tf", "matrix", "target", "gene", "expression", "early", "stop", "window", "length", "early", "stop", "window", "length", "seed", "demon", "seed", "regressor", "type", "regressor", "type", "upper", "if", "isinstance", "target", "gene", "expression", "scipy", "sparse", "spmatrix", "target", "gene", "expression", "target", "gene", "expression", "flatten", "assert", "tf", "matrix", "shape", "0", "target", "gene", "expression", "shape", "0", "def", "do", "sklearn", "regression", "regressor", "sklearn", "regressor", "factory", "regressor", "type", "random", "state", "seed", "regressor", "kwargs", "with", "early", "stopping", "is", "oob", "heuristic", "supported", "regressor", "type", "regressor", "kwargs", "if", "with", "early", "stopping", "regressor", "fit", "tf", "matrix", "target", "gene", "expression", "monitor", "earlystopmonitor", "early", "stop", "window", "length", "else", "regressor", "fit", "tf", "matrix", "target", "gene", "expression", "return", "regressor"], "docstring": "linear regression", "docstring_tokens": ["linear", "regression"], "idx": 255}
{"url": "https://github.com/SoftwareDefinedBuildings/XBOS/blob/c12d4fb14518ea3ae98c471c28e0710fdf74dd25/apps/Data_quality_analysis/Model_Data.py#L176-L203", "repo": "XBOS", "func_name": "linear_regression", "original_string": ["    def linear_regression(self):\n", "        \"\"\" Linear Regression.\n", "\n", "        This function runs linear regression and stores the, \n", "        1. Model\n", "        2. Model name \n", "        3. Mean score of cross validation\n", "        4. Metrics\n", "\n", "        \"\"\"\n", "\n", "        model = LinearRegression()\n", "        scores = []\n", "\n", "        kfold = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n", "        for i, (train, test) in enumerate(kfold.split(self.baseline_in, self.baseline_out)):\n", "            model.fit(self.baseline_in.iloc[train], self.baseline_out.iloc[train])\n", "            scores.append(model.score(self.baseline_in.iloc[test], self.baseline_out.iloc[test]))\n", "\n", "        mean_score = sum(scores) / len(scores)\n", "        \n", "        self.models.append(model)\n", "        self.model_names.append('Linear Regression')\n", "        self.max_scores.append(mean_score)\n", "\n", "        self.metrics['Linear Regression'] = {}\n", "        self.metrics['Linear Regression']['R2'] = mean_score\n", "        self.metrics['Linear Regression']['Adj R2'] = self.adj_r2(mean_score, self.baseline_in.shape[0], self.baseline_in.shape[1])\n"], "language": "python", "code": "def linear_regression(self):\n    \"\"\"\"\"\"\n    model = LinearRegression()\n    scores = []\n    kfold = KFold(n_splits=self.cv, shuffle=True, random_state=42)\n    for i, (train, test) in enumerate(kfold.split(self.baseline_in, self.\n        baseline_out)):\n        model.fit(self.baseline_in.iloc[train], self.baseline_out.iloc[train])\n        scores.append(model.score(self.baseline_in.iloc[test], self.\n            baseline_out.iloc[test]))\n    mean_score = sum(scores) / len(scores)\n    self.models.append(model)\n    self.model_names.append('Linear Regression')\n    self.max_scores.append(mean_score)\n    self.metrics['Linear Regression'] = {}\n    self.metrics['Linear Regression']['R2'] = mean_score\n    self.metrics['Linear Regression']['Adj R2'] = self.adj_r2(mean_score,\n        self.baseline_in.shape[0], self.baseline_in.shape[1])\n", "code_tokens": ["linear", "regression", "self", "model", "linearregression", "scores", "kfold", "kfold", "splits", "self", "cv", "shuffle", "true", "random", "state", "42", "for", "train", "test", "in", "enumerate", "kfold", "split", "self", "baseline", "in", "self", "baseline", "out", "model", "fit", "self", "baseline", "in", "iloc", "train", "self", "baseline", "out", "iloc", "train", "scores", "append", "model", "score", "self", "baseline", "in", "iloc", "test", "self", "baseline", "out", "iloc", "test", "mean", "score", "sum", "scores", "len", "scores", "self", "models", "append", "model", "self", "model", "names", "append", "linear", "regression", "self", "max", "scores", "append", "mean", "score", "self", "metrics", "linear", "regression", "self", "metrics", "linear", "regression", "mean", "score", "self", "metrics", "linear", "regression", "adj", "self", "adj", "mean", "score", "self", "baseline", "in", "shape", "0", "self", "baseline", "in", "shape", "1"], "docstring": "linear regression", "docstring_tokens": ["linear", "regression"], "idx": 256}
{"url": "https://github.com/LiftoffSoftware/htmltag/blob/f6989f9a3301e7c96ee613e5dbbe43b2bde615c7/htmltag.py#L442-L448", "repo": "htmltag", "func_name": "escape", "original_string": ["    def escape(self, string):\n", "        \"\"\"\n", "        Returns *string* with all instances of '<', '>', and '&' converted into\n", "        HTML entities.\n", "        \"\"\"\n", "        html_entities = {\"&\": \"&amp;\", '<': '&lt;', '>': '&gt;'}\n", "        return HTML(\"\".join(html_entities.get(c, c) for c in string))\n"], "language": "python", "code": "def escape(self, string):\n    \"\"\"\"\"\"\n    html_entities = {'&': '&amp;', '<': '&lt;', '>': '&gt;'}\n    return HTML(''.join(html_entities.get(c, c) for c in string))\n", "code_tokens": ["escape", "self", "string", "html", "entities", "amp", "lt", "gt", "return", "html", "join", "html", "entities", "get", "for", "in", "string"], "docstring": "html entities replace", "docstring_tokens": ["html", "entities", "replace"], "idx": 257}
{"url": "https://github.com/OpenTreeOfLife/peyotl/blob/5e4e52a0fdbd17f490aa644ad79fda6ea2eda7c0/peyotl/nexson_syntax/nexson2nexml.py#L101-L137", "repo": "peyotl", "func_name": "_partition_keys_for_xml", "original_string": ["    def _partition_keys_for_xml(self, o):\n", "        \"\"\"Breaks o into four content type by key syntax:\n", "            attrib keys (start with '@'),\n", "            text (value associated with the '$' or None),\n", "            child element keys (all others)\n", "            meta element\n", "        \"\"\"\n", "        ak = {}\n", "        tk = None\n", "        ck = {}\n", "        mc = {}\n", "        # _LOG.debug('o = {o}'.format(o=o))\n", "        for k, v in o.items():\n", "            if k.startswith('@'):\n", "                if k == '@xmlns':\n", "                    if '$' in v:\n", "                        ak['xmlns'] = v['$']\n", "                    for nsk, nsv in v.items():\n", "                        if nsk != '$':\n", "                            ak['xmlns:' + nsk] = nsv\n", "                else:\n", "                    s = k[1:]\n", "                    if isinstance(v, bool):\n", "                        v = u'true' if v else u'false'\n", "                    ak[s] = UNICODE(v)\n", "            elif k == '$':\n", "                tk = v\n", "            elif k.startswith('^') and (not self._migrating_from_bf):\n", "                s = k[1:]\n", "                val = _convert_hbf_meta_val_for_xml(s, v)\n", "                _add_value_to_dict_bf(mc, s, val)\n", "            elif (k == u'meta') and self._migrating_from_bf:\n", "                s, val = _convert_bf_meta_val_for_xml(v)\n", "                _add_value_to_dict_bf(mc, s, val)\n", "            else:\n", "                ck[k] = v\n", "        return ak, tk, ck, mc\n"], "language": "python", "code": "def _partition_keys_for_xml(self, o):\n    \"\"\"\"\"\"\n    ak = {}\n    tk = None\n    ck = {}\n    mc = {}\n    for k, v in o.items():\n        if k.startswith('@'):\n            if k == '@xmlns':\n                if '$' in v:\n                    ak['xmlns'] = v['$']\n                for nsk, nsv in v.items():\n                    if nsk != '$':\n                        ak['xmlns:' + nsk] = nsv\n            else:\n                s = k[1:]\n                if isinstance(v, bool):\n                    v = u'true' if v else u'false'\n                ak[s] = UNICODE(v)\n        elif k == '$':\n            tk = v\n        elif k.startswith('^') and not self._migrating_from_bf:\n            s = k[1:]\n            val = _convert_hbf_meta_val_for_xml(s, v)\n            _add_value_to_dict_bf(mc, s, val)\n        elif k == u'meta' and self._migrating_from_bf:\n            s, val = _convert_bf_meta_val_for_xml(v)\n            _add_value_to_dict_bf(mc, s, val)\n        else:\n            ck[k] = v\n    return ak, tk, ck, mc\n", "code_tokens": ["partition", "keys", "for", "xml", "self", "ak", "tk", "none", "ck", "mc", "for", "in", "items", "if", "startswith", "if", "xmlns", "if", "in", "ak", "xmlns", "for", "nsk", "nsv", "in", "items", "if", "nsk", "ak", "xmlns", "nsk", "nsv", "else", "1", "if", "isinstance", "bool", "true", "if", "else", "false", "ak", "unicode", "elif", "tk", "elif", "startswith", "and", "not", "self", "migrating", "from", "bf", "1", "val", "convert", "hbf", "meta", "val", "for", "xml", "add", "value", "to", "dict", "bf", "mc", "val", "elif", "meta", "and", "self", "migrating", "from", "bf", "val", "convert", "bf", "meta", "val", "for", "xml", "add", "value", "to", "dict", "bf", "mc", "val", "else", "ck", "return", "ak", "tk", "ck", "mc"], "docstring": "html entities replace", "docstring_tokens": ["html", "entities", "replace"], "idx": 258}
{"url": "https://github.com/nickoala/telepot/blob/3792fde251d0f1d5a6ca16c8ad1a71f89360c41d/telepot/text.py#L65-L88", "repo": "telepot", "func_name": "apply_entities_as_html", "original_string": ["def apply_entities_as_html(text, entities):\n", "    \"\"\"\n", "    Format text as HTML. Also take care of escaping special characters.\n", "    Returned value can be passed to :meth:`.Bot.sendMessage` with appropriate\n", "    ``parse_mode``.\n", "\n", "    :param text:\n", "        plain text\n", "\n", "    :param entities:\n", "        a list of `MessageEntity <https://core.telegram.org/bots/api#messageentity>`_ objects\n", "    \"\"\"\n", "    escapes = {'<': '&lt;',\n", "               '>': '&gt;',\n", "               '&': '&amp;',}\n", "\n", "    formatters = {'bold':         lambda s,e: '<b>'+s+'</b>',\n", "                  'italic':       lambda s,e: '<i>'+s+'</i>',\n", "                  'text_link':    lambda s,e: '<a href=\"'+e['url']+'\">'+s+'</a>',\n", "                  'text_mention': lambda s,e: '<a href=\"tg://user?id='+str(e['user']['id'])+'\">'+s+'</a>',\n", "                  'code':         lambda s,e: '<code>'+s+'</code>',\n", "                  'pre':          lambda s,e: '<pre>'+s+'</pre>'}\n", "\n", "    return _apply_entities(text, entities, escapes, formatters)\n"], "language": "python", "code": "def apply_entities_as_html(text, entities):\n    \"\"\"\"\"\"\n    escapes = {'<': '&lt;', '>': '&gt;', '&': '&amp;'}\n    formatters = {'bold': lambda s, e: '<b>' + s + '</b>', 'italic': lambda\n        s, e: '<i>' + s + '</i>', 'text_link': lambda s, e: '<a href=\"' + e\n        ['url'] + '\">' + s + '</a>', 'text_mention': lambda s, e: \n        '<a href=\"tg://user?id=' + str(e['user']['id']) + '\">' + s + '</a>',\n        'code': lambda s, e: '<code>' + s + '</code>', 'pre': lambda s, e: \n        '<pre>' + s + '</pre>'}\n    return _apply_entities(text, entities, escapes, formatters)\n", "code_tokens": ["apply", "entities", "as", "html", "text", "entities", "escapes", "lt", "gt", "amp", "formatters", "bold", "lambda", "italic", "lambda", "text", "link", "lambda", "href", "url", "text", "mention", "lambda", "href", "tg", "user", "id", "str", "user", "id", "code", "lambda", "code", "code", "pre", "lambda", "pre", "pre", "return", "apply", "entities", "text", "entities", "escapes", "formatters"], "docstring": "html entities replace", "docstring_tokens": ["html", "entities", "replace"], "idx": 259}
{"url": "https://github.com/honzajavorek/tipi/blob/cbe51192725608b6fba1244a48610ae231b13e08/tipi/repl.py#L65-L74", "repo": "tipi", "func_name": "replace", "original_string": ["def replace(html, replacements=None):\n", "    \"\"\"Performs replacements on given HTML string.\"\"\"\n", "    if not replacements:\n", "        return html  # no replacements\n", "    html = HTMLFragment(html)\n", "\n", "    for r in replacements:\n", "        r.replace(html)\n", "\n", "    return unicode(html)\n"], "language": "python", "code": "def replace(html, replacements=None):\n    \"\"\"\"\"\"\n    if not replacements:\n        return html\n    html = HTMLFragment(html)\n    for r in replacements:\n        r.replace(html)\n    return unicode(html)\n", "code_tokens": ["replace", "html", "replacements", "none", "if", "not", "replacements", "return", "html", "html", "htmlfragment", "html", "for", "in", "replacements", "replace", "html", "return", "unicode", "html"], "docstring": "html entities replace", "docstring_tokens": ["html", "entities", "replace"], "idx": 260}
{"url": "https://github.com/fboender/ansible-cmdb/blob/ebd960ac10684e8c9ec2b12751bba2c4c9504ab7/lib/mako/filters.py#L159-L174", "repo": "ansible-cmdb", "func_name": "htmlentityreplace_errors", "original_string": ["def htmlentityreplace_errors(ex):\n", "    \"\"\"An encoding error handler.\n", "\n", "    This python `codecs`_ error handler replaces unencodable\n", "    characters with HTML entities, or, if no HTML entity exists for\n", "    the character, XML character references.\n", "\n", "    >>> u'The cost was \\u20ac12.'.encode('latin1', 'htmlentityreplace')\n", "    'The cost was &euro;12.'\n", "    \"\"\"\n", "    if isinstance(ex, UnicodeEncodeError):\n", "        # Handle encoding errors\n", "        bad_text = ex.object[ex.start:ex.end]\n", "        text = _html_entities_escaper.escape(bad_text)\n", "        return (compat.text_type(text), ex.end)\n", "    raise ex\n"], "language": "python", "code": "def htmlentityreplace_errors(ex):\n    \"\"\"\"\"\"\n    if isinstance(ex, UnicodeEncodeError):\n        bad_text = ex.object[ex.start:ex.end]\n        text = _html_entities_escaper.escape(bad_text)\n        return compat.text_type(text), ex.end\n    raise ex\n", "code_tokens": ["htmlentityreplace", "errors", "ex", "if", "isinstance", "ex", "unicodeencodeerror", "bad", "text", "ex", "object", "ex", "start", "ex", "end", "text", "html", "entities", "escaper", "escape", "bad", "text", "return", "compat", "text", "type", "text", "ex", "end", "raise", "ex"], "docstring": "html entities replace", "docstring_tokens": ["html", "entities", "replace"], "idx": 261}
{"url": "https://github.com/markfinger/django-node/blob/a2f56bf027fd3c4cbc6a0213881922a50acae1d6/django_node/utils.py#L183-L193", "repo": "django-node", "func_name": "decode_html_entities", "original_string": ["def decode_html_entities(html):\n", "    \"\"\"\n", "    Decodes a limited set of HTML entities.\n", "    \"\"\"\n", "    if not html:\n", "        return html\n", "\n", "    for entity, char in six.iteritems(html_entity_map):\n", "        html = html.replace(entity, char)\n", "\n", "    return html\n"], "language": "python", "code": "def decode_html_entities(html):\n    \"\"\"\"\"\"\n    if not html:\n        return html\n    for entity, char in six.iteritems(html_entity_map):\n        html = html.replace(entity, char)\n    return html\n", "code_tokens": ["decode", "html", "entities", "html", "if", "not", "html", "return", "html", "for", "entity", "char", "in", "six", "iteritems", "html", "entity", "map", "html", "html", "replace", "entity", "char", "return", "html"], "docstring": "html entities replace", "docstring_tokens": ["html", "entities", "replace"], "idx": 262}
{"url": "https://github.com/html5lib/html5lib-python/blob/4b2275497b624c6e97150fa2eb16a7db7ed42111/utils/entities.py#L82-L89", "repo": "html5lib-python", "func_name": "make_entities_code", "original_string": ["def make_entities_code(entities):\n", "    entities_text = \"\\n\".join(\"    \\\"%s\\\": u\\\"%s\\\",\" % (\n", "        name, entities[name].encode(\n", "            \"unicode-escape\").replace(\"\\\"\", \"\\\\\\\"\"))\n", "        for name in sorted(entities.keys()))\n", "    return \"\"\"entities = {\n", "%s\n", "}\"\"\" % entities_text\n"], "language": "python", "code": "def make_entities_code(entities):\n    entities_text = '\\n'.join('    \"%s\": u\"%s\",' % (name, entities[name].\n        encode('unicode-escape').replace('\"', '\\\\\"')) for name in sorted(\n        entities.keys()))\n    return 'entities = {\\n%s\\n}' % entities_text\n", "code_tokens": ["make", "entities", "code", "entities", "entities", "text", "join", "name", "entities", "name", "encode", "unicode", "escape", "replace", "for", "name", "in", "sorted", "entities", "keys", "return", "entities", "entities", "text"], "docstring": "html entities replace", "docstring_tokens": ["html", "entities", "replace"], "idx": 263}
{"url": "https://github.com/tnkteja/myhelp/blob/fb3a4809d448ad14d5b2e6ddf2e7e89ad52b71cb/virtualEnvironment/lib/python2.7/site-packages/coverage/html.py#L375-L386", "repo": "myhelp", "func_name": "escape", "original_string": ["def escape(t):\n", "    \"\"\"HTML-escape the text in `t`.\"\"\"\n", "    return (t\n", "            # Convert HTML special chars into HTML entities.\n", "            .replace(\"&\", \"&amp;\").replace(\"<\", \"&lt;\").replace(\">\", \"&gt;\")\n", "            .replace(\"'\", \"&#39;\").replace('\"', \"&quot;\")\n", "            # Convert runs of spaces: \"......\" -> \"&nbsp;.&nbsp;.&nbsp;.\"\n", "            .replace(\"  \", \"&nbsp; \")\n", "            # To deal with odd-length runs, convert the final pair of spaces\n", "            # so that \".....\" -> \"&nbsp;.&nbsp;&nbsp;.\"\n", "            .replace(\"  \", \"&nbsp; \")\n", "        )\n"], "language": "python", "code": "def escape(t):\n    \"\"\"\"\"\"\n    return t.replace('&', '&amp;').replace('<', '&lt;').replace('>', '&gt;'\n        ).replace(\"'\", '&#39;').replace('\"', '&quot;').replace('  ', '&nbsp; '\n        ).replace('  ', '&nbsp; ')\n", "code_tokens": ["escape", "return", "replace", "amp", "replace", "lt", "replace", "gt", "replace", "39", "replace", "quot", "replace", "nbsp", "replace", "nbsp"], "docstring": "html entities replace", "docstring_tokens": ["html", "entities", "replace"], "idx": 264}
{"url": "https://github.com/collectiveacuity/labPack/blob/52949ece35e72e3cc308f54d9ffa6bfbd96805b8/labpack/records/time.py#L181-L206", "repo": "labPack", "func_name": "fromEpoch", "original_string": ["    def fromEpoch(cls, epoch_time):\n", "\n", "        ''' a method for constructing a labDT object from epoch timestamp\n", "\n", "        :param epoch_time: number with epoch timestamp info\n", "        :return: labDT object\n", "        '''\n", "\n", "    # validate input\n", "        title = 'Epoch time input for labDT.fromEpoch'\n", "        if not isinstance(epoch_time, float) and not isinstance(epoch_time, int):\n", "            raise TypeError('\\n%s must be an integer or float.' % title)\n", "\n", "    # construct labDT from epoch time\n", "        dT = datetime.utcfromtimestamp(epoch_time).replace(tzinfo=pytz.utc)\n", "        dt_kwargs = {\n", "            'year': dT.year,\n", "            'month': dT.month,\n", "            'day': dT.day,\n", "            'hour': dT.hour,\n", "            'minute': dT.minute,\n", "            'second': dT.second,\n", "            'microsecond': dT.microsecond,\n", "            'tzinfo': dT.tzinfo\n", "        }\n", "        return labDT(**dt_kwargs)\n"], "language": "python", "code": "def fromEpoch(cls, epoch_time):\n    \"\"\"\"\"\"\n    title = 'Epoch time input for labDT.fromEpoch'\n    if not isinstance(epoch_time, float) and not isinstance(epoch_time, int):\n        raise TypeError('\\n%s must be an integer or float.' % title)\n    dT = datetime.utcfromtimestamp(epoch_time).replace(tzinfo=pytz.utc)\n    dt_kwargs = {'year': dT.year, 'month': dT.month, 'day': dT.day, 'hour':\n        dT.hour, 'minute': dT.minute, 'second': dT.second, 'microsecond':\n        dT.microsecond, 'tzinfo': dT.tzinfo}\n    return labDT(**dt_kwargs)\n", "code_tokens": ["fromepoch", "cls", "epoch", "time", "title", "epoch", "time", "input", "for", "labdt", "fromepoch", "if", "not", "isinstance", "epoch", "time", "float", "and", "not", "isinstance", "epoch", "time", "int", "raise", "typeerror", "must", "be", "an", "integer", "or", "float", "title", "dt", "datetime", "utcfromtimestamp", "epoch", "time", "replace", "tzinfo", "pytz", "utc", "dt", "kwargs", "year", "dt", "year", "month", "dt", "month", "day", "dt", "day", "hour", "dt", "hour", "minute", "dt", "minute", "second", "dt", "second", "microsecond", "dt", "microsecond", "tzinfo", "dt", "tzinfo", "return", "labdt", "dt", "kwargs"], "docstring": "convert a utc time to epoch", "docstring_tokens": ["convert", "a", "utc", "time", "to", "epoch"], "idx": 265}
{"url": "https://github.com/theonion/django-bulbs/blob/0c0e6e3127a7dc487b96677fab95cacd2b3806da/bulbs/utils/methods.py#L66-L68", "repo": "django-bulbs", "func_name": "datetime_to_epoch_seconds", "original_string": ["def datetime_to_epoch_seconds(value):\n", "    epoch = datetime.utcfromtimestamp(0).replace(tzinfo=timezone.utc)\n", "    return (value - epoch).total_seconds()\n"], "language": "python", "code": "def datetime_to_epoch_seconds(value):\n    epoch = datetime.utcfromtimestamp(0).replace(tzinfo=timezone.utc)\n    return (value - epoch).total_seconds()\n", "code_tokens": ["datetime", "to", "epoch", "seconds", "value", "epoch", "datetime", "utcfromtimestamp", "0", "replace", "tzinfo", "timezone", "utc", "return", "value", "epoch", "total", "seconds"], "docstring": "convert a utc time to epoch", "docstring_tokens": ["convert", "a", "utc", "time", "to", "epoch"], "idx": 266}
{"url": "https://github.com/pymacaron/pymacaron/blob/af244f203f8216108b39d374d46bf8e1813f13d5/pymacaron/utils.py#L48-L60", "repo": "pymacaron", "func_name": "to_epoch", "original_string": ["def to_epoch(t):\n", "    \"\"\"Take a datetime, either as a string or a datetime.datetime object,\n", "    and return the corresponding epoch\"\"\"\n", "    if isinstance(t, str):\n", "        if '+' not in t:\n", "            t = t + '+00:00'\n", "        t = parser.parse(t)\n", "    elif t.tzinfo is None or t.tzinfo.utcoffset(t) is None:\n", "        t = t.replace(tzinfo=pytz.timezone('utc'))\n", "\n", "    t0 = datetime.datetime(1970, 1, 1, 0, 0, 0, 0, pytz.timezone('utc'))\n", "    delta = t - t0\n", "    return int(delta.total_seconds())\n"], "language": "python", "code": "def to_epoch(t):\n    \"\"\"\"\"\"\n    if isinstance(t, str):\n        if '+' not in t:\n            t = t + '+00:00'\n        t = parser.parse(t)\n    elif t.tzinfo is None or t.tzinfo.utcoffset(t) is None:\n        t = t.replace(tzinfo=pytz.timezone('utc'))\n    t0 = datetime.datetime(1970, 1, 1, 0, 0, 0, 0, pytz.timezone('utc'))\n    delta = t - t0\n    return int(delta.total_seconds())\n", "code_tokens": ["to", "epoch", "if", "isinstance", "str", "if", "not", "in", "00", "00", "parser", "parse", "elif", "tzinfo", "is", "none", "or", "tzinfo", "utcoffset", "is", "none", "replace", "tzinfo", "pytz", "timezone", "utc", "datetime", "datetime", "1970", "1", "1", "0", "0", "0", "0", "pytz", "timezone", "utc", "delta", "return", "int", "delta", "total", "seconds"], "docstring": "convert a utc time to epoch", "docstring_tokens": ["convert", "a", "utc", "time", "to", "epoch"], "idx": 267}
{"url": "https://github.com/aws/aws-xray-sdk-python/blob/707358cd3a516d51f2ebf71cf34f00e8d906a667/aws_xray_sdk/core/sampling/connector.py#L136-L149", "repo": "aws-xray-sdk-python", "func_name": "_dt_to_epoch", "original_string": ["    def _dt_to_epoch(self, dt):\n", "        \"\"\"\n", "        Convert a offset-aware datetime to POSIX time.\n", "        \"\"\"\n", "        if PY2:\n", "            # The input datetime is from botocore unmarshalling and it is\n", "            # offset-aware so the timedelta of subtracting this time\n", "            # to 01/01/1970 using the same tzinfo gives us\n", "            # Unix Time (also known as POSIX Time).\n", "            time_delta = dt - datetime(1970, 1, 1).replace(tzinfo=dt.tzinfo)\n", "            return int(time_delta.total_seconds())\n", "        else:\n", "            # Added in python 3.3+ and directly returns POSIX time.\n", "            return int(dt.timestamp())\n"], "language": "python", "code": "def _dt_to_epoch(self, dt):\n    \"\"\"\"\"\"\n    if PY2:\n        time_delta = dt - datetime(1970, 1, 1).replace(tzinfo=dt.tzinfo)\n        return int(time_delta.total_seconds())\n    else:\n        return int(dt.timestamp())\n", "code_tokens": ["dt", "to", "epoch", "self", "dt", "if", "time", "delta", "dt", "datetime", "1970", "1", "1", "replace", "tzinfo", "dt", "tzinfo", "return", "int", "time", "delta", "total", "seconds", "else", "return", "int", "dt", "timestamp"], "docstring": "convert a utc time to epoch", "docstring_tokens": ["convert", "a", "utc", "time", "to", "epoch"], "idx": 268}
{"url": "https://github.com/apache/airflow/blob/b69c686ad8a0c89b9136bb4b31767257eb7b2597/airflow/utils/timezone.py#L67-L79", "repo": "airflow", "func_name": "utc_epoch", "original_string": ["\n", "def utc_epoch() -> dt.datetime:\n", "    \"\"\"\n", "    Gets the epoch in the users timezone\n", "\n", "    :return:\n", "    \"\"\"\n", "    # pendulum utcnow() is not used as that sets a TimezoneInfo object\n", "    # instead of a Timezone. This is not picklable and also creates issues\n", "    # when using replace()\n", "    result = dt.datetime(1970, 1, 1)\n", "    result = result.replace(tzinfo=utc)\n", "\n"], "language": "python", "code": "def utc_epoch() ->dt.datetime:\n    \"\"\"\"\"\"\n    result = dt.datetime(1970, 1, 1)\n    result = result.replace(tzinfo=utc)\n", "code_tokens": ["utc", "epoch", "dt", "datetime", "result", "dt", "datetime", "1970", "1", "1", "result", "result", "replace", "tzinfo", "utc"], "docstring": "convert a utc time to epoch", "docstring_tokens": ["convert", "a", "utc", "time", "to", "epoch"], "idx": 269}
{"url": "https://github.com/myusuf3/delorean/blob/3e8a7b8cfd4c26546f62bde2f34002893adfa08a/delorean/dates.py#L492-L512", "repo": "delorean", "func_name": "epoch", "original_string": ["    def epoch(self):\n", "        \"\"\"\n", "        Returns the total seconds since epoch associated with\n", "        the Delorean object.\n", "\n", "        .. testsetup::\n", "\n", "            from datetime import datetime\n", "            from delorean import Delorean\n", "\n", "        .. doctest::\n", "\n", "            >>> d = Delorean(datetime(2015, 1, 1), timezone='US/Pacific')\n", "            >>> d.epoch\n", "            1420099200.0\n", "\n", "        \"\"\"\n", "        epoch_sec = pytz.utc.localize(datetime.utcfromtimestamp(0))\n", "        now_sec = pytz.utc.normalize(self._dt)\n", "        delta_sec = now_sec - epoch_sec\n", "        return get_total_second(delta_sec)\n"], "language": "python", "code": "def epoch(self):\n    \"\"\"\"\"\"\n    epoch_sec = pytz.utc.localize(datetime.utcfromtimestamp(0))\n    now_sec = pytz.utc.normalize(self._dt)\n    delta_sec = now_sec - epoch_sec\n    return get_total_second(delta_sec)\n", "code_tokens": ["epoch", "self", "epoch", "sec", "pytz", "utc", "localize", "datetime", "utcfromtimestamp", "0", "now", "sec", "pytz", "utc", "normalize", "self", "dt", "delta", "sec", "now", "sec", "epoch", "sec", "return", "get", "total", "second", "delta", "sec"], "docstring": "convert a utc time to epoch", "docstring_tokens": ["convert", "a", "utc", "time", "to", "epoch"], "idx": 270}
{"url": "https://github.com/demosdemon/format-pipfile/blob/f95162c49d8fc13153080ddb11ac5a5dcd4d2e7c/ci/appveyor-download.py#L95-L102", "repo": "format-pipfile", "func_name": "unpack_zipfile", "original_string": ["def unpack_zipfile(filename):\n", "    \"\"\"Unpack a zipfile, using the names in the zip.\"\"\"\n", "    with open(filename, \"rb\") as fzip:\n", "        z = zipfile.ZipFile(fzip)\n", "        for name in z.namelist():\n", "            print((\"      extracting {}\".format(name)))\n", "            ensure_dirs(name)\n", "            z.extract(name)\n"], "language": "python", "code": "def unpack_zipfile(filename):\n    \"\"\"\"\"\"\n    with open(filename, 'rb') as fzip:\n        z = zipfile.ZipFile(fzip)\n        for name in z.namelist():\n            print('      extracting {}'.format(name))\n            ensure_dirs(name)\n            z.extract(name)\n", "code_tokens": ["unpack", "zipfile", "filename", "with", "open", "filename", "rb", "as", "fzip", "zipfile", "zipfile", "fzip", "for", "name", "in", "namelist", "print", "extracting", "format", "name", "ensure", "dirs", "name", "extract", "name"], "docstring": "how to extract zip file recursively", "docstring_tokens": ["how", "to", "extract", "zip", "file", "recursively"], "idx": 271}
{"url": "https://github.com/drestebon/papageorge/blob/a30ea59bf6b4f5d151bd3f476a0a8357d89495d4/lib/papageorge/cli.py#L573-L579", "repo": "papageorge", "func_name": "continuation", "original_string": ["    def continuation(self, regexp, txt):\n", "        txt_ = self.txt_list.body.pop().get_text()\n", "        self.txt_list.body.append(ConsoleText((txt_[1][0][0], \n", "                                         txt_[0]+regexp.groups()[0])))\n", "        pos = len(self.txt_list.body)-1\n", "        self.txt_list.set_focus(pos)\n", "        return False\n"], "language": "python", "code": "def continuation(self, regexp, txt):\n    txt_ = self.txt_list.body.pop().get_text()\n    self.txt_list.body.append(ConsoleText((txt_[1][0][0], txt_[0] + regexp.\n        groups()[0])))\n    pos = len(self.txt_list.body) - 1\n    self.txt_list.set_focus(pos)\n    return False\n", "code_tokens": ["continuation", "self", "regexp", "txt", "txt", "self", "txt", "list", "body", "pop", "get", "text", "self", "txt", "list", "body", "append", "consoletext", "txt", "1", "0", "0", "txt", "0", "regexp", "groups", "0", "pos", "len", "self", "txt", "list", "body", "1", "self", "txt", "list", "set", "focus", "pos", "return", "false"], "docstring": "set file attrib hidden", "docstring_tokens": ["set", "file", "attrib", "hidden"], "idx": 272}
{"url": "https://github.com/KelSolaar/Umbra/blob/66f45f08d9d723787f1191989f8b0dda84b412ce/umbra/components/factory/script_editor/editor.py#L448-L467", "repo": "Umbra", "func_name": "set_file", "original_string": ["    def set_file(self, file=None, is_modified=False, is_untitled=False):\n", "        \"\"\"\n", "        Sets the editor file.\n", "\n", "        :param File: File to set.\n", "        :type File: unicode\n", "        :param is_modified: File modified state.\n", "        :type is_modified: bool\n", "        :param is_untitled: File untitled state.\n", "        :type is_untitled: bool\n", "        :return: Method success.\n", "        :rtype: bool\n", "        \"\"\"\n", "\n", "        LOGGER.debug(\"> Setting '{0}' editor file.\".format(file))\n", "        self.__file = file\n", "        self.__is_untitled = is_untitled\n", "        self.set_modified(is_modified)\n", "        self.set_title()\n", "        return True\n"], "language": "python", "code": "def set_file(self, file=None, is_modified=False, is_untitled=False):\n    \"\"\"\"\"\"\n    LOGGER.debug(\"> Setting '{0}' editor file.\".format(file))\n    self.__file = file\n    self.__is_untitled = is_untitled\n    self.set_modified(is_modified)\n    self.set_title()\n    return True\n", "code_tokens": ["set", "file", "self", "file", "none", "is", "modified", "false", "is", "untitled", "false", "logger", "debug", "setting", "0", "editor", "file", "format", "file", "self", "file", "file", "self", "is", "untitled", "is", "untitled", "self", "set", "modified", "is", "modified", "self", "set", "title", "return", "true"], "docstring": "set file attrib hidden", "docstring_tokens": ["set", "file", "attrib", "hidden"], "idx": 273}
{"url": "https://github.com/mattharrison/rst2odp/blob/4adbf29b28c8207ec882f792ded07e98b1d3e7d0/odplib/preso.py#L170-L175", "repo": "rst2odp", "func_name": "sub_el", "original_string": ["def sub_el(parent, tag, attrib=None):\n", "    attrib = attrib or {}\n", "    tag = get_nstag(tag)\n", "    attrib = update_attrib(attrib)\n", "    el = et.SubElement(parent, tag, attrib)  # , nsmap=NAMESPACES)\n", "    return el\n"], "language": "python", "code": "def sub_el(parent, tag, attrib=None):\n    attrib = attrib or {}\n    tag = get_nstag(tag)\n    attrib = update_attrib(attrib)\n    el = et.SubElement(parent, tag, attrib)\n    return el\n", "code_tokens": ["sub", "el", "parent", "tag", "attrib", "none", "attrib", "attrib", "or", "tag", "get", "nstag", "tag", "attrib", "update", "attrib", "attrib", "el", "et", "subelement", "parent", "tag", "attrib", "return", "el"], "docstring": "set file attrib hidden", "docstring_tokens": ["set", "file", "attrib", "hidden"], "idx": 274}
{"url": "https://github.com/SuminAndrew/lxml-asserts/blob/a5e8d25177357ba52e5eb0abacdc2f4bab59d584/lxml_asserts/__init__.py#L26-L59", "repo": "lxml-asserts", "func_name": "_assert_tag_and_attributes_are_equal", "original_string": ["def _assert_tag_and_attributes_are_equal(xml1, xml2, can_extend=False):\n", "    if xml1.tag != xml2.tag:\n", "        raise AssertionError(u'Tags do not match: {tag1} != {tag2}'.format(\n", "            tag1=_describe_element(xml1), tag2=_describe_element(xml2)\n", "        ))\n", "\n", "    added_attributes = set(xml2.attrib).difference(xml1.attrib)\n", "    missing_attributes = set(xml1.attrib).difference(xml2.attrib)\n", "\n", "    if missing_attributes:\n", "        raise AssertionError(u'Second xml misses attributes: {path}/({attributes})'.format(\n", "            path=_describe_element(xml2), attributes=','.join(missing_attributes)\n", "        ))\n", "\n", "    if not can_extend and added_attributes:\n", "        raise AssertionError(u'Second xml has additional attributes: {path}/({attributes})'.format(\n", "            path=_describe_element(xml2), attributes=','.join(added_attributes)\n", "        ))\n", "\n", "    for attrib in xml1.attrib:\n", "        if not _xml_compare_text(xml1.attrib[attrib], xml2.attrib[attrib], False):\n", "            raise AssertionError(u\"Attribute values are not equal: {path}/{attribute}['{v1}' != '{v2}']\".format(\n", "                path=_describe_element(xml1), attribute=attrib, v1=xml1.attrib[attrib], v2=xml2.attrib[attrib]\n", "            ))\n", "\n", "    if not _xml_compare_text(xml1.text, xml2.text, True):\n", "        raise AssertionError(u\"Tags text differs: {path}['{t1}' != '{t2}']\".format(\n", "            path=_describe_element(xml1), t1=xml1.text, t2=xml2.text\n", "        ))\n", "\n", "    if not _xml_compare_text(xml1.tail, xml2.tail, True):\n", "        raise AssertionError(u\"Tags tail differs: {path}['{t1}' != '{t2}']\".format(\n", "            path=_describe_element(xml1), t1=xml1.tail, t2=xml2.tail\n", "        ))\n"], "language": "python", "code": "def _assert_tag_and_attributes_are_equal(xml1, xml2, can_extend=False):\n    if xml1.tag != xml2.tag:\n        raise AssertionError(u'Tags do not match: {tag1} != {tag2}'.format(\n            tag1=_describe_element(xml1), tag2=_describe_element(xml2)))\n    added_attributes = set(xml2.attrib).difference(xml1.attrib)\n    missing_attributes = set(xml1.attrib).difference(xml2.attrib)\n    if missing_attributes:\n        raise AssertionError(\n            u'Second xml misses attributes: {path}/({attributes})'.format(\n            path=_describe_element(xml2), attributes=','.join(\n            missing_attributes)))\n    if not can_extend and added_attributes:\n        raise AssertionError(\n            u'Second xml has additional attributes: {path}/({attributes})'.\n            format(path=_describe_element(xml2), attributes=','.join(\n            added_attributes)))\n    for attrib in xml1.attrib:\n        if not _xml_compare_text(xml1.attrib[attrib], xml2.attrib[attrib], \n            False):\n            raise AssertionError(\n                u\"Attribute values are not equal: {path}/{attribute}['{v1}' != '{v2}']\"\n                .format(path=_describe_element(xml1), attribute=attrib, v1=\n                xml1.attrib[attrib], v2=xml2.attrib[attrib]))\n    if not _xml_compare_text(xml1.text, xml2.text, True):\n        raise AssertionError(u\"Tags text differs: {path}['{t1}' != '{t2}']\"\n            .format(path=_describe_element(xml1), t1=xml1.text, t2=xml2.text))\n    if not _xml_compare_text(xml1.tail, xml2.tail, True):\n        raise AssertionError(u\"Tags tail differs: {path}['{t1}' != '{t2}']\"\n            .format(path=_describe_element(xml1), t1=xml1.tail, t2=xml2.tail))\n", "code_tokens": ["assert", "tag", "and", "attributes", "are", "equal", "can", "extend", "false", "if", "tag", "tag", "raise", "assertionerror", "tags", "do", "not", "match", "format", "describe", "element", "describe", "element", "added", "attributes", "set", "attrib", "difference", "attrib", "missing", "attributes", "set", "attrib", "difference", "attrib", "if", "missing", "attributes", "raise", "assertionerror", "second", "xml", "misses", "attributes", "path", "attributes", "format", "path", "describe", "element", "attributes", "join", "missing", "attributes", "if", "not", "can", "extend", "and", "added", "attributes", "raise", "assertionerror", "second", "xml", "has", "additional", "attributes", "path", "attributes", "format", "path", "describe", "element", "attributes", "join", "added", "attributes", "for", "attrib", "in", "attrib", "if", "not", "xml", "compare", "text", "attrib", "attrib", "attrib", "attrib", "false", "raise", "assertionerror", "attribute", "values", "are", "not", "equal", "path", "attribute", "format", "path", "describe", "element", "attribute", "attrib", "attrib", "attrib", "attrib", "attrib", "if", "not", "xml", "compare", "text", "text", "text", "true", "raise", "assertionerror", "tags", "text", "differs", "path", "format", "path", "describe", "element", "text", "text", "if", "not", "xml", "compare", "text", "tail", "tail", "true", "raise", "assertionerror", "tags", "tail", "differs", "path", "format", "path", "describe", "element", "tail", "tail"], "docstring": "set file attrib hidden", "docstring_tokens": ["set", "file", "attrib", "hidden"], "idx": 275}
{"url": "https://github.com/bmuller/txairbrake/blob/38e65fe2330c6ce7fb788bad0cff0a85cceb1943/txairbrake/observers.py#L112-L125", "repo": "txairbrake", "func_name": "_tracebackToTree", "original_string": ["    def _tracebackToTree(self, failure):\n", "        backtrace = ET.Element('backtrace')\n", "\n", "        frames = failure.stack + failure.frames\n", "\n", "        for function_name, filename, line_number, localz, globalz in frames:\n", "            attrib = {'file': filename,\n", "                      'number': str(line_number),\n", "                      'method': \"%s: %s\" % (\n", "                        function_name,\n", "                        linecache.getline(filename, line_number).strip())}\n", "            backtrace.append(ET.Element('line', attrib=attrib))\n", "\n", "        return backtrace\n"], "language": "python", "code": "def _tracebackToTree(self, failure):\n    backtrace = ET.Element('backtrace')\n    frames = failure.stack + failure.frames\n    for function_name, filename, line_number, localz, globalz in frames:\n        attrib = {'file': filename, 'number': str(line_number), 'method': \n            '%s: %s' % (function_name, linecache.getline(filename,\n            line_number).strip())}\n        backtrace.append(ET.Element('line', attrib=attrib))\n    return backtrace\n", "code_tokens": ["tracebacktotree", "self", "failure", "backtrace", "et", "element", "backtrace", "frames", "failure", "stack", "failure", "frames", "for", "function", "name", "filename", "line", "number", "localz", "globalz", "in", "frames", "attrib", "file", "filename", "number", "str", "line", "number", "method", "function", "name", "linecache", "getline", "filename", "line", "number", "strip", "backtrace", "append", "et", "element", "line", "attrib", "attrib", "return", "backtrace"], "docstring": "set file attrib hidden", "docstring_tokens": ["set", "file", "attrib", "hidden"], "idx": 276}
{"url": "https://github.com/lamestation/lamestation-sdk-tools/blob/25d1fe8dc276cf4addbb13fd29ea3cf22b09f9ca/lspaint/EventHandler.py#L97-L106", "repo": "lamestation-sdk-tools", "func_name": "SetUndoRedo", "original_string": ["    def SetUndoRedo(self):\n", "        f = FileManager().CurrentFile()\n", "        self.parent.toolbar.EnableTool( wx.ID_UNDO, f.undo)\n", "        self.parent.menu.Enable( wx.ID_UNDO, f.undo)\n", "\n", "#        self.parent.toolbar.EnableTool( wx.ID_SAVE, f.undo)\n", "#        self.parent.menu.Enable( wx.ID_SAVE, f.undo)\n", "\n", "        self.parent.toolbar.EnableTool( wx.ID_REDO, f.redo)\n", "        self.parent.menu.Enable( wx.ID_REDO, f.redo)\n"], "language": "python", "code": "def SetUndoRedo(self):\n    f = FileManager().CurrentFile()\n    self.parent.toolbar.EnableTool(wx.ID_UNDO, f.undo)\n    self.parent.menu.Enable(wx.ID_UNDO, f.undo)\n    self.parent.toolbar.EnableTool(wx.ID_REDO, f.redo)\n    self.parent.menu.Enable(wx.ID_REDO, f.redo)\n", "code_tokens": ["setundoredo", "self", "filemanager", "currentfile", "self", "parent", "toolbar", "enabletool", "wx", "id", "undo", "undo", "self", "parent", "menu", "enable", "wx", "id", "undo", "undo", "self", "parent", "toolbar", "enabletool", "wx", "id", "redo", "redo", "self", "parent", "menu", "enable", "wx", "id", "redo", "redo"], "docstring": "set file attrib hidden", "docstring_tokens": ["set", "file", "attrib", "hidden"], "idx": 277}
{"url": "https://github.com/googledatalab/pydatalab/blob/d9031901d5bca22fe0d5925d204e6698df9852e1/solutionbox/ml_workbench/xgboost/trainer/task.py#L155-L261", "repo": "pydatalab", "func_name": "parse_arguments", "original_string": ["def parse_arguments(argv):\n", "  \"\"\"Parse the command line arguments.\"\"\"\n", "  parser = DatalabParser(\n", "      epilog=('Note that if using a DNN model, --hidden-layer-size1=NUM, '\n", "              '--hidden-layer-size2=NUM, ..., is also required. '),\n", "      datalab_epilog=(\"\"\"\n", "  Note that if using a DNN model,\n", "  hidden-layer-size1: NUM\n", "  hidden-layer-size2: NUM\n", "  ...\n", "  is also required. \"\"\"))\n", "\n", "  # HP parameters\n", "  parser.add_argument(\n", "      '--epsilon', type=float, default=0.0005, metavar='R',\n", "      help='tf.train.AdamOptimizer epsilon. Only used in dnn models.')\n", "  parser.add_argument(\n", "      '--l1-regularization', type=float, default=0.0, metavar='R',\n", "      help='L1 term for linear models.')\n", "  parser.add_argument(\n", "      '--l2-regularization', type=float, default=0.0, metavar='R',\n", "      help='L2 term for linear models.')\n", "\n", "  # Model parameters\n", "  parser.add_argument(\n", "    '--model', required=True,\n", "    choices=['linear_classification', 'linear_regression', 'dnn_classification', 'dnn_regression'])\n", "  parser.add_argument(\n", "      '--top-n', type=int, default=0, metavar='N',\n", "      help=('For classification problems, the output graph will contain the '\n", "            'labels and scores for the top n classes, and results will be in the form of '\n", "            '\"predicted, predicted_2, ..., probability, probability_2, ...\". '\n", "            'If --top-n=0, then all labels and scores are returned in the form of '\n", "            '\"predicted, class_name1, class_name2,...\".'))\n", "\n", "  # HP parameters\n", "  parser.add_argument(\n", "      '--learning-rate', type=float, default=0.01, metavar='R',\n", "      help='optimizer learning rate.')\n", "\n", "  # Training input parameters\n", "  parser.add_argument(\n", "      '--max-steps', type=int, metavar='N',\n", "      help='Maximum number of training steps to perform. If unspecified, will '\n", "           'honor \"max-epochs\".')\n", "  parser.add_argument(\n", "      '--max-epochs', type=int, default=1000, metavar='N',\n", "      help='Maximum number of training data epochs on which to train. If '\n", "           'both \"max-steps\" and \"max-epochs\" are specified, the training '\n", "           'job will run for \"max-steps\" or \"num-epochs\", whichever occurs '\n", "           'first. If early stopping is enabled, training may also stop '\n", "           'earlier.')\n", "  parser.add_argument(\n", "      '--train-batch-size', type=int, default=64, metavar='N',\n", "      help='How many training examples are used per step. If num-epochs is '\n", "           'used, the last batch may not be full.')\n", "  parser.add_argument(\n", "      '--eval-batch-size', type=int, default=64, metavar='N',\n", "      help='Batch size during evaluation. Larger values increase performance '\n", "           'but also increase peak memory usgae on the master node. One pass '\n", "           'over the full eval set is performed per evaluation run.')\n", "  parser.add_argument(\n", "      '--min-eval-frequency', type=int, default=1000, metavar='N',\n", "      help='Minimum number of training steps between evaluations. Evaluation '\n", "           'does not occur if no new checkpoint is available, hence, this is '\n", "           'the minimum. If 0, the evaluation will only happen after training. ')\n", "  parser.add_argument(\n", "      '--early-stopping-num_evals', type=int, default=3,\n", "      help='Automatic training stop after results of specified number of evals '\n", "           'in a row show the model performance does not improve. Set to 0 to '\n", "           'disable early stopping.')\n", "  parser.add_argument(\n", "      '--logging-level', choices=['error', 'warning', 'info'],\n", "      help='The TF logging level. If absent, use info for cloud training '\n", "           'and warning for local training.')\n", "\n", "  args, remaining_args = parser.parse_known_args(args=argv[1:])\n", "\n", "  # All HP parambeters must be unique, so we need to support an unknown number\n", "  # of --hidden-layer-size1=10 --lhidden-layer-size2=10 ...\n", "  # Look at remaining_args for hidden-layer-size\\d+ to get the layer info.\n", "\n", "  # Get number of layers\n", "  pattern = re.compile('hidden-layer-size(\\d+)')\n", "  num_layers = 0\n", "  for other_arg in remaining_args:\n", "    match = re.search(pattern, other_arg)\n", "    if match:\n", "      if int(match.group(1)) <= 0:\n", "        raise ValueError('layer size must be a positive integer. Was given %s' % other_arg)\n", "      num_layers = max(num_layers, int(match.group(1)))\n", "\n", "  # Build a new parser so we catch unknown args and missing layer_sizes.\n", "  parser = argparse.ArgumentParser()\n", "  for i in range(num_layers):\n", "    parser.add_argument('--hidden-layer-size%s' % str(i + 1), type=int, required=True)\n", "\n", "  layer_args = vars(parser.parse_args(args=remaining_args))\n", "  hidden_layer_sizes = []\n", "  for i in range(num_layers):\n", "    key = 'hidden_layer_size%s' % str(i + 1)\n", "    hidden_layer_sizes.append(layer_args[key])\n", "\n", "  assert len(hidden_layer_sizes) == num_layers\n", "  args.hidden_layer_sizes = hidden_layer_sizes\n", "\n", "  return args\n"], "language": "python", "code": "def parse_arguments(argv):\n    \"\"\"\"\"\"\n    parser = DatalabParser(epilog=\n        'Note that if using a DNN model, --hidden-layer-size1=NUM, --hidden-layer-size2=NUM, ..., is also required. '\n        , datalab_epilog=\n        \"\"\"\n  Note that if using a DNN model,\n  hidden-layer-size1: NUM\n  hidden-layer-size2: NUM\n  ...\n  is also required. \"\"\"\n        )\n    parser.add_argument('--epsilon', type=float, default=0.0005, metavar=\n        'R', help='tf.train.AdamOptimizer epsilon. Only used in dnn models.')\n    parser.add_argument('--l1-regularization', type=float, default=0.0,\n        metavar='R', help='L1 term for linear models.')\n    parser.add_argument('--l2-regularization', type=float, default=0.0,\n        metavar='R', help='L2 term for linear models.')\n    parser.add_argument('--model', required=True, choices=[\n        'linear_classification', 'linear_regression', 'dnn_classification',\n        'dnn_regression'])\n    parser.add_argument('--top-n', type=int, default=0, metavar='N', help=\n        'For classification problems, the output graph will contain the labels and scores for the top n classes, and results will be in the form of \"predicted, predicted_2, ..., probability, probability_2, ...\". If --top-n=0, then all labels and scores are returned in the form of \"predicted, class_name1, class_name2,...\".'\n        )\n    parser.add_argument('--learning-rate', type=float, default=0.01,\n        metavar='R', help='optimizer learning rate.')\n    parser.add_argument('--max-steps', type=int, metavar='N', help=\n        'Maximum number of training steps to perform. If unspecified, will honor \"max-epochs\".'\n        )\n    parser.add_argument('--max-epochs', type=int, default=1000, metavar='N',\n        help=\n        'Maximum number of training data epochs on which to train. If both \"max-steps\" and \"max-epochs\" are specified, the training job will run for \"max-steps\" or \"num-epochs\", whichever occurs first. If early stopping is enabled, training may also stop earlier.'\n        )\n    parser.add_argument('--train-batch-size', type=int, default=64, metavar\n        ='N', help=\n        'How many training examples are used per step. If num-epochs is used, the last batch may not be full.'\n        )\n    parser.add_argument('--eval-batch-size', type=int, default=64, metavar=\n        'N', help=\n        'Batch size during evaluation. Larger values increase performance but also increase peak memory usgae on the master node. One pass over the full eval set is performed per evaluation run.'\n        )\n    parser.add_argument('--min-eval-frequency', type=int, default=1000,\n        metavar='N', help=\n        'Minimum number of training steps between evaluations. Evaluation does not occur if no new checkpoint is available, hence, this is the minimum. If 0, the evaluation will only happen after training. '\n        )\n    parser.add_argument('--early-stopping-num_evals', type=int, default=3,\n        help=\n        'Automatic training stop after results of specified number of evals in a row show the model performance does not improve. Set to 0 to disable early stopping.'\n        )\n    parser.add_argument('--logging-level', choices=['error', 'warning',\n        'info'], help=\n        'The TF logging level. If absent, use info for cloud training and warning for local training.'\n        )\n    args, remaining_args = parser.parse_known_args(args=argv[1:])\n    pattern = re.compile('hidden-layer-size(\\\\d+)')\n    num_layers = 0\n    for other_arg in remaining_args:\n        match = re.search(pattern, other_arg)\n        if match:\n            if int(match.group(1)) <= 0:\n                raise ValueError(\n                    'layer size must be a positive integer. Was given %s' %\n                    other_arg)\n            num_layers = max(num_layers, int(match.group(1)))\n    parser = argparse.ArgumentParser()\n    for i in range(num_layers):\n        parser.add_argument('--hidden-layer-size%s' % str(i + 1), type=int,\n            required=True)\n    layer_args = vars(parser.parse_args(args=remaining_args))\n    hidden_layer_sizes = []\n    for i in range(num_layers):\n        key = 'hidden_layer_size%s' % str(i + 1)\n        hidden_layer_sizes.append(layer_args[key])\n    assert len(hidden_layer_sizes) == num_layers\n    args.hidden_layer_sizes = hidden_layer_sizes\n    return args\n", "code_tokens": ["parse", "arguments", "argv", "parser", "datalabparser", "epilog", "note", "that", "if", "using", "dnn", "model", "hidden", "layer", "num", "hidden", "layer", "num", "is", "also", "required", "datalab", "epilog", "note", "that", "if", "using", "dnn", "model", "hidden", "layer", "num", "hidden", "layer", "num", "is", "also", "required", "parser", "add", "argument", "epsilon", "type", "float", "default", "0", "0005", "metavar", "help", "tf", "train", "adamoptimizer", "epsilon", "only", "used", "in", "dnn", "models", "parser", "add", "argument", "regularization", "type", "float", "default", "0", "0", "metavar", "help", "term", "for", "linear", "models", "parser", "add", "argument", "regularization", "type", "float", "default", "0", "0", "metavar", "help", "term", "for", "linear", "models", "parser", "add", "argument", "model", "required", "true", "choices", "linear", "classification", "linear", "regression", "dnn", "classification", "dnn", "regression", "parser", "add", "argument", "top", "type", "int", "default", "0", "metavar", "help", "for", "classification", "problems", "the", "output", "graph", "will", "contain", "the", "labels", "and", "scores", "for", "the", "top", "classes", "and", "results", "will", "be", "in", "the", "form", "of", "predicted", "predicted", "2", "probability", "probability", "2", "if", "top", "0", "then", "all", "labels", "and", "scores", "are", "returned", "in", "the", "form", "of", "predicted", "class", "class", "parser", "add", "argument", "learning", "rate", "type", "float", "default", "0", "01", "metavar", "help", "optimizer", "learning", "rate", "parser", "add", "argument", "max", "steps", "type", "int", "metavar", "help", "maximum", "number", "of", "training", "steps", "to", "perform", "if", "unspecified", "will", "honor", "max", "epochs", "parser", "add", "argument", "max", "epochs", "type", "int", "default", "1000", "metavar", "help", "maximum", "number", "of", "training", "data", "epochs", "on", "which", "to", "train", "if", "both", "max", "steps", "and", "max", "epochs", "are", "specified", "the", "training", "job", "will", "run", "for", "max", "steps", "or", "num", "epochs", "whichever", "occurs", "first", "if", "early", "stopping", "is", "enabled", "training", "may", "also", "stop", "earlier", "parser", "add", "argument", "train", "batch", "size", "type", "int", "default", "64", "metavar", "help", "how", "many", "training", "examples", "are", "used", "per", "step", "if", "num", "epochs", "is", "used", "the", "last", "batch", "may", "not", "be", "full", "parser", "add", "argument", "eval", "batch", "size", "type", "int", "default", "64", "metavar", "help", "batch", "size", "during", "evaluation", "larger", "values", "increase", "performance", "but", "also", "increase", "peak", "memory", "usgae", "on", "the", "master", "node", "one", "pass", "over", "the", "full", "eval", "set", "is", "performed", "per", "evaluation", "run", "parser", "add", "argument", "min", "eval", "frequency", "type", "int", "default", "1000", "metavar", "help", "minimum", "number", "of", "training", "steps", "between", "evaluations", "evaluation", "does", "not", "occur", "if", "no", "new", "checkpoint", "is", "available", "hence", "this", "is", "the", "minimum", "if", "0", "the", "evaluation", "will", "only", "happen", "after", "training", "parser", "add", "argument", "early", "stopping", "num", "evals", "type", "int", "default", "3", "help", "automatic", "training", "stop", "after", "results", "of", "specified", "number", "of", "evals", "in", "row", "show", "the", "model", "performance", "does", "not", "improve", "set", "to", "0", "to", "disable", "early", "stopping", "parser", "add", "argument", "logging", "level", "choices", "error", "warning", "info", "help", "the", "tf", "logging", "level", "if", "absent", "use", "info", "for", "cloud", "training", "and", "warning", "for", "local", "training", "args", "remaining", "args", "parser", "parse", "known", "args", "args", "argv", "1", "pattern", "re", "compile", "hidden", "layer", "size", "num", "layers", "0", "for", "other", "arg", "in", "remaining", "args", "match", "re", "search", "pattern", "other", "arg", "if", "match", "if", "int", "match", "group", "1", "0", "raise", "valueerror", "layer", "size", "must", "be", "positive", "integer", "was", "given", "other", "arg", "num", "layers", "max", "num", "layers", "int", "match", "group", "1", "parser", "argparse", "argumentparser", "for", "in", "range", "num", "layers", "parser", "add", "argument", "hidden", "layer", "size", "str", "1", "type", "int", "required", "true", "layer", "args", "vars", "parser", "parse", "args", "args", "remaining", "args", "hidden", "layer", "sizes", "for", "in", "range", "num", "layers", "key", "hidden", "layer", "size", "str", "1", "hidden", "layer", "sizes", "append", "layer", "args", "key", "assert", "len", "hidden", "layer", "sizes", "num", "layers", "args", "hidden", "layer", "sizes", "hidden", "layer", "sizes", "return", "args"], "docstring": "set file attrib hidden", "docstring_tokens": ["set", "file", "attrib", "hidden"], "idx": 278}
{"url": "https://github.com/census-instrumentation/opencensus-python/blob/992b223f7e34c5dcb65922b7d5c827e7a1351e7d/opencensus/trace/propagation/binary_format.py#L95-L136", "repo": "opencensus-python", "func_name": "from_header", "original_string": ["    def from_header(self, binary):\n", "        \"\"\"Generate a SpanContext object using the trace context header.\n", "        The value of enabled parsed from header is int. Need to convert to\n", "        bool.\n", "\n", "        :type binary: bytes\n", "        :param binary: Trace context header which was extracted from the\n", "                       request headers.\n", "\n", "        :rtype: :class:`~opencensus.trace.span_context.SpanContext`\n", "        :returns: SpanContext generated from the trace context header.\n", "        \"\"\"\n", "        # If no binary provided, generate a new SpanContext\n", "        if binary is None:\n", "            return span_context_module.SpanContext(from_header=False)\n", "\n", "        # If cannot parse, return a new SpanContext and ignore the context\n", "        # from binary.\n", "        try:\n", "            data = Header._make(struct.unpack(BINARY_FORMAT, binary))\n", "        except struct.error:\n", "            logging.warning(\n", "                'Cannot parse the incoming binary data {}, '\n", "                'wrong format. Total bytes length should be {}.'.format(\n", "                    binary, FORMAT_LENGTH\n", "                )\n", "            )\n", "            return span_context_module.SpanContext(from_header=False)\n", "\n", "        # data.trace_id is in bytes with length 16, hexlify it to hex bytes\n", "        # with length 32, then decode it to hex string using utf-8.\n", "        trace_id = str(binascii.hexlify(data.trace_id).decode(UTF8))\n", "        span_id = str(binascii.hexlify(data.span_id).decode(UTF8))\n", "        trace_options = TraceOptions(data.trace_option)\n", "\n", "        span_context = span_context_module.SpanContext(\n", "                trace_id=trace_id,\n", "                span_id=span_id,\n", "                trace_options=trace_options,\n", "                from_header=True)\n", "\n", "        return span_context\n"], "language": "python", "code": "def from_header(self, binary):\n    \"\"\"\"\"\"\n    if binary is None:\n        return span_context_module.SpanContext(from_header=False)\n    try:\n        data = Header._make(struct.unpack(BINARY_FORMAT, binary))\n    except struct.error:\n        logging.warning(\n            'Cannot parse the incoming binary data {}, wrong format. Total bytes length should be {}.'\n            .format(binary, FORMAT_LENGTH))\n        return span_context_module.SpanContext(from_header=False)\n    trace_id = str(binascii.hexlify(data.trace_id).decode(UTF8))\n    span_id = str(binascii.hexlify(data.span_id).decode(UTF8))\n    trace_options = TraceOptions(data.trace_option)\n    span_context = span_context_module.SpanContext(trace_id=trace_id,\n        span_id=span_id, trace_options=trace_options, from_header=True)\n    return span_context\n", "code_tokens": ["from", "header", "self", "binary", "if", "binary", "is", "none", "return", "span", "context", "module", "spancontext", "from", "header", "false", "try", "data", "header", "make", "struct", "unpack", "binary", "format", "binary", "except", "struct", "error", "logging", "warning", "cannot", "parse", "the", "incoming", "binary", "data", "wrong", "format", "total", "bytes", "length", "should", "be", "format", "binary", "format", "length", "return", "span", "context", "module", "spancontext", "from", "header", "false", "trace", "id", "str", "binascii", "hexlify", "data", "trace", "id", "decode", "span", "id", "str", "binascii", "hexlify", "data", "span", "id", "decode", "trace", "options", "traceoptions", "data", "trace", "option", "span", "context", "span", "context", "module", "spancontext", "trace", "id", "trace", "id", "span", "id", "span", "id", "trace", "options", "trace", "options", "from", "header", "true", "return", "span", "context"], "docstring": "parse binary file to custom class", "docstring_tokens": ["parse", "binary", "file", "to", "custom", "class"], "idx": 279}
{"url": "https://github.com/williballenthin/python-evtx/blob/4e9e29544adde64c79ff9b743269ecb18c677eb4/Evtx/BinaryParser.py#L121-L128", "repo": "python-evtx", "func_name": "__init__", "original_string": ["    def __init__(self, value):\n", "        \"\"\"\n", "        Constructor.\n", "        Arguments:\n", "        - `value`: A string description.\n", "        \"\"\"\n", "        super(BinaryParserException, self).__init__()\n", "        self._value = value\n"], "language": "python", "code": "def __init__(self, value):\n    \"\"\"\"\"\"\n    super(BinaryParserException, self).__init__()\n    self._value = value\n", "code_tokens": ["init", "self", "value", "super", "binaryparserexception", "self", "init", "self", "value", "value"], "docstring": "parse binary file to custom class", "docstring_tokens": ["parse", "binary", "file", "to", "custom", "class"], "idx": 280}
{"url": "https://github.com/sci-bots/pygtkhelpers/blob/3a6e6d6340221c686229cd1c951d7537dae81b07/pygtkhelpers/debug/console.py#L645-L668", "repo": "pygtkhelpers", "func_name": "ConsoleType", "original_string": ["def ConsoleType(t=gtk.TextView):\n", "    class console_type(t, _Console):\n", "        __gsignals__ = {\n", "            'command': (gobject.SIGNAL_RUN_LAST, gobject.TYPE_NONE,\n", "                        (object, )),\n", "            'key-press-event': 'override'}\n", "\n", "        def __init__(self, *args, **kwargs):\n", "            if gtk.pygtk_version[1] < 8:\n", "                gobject.GObject.__init__(self)\n", "            else:\n", "                t.__init__(self)\n", "            _Console.__init__(self, *args, **kwargs)\n", "\n", "        def do_command(self, code):\n", "            return _Console.do_command(self, code)\n", "\n", "        def do_key_press_event(self, event):\n", "            return _Console.do_key_press_event(self, event, t)\n", "\n", "    if gtk.pygtk_version[1] < 8:\n", "        gobject.type_register(console_type)\n", "\n", "    return console_type\n"], "language": "python", "code": "def ConsoleType(t=gtk.TextView):\n\n\n    class console_type(t, _Console):\n        __gsignals__ = {'command': (gobject.SIGNAL_RUN_LAST, gobject.\n            TYPE_NONE, (object,)), 'key-press-event': 'override'}\n\n        def __init__(self, *args, **kwargs):\n            if gtk.pygtk_version[1] < 8:\n                gobject.GObject.__init__(self)\n            else:\n                t.__init__(self)\n            _Console.__init__(self, *args, **kwargs)\n\n        def do_command(self, code):\n            return _Console.do_command(self, code)\n\n        def do_key_press_event(self, event):\n            return _Console.do_key_press_event(self, event, t)\n    if gtk.pygtk_version[1] < 8:\n        gobject.type_register(console_type)\n    return console_type\n", "code_tokens": ["consoletype", "gtk", "textview", "class", "console", "type", "console", "gsignals", "command", "gobject", "signal", "run", "last", "gobject", "type", "none", "object", "key", "press", "event", "override", "def", "init", "self", "args", "kwargs", "if", "gtk", "pygtk", "version", "1", "8", "gobject", "gobject", "init", "self", "else", "init", "self", "console", "init", "self", "args", "kwargs", "def", "do", "command", "self", "code", "return", "console", "do", "command", "self", "code", "def", "do", "key", "press", "event", "self", "event", "return", "console", "do", "key", "press", "event", "self", "event", "if", "gtk", "pygtk", "version", "1", "8", "gobject", "type", "register", "console", "type", "return", "console", "type"], "docstring": "parse binary file to custom class", "docstring_tokens": ["parse", "binary", "file", "to", "custom", "class"], "idx": 281}
{"url": "https://github.com/jldantas/libmft/blob/65a988605fe7663b788bd81dcb52c0a4eaad1549/libmft/attribute.py#L1599-L1630", "repo": "libmft", "func_name": "_from_binary_reparse", "original_string": ["def _from_binary_reparse(cls, binary_stream):\n", "    \"\"\"See base class.\"\"\"\n", "    ''' Reparse type flags - 4\n", "            Reparse tag - 4 bits\n", "            Reserved - 12 bits\n", "            Reparse type - 2 bits\n", "        Reparse data length - 2\n", "        Padding - 2\n", "    '''\n", "    #content = cls._REPR.unpack(binary_view[:cls._REPR.size])\n", "    reparse_tag, data_len = cls._REPR.unpack(binary_stream[:cls._REPR.size])\n", "\n", "    #reparse_tag (type, flags) data_len, guid, data\n", "    reparse_type = ReparseType(reparse_tag & 0x0000FFFF)\n", "    reparse_flags = ReparseFlags((reparse_tag & 0xF0000000) >> 28)\n", "    guid = None #guid exists only in third party reparse points\n", "    if reparse_flags & ReparseFlags.IS_MICROSOFT:#a microsoft tag\n", "        if reparse_type is ReparseType.SYMLINK:\n", "            data = SymbolicLink.create_from_binary(binary_stream[cls._REPR.size:])\n", "        elif reparse_type is ReparseType.MOUNT_POINT:\n", "            data = JunctionOrMount.create_from_binary(binary_stream[cls._REPR.size:])\n", "        else:\n", "            data = binary_stream[cls._REPR.size:].tobytes()\n", "    else:\n", "        guid = UUID(bytes_le=binary_stream[cls._REPR.size:cls._REPR.size+16].tobytes())\n", "        data = binary_stream[cls._REPR.size+16:].tobytes()\n", "\n", "    nw_obj = cls((reparse_type, reparse_flags, data_len, guid, data))\n", "\n", "    _MOD_LOGGER.debug(\"Attempted to unpack REPARSE_POINT from \\\"%s\\\"\\nResult: %s\", binary_stream.tobytes(), nw_obj)\n", "\n", "    return nw_obj\n"], "language": "python", "code": "def _from_binary_reparse(cls, binary_stream):\n    \"\"\"\"\"\"\n    \"\"\" Reparse type flags - 4\n            Reparse tag - 4 bits\n            Reserved - 12 bits\n            Reparse type - 2 bits\n        Reparse data length - 2\n        Padding - 2\n    \"\"\"\n    reparse_tag, data_len = cls._REPR.unpack(binary_stream[:cls._REPR.size])\n    reparse_type = ReparseType(reparse_tag & 65535)\n    reparse_flags = ReparseFlags((reparse_tag & 4026531840) >> 28)\n    guid = None\n    if reparse_flags & ReparseFlags.IS_MICROSOFT:\n        if reparse_type is ReparseType.SYMLINK:\n            data = SymbolicLink.create_from_binary(binary_stream[cls._REPR.\n                size:])\n        elif reparse_type is ReparseType.MOUNT_POINT:\n            data = JunctionOrMount.create_from_binary(binary_stream[cls.\n                _REPR.size:])\n        else:\n            data = binary_stream[cls._REPR.size:].tobytes()\n    else:\n        guid = UUID(bytes_le=binary_stream[cls._REPR.size:cls._REPR.size + \n            16].tobytes())\n        data = binary_stream[cls._REPR.size + 16:].tobytes()\n    nw_obj = cls((reparse_type, reparse_flags, data_len, guid, data))\n    _MOD_LOGGER.debug('Attempted to unpack REPARSE_POINT from \"%s\"\\nResult: %s'\n        , binary_stream.tobytes(), nw_obj)\n    return nw_obj\n", "code_tokens": ["from", "binary", "reparse", "cls", "binary", "stream", "reparse", "type", "flags", "4", "reparse", "tag", "4", "bits", "reserved", "12", "bits", "reparse", "type", "2", "bits", "reparse", "data", "length", "2", "padding", "2", "reparse", "tag", "data", "len", "cls", "repr", "unpack", "binary", "stream", "cls", "repr", "size", "reparse", "type", "reparsetype", "reparse", "tag", "65535", "reparse", "flags", "reparseflags", "reparse", "tag", "4026531840", "28", "guid", "none", "if", "reparse", "flags", "reparseflags", "is", "microsoft", "if", "reparse", "type", "is", "reparsetype", "symlink", "data", "symboliclink", "create", "from", "binary", "binary", "stream", "cls", "repr", "size", "elif", "reparse", "type", "is", "reparsetype", "mount", "point", "data", "junctionormount", "create", "from", "binary", "binary", "stream", "cls", "repr", "size", "else", "data", "binary", "stream", "cls", "repr", "size", "tobytes", "else", "guid", "uuid", "bytes", "le", "binary", "stream", "cls", "repr", "size", "cls", "repr", "size", "16", "tobytes", "data", "binary", "stream", "cls", "repr", "size", "16", "tobytes", "nw", "obj", "cls", "reparse", "type", "reparse", "flags", "data", "len", "guid", "data", "mod", "logger", "debug", "attempted", "to", "unpack", "reparse", "point", "from", "nresult", "binary", "stream", "tobytes", "nw", "obj", "return", "nw", "obj"], "docstring": "parse binary file to custom class", "docstring_tokens": ["parse", "binary", "file", "to", "custom", "class"], "idx": 282}
{"url": "https://github.com/fossasia/knittingpattern/blob/8e608896b0ab82fea1ca9fbfa2b4ee023d8c8027/knittingpattern/Dumper/file.py#L109-L114", "repo": "knittingpattern", "func_name": "binary_file", "original_string": ["    def binary_file(self, file=None):\n", "        \"\"\"Same as :meth:`file` but for binary content.\"\"\"\n", "        if file is None:\n", "            file = BytesIO()\n", "        self._binary_file(file)\n", "        return file\n"], "language": "python", "code": "def binary_file(self, file=None):\n    \"\"\"\"\"\"\n    if file is None:\n        file = BytesIO()\n    self._binary_file(file)\n    return file\n", "code_tokens": ["binary", "file", "self", "file", "none", "if", "file", "is", "none", "file", "bytesio", "self", "binary", "file", "file", "return", "file"], "docstring": "parse binary file to custom class", "docstring_tokens": ["parse", "binary", "file", "to", "custom", "class"], "idx": 283}
{"url": "https://github.com/GreatFruitOmsk/tailhead/blob/a3b1324a39935f8ffcfda59328a9a458672889d9/tailhead/__init__.py#L72-L79", "repo": "tailhead", "func_name": "read", "original_string": ["    def read(self, read_size=-1):\n", "        \"\"\"\n", "        Read given number of bytes from file.\n", "        :param read_size: Number of bytes to read. -1 to read all.\n", "        :return: Number of bytes read and data that was read.\n", "        \"\"\"\n", "        read_str = self.file.read(read_size)\n", "        return len(read_str), read_str\n"], "language": "python", "code": "def read(self, read_size=-1):\n    \"\"\"\"\"\"\n    read_str = self.file.read(read_size)\n    return len(read_str), read_str\n", "code_tokens": ["read", "self", "read", "size", "1", "read", "str", "self", "file", "read", "read", "size", "return", "len", "read", "str", "read", "str"], "docstring": "read properties file", "docstring_tokens": ["read", "properties", "file"], "idx": 284}
{"url": "https://github.com/markreidvfx/pyaaf2/blob/37de8c10d3c3495cc00c705eb6c5048bc4a7e51f/tools/model_dump.py#L99-L127", "repo": "pyaaf2", "func_name": "read_properties", "original_string": ["\n", "def read_properties(entry):\n", "    stream = entry.get('properties')\n", "    if stream is None:\n", "        raise Exception(\"can not find properties\")\n", "\n", "    s = stream.open()\n", "    # read the whole stream\n", "    f = BytesIO(s.read())\n", "\n", "    byte_order = read_u8(f)\n", "    if byte_order != 0x4c:\n", "        raise NotImplementedError(\"be byteorder\")\n", "    version = read_u8(f)\n", "    entry_count = read_u16le(f)\n", "\n", "    props = []\n", "    for i in range(entry_count):\n", "        pid = read_u16le(f)\n", "        format = read_u16le(f)\n", "        byte_size = read_u16le(f)\n", "\n", "        props.append([pid, format, byte_size])\n", "\n", "    property_entries = {}\n", "    for pid, format, byte_size in props:\n", "        data = f.read(byte_size)\n", "        property_entries[pid] = data\n", "\n"], "language": "python", "code": "def read_properties(entry):\n    stream = entry.get('properties')\n    if stream is None:\n        raise Exception('can not find properties')\n    s = stream.open()\n    f = BytesIO(s.read())\n    byte_order = read_u8(f)\n    if byte_order != 76:\n        raise NotImplementedError('be byteorder')\n    version = read_u8(f)\n    entry_count = read_u16le(f)\n    props = []\n    for i in range(entry_count):\n        pid = read_u16le(f)\n        format = read_u16le(f)\n        byte_size = read_u16le(f)\n        props.append([pid, format, byte_size])\n    property_entries = {}\n    for pid, format, byte_size in props:\n        data = f.read(byte_size)\n        property_entries[pid] = data\n", "code_tokens": ["read", "properties", "entry", "stream", "entry", "get", "properties", "if", "stream", "is", "none", "raise", "exception", "can", "not", "find", "properties", "stream", "open", "bytesio", "read", "byte", "order", "read", "if", "byte", "order", "76", "raise", "notimplementederror", "be", "byteorder", "version", "read", "entry", "count", "read", "props", "for", "in", "range", "entry", "count", "pid", "read", "format", "read", "byte", "size", "read", "props", "append", "pid", "format", "byte", "size", "property", "entries", "for", "pid", "format", "byte", "size", "in", "props", "data", "read", "byte", "size", "property", "entries", "pid", "data"], "docstring": "read properties file", "docstring_tokens": ["read", "properties", "file"], "idx": 285}
{"url": "https://github.com/piglei/uwsgi-sloth/blob/2834ac5ed17d89ca5f19151c649ac610f6f37bd1/uwsgi_sloth/tailer.py#L45-L51", "repo": "uwsgi-sloth", "func_name": "read", "original_string": ["    def read(self, read_size=None):\n", "        if read_size:\n", "            read_str = self.file.read(read_size)\n", "        else:\n", "            read_str = self.file.read()\n", "\n", "        return len(read_str), read_str\n"], "language": "python", "code": "def read(self, read_size=None):\n    if read_size:\n        read_str = self.file.read(read_size)\n    else:\n        read_str = self.file.read()\n    return len(read_str), read_str\n", "code_tokens": ["read", "self", "read", "size", "none", "if", "read", "size", "read", "str", "self", "file", "read", "read", "size", "else", "read", "str", "self", "file", "read", "return", "len", "read", "str", "read", "str"], "docstring": "read properties file", "docstring_tokens": ["read", "properties", "file"], "idx": 286}
{"url": "https://github.com/readbeyond/aeneas/blob/9d95535ad63eef4a98530cfdff033b8c35315ee1/aeneas/audiofile.py#L330-L376", "repo": "aeneas", "func_name": "read_properties", "original_string": ["    def read_properties(self):\n", "        \"\"\"\n", "        Populate this object by reading\n", "        the audio properties of the file at the given path.\n", "\n", "        Currently this function uses\n", "        :class:`~aeneas.ffprobewrapper.FFPROBEWrapper`\n", "        to get the audio file properties.\n", "\n", "        :raises: :class:`~aeneas.audiofile.AudioFileProbeError`: if the path to the ``ffprobe`` executable cannot be called\n", "        :raises: :class:`~aeneas.audiofile.AudioFileUnsupportedFormatError`: if the audio file has a format not supported\n", "        :raises: OSError: if the audio file cannot be read\n", "        \"\"\"\n", "        self.log(u\"Reading properties...\")\n", "\n", "        # check the file can be read\n", "        if not gf.file_can_be_read(self.file_path):\n", "            self.log_exc(u\"File '%s' cannot be read\" % (self.file_path), None, True, OSError)\n", "\n", "        # get the file size\n", "        self.log([u\"Getting file size for '%s'\", self.file_path])\n", "        self.file_size = gf.file_size(self.file_path)\n", "        self.log([u\"File size for '%s' is '%d'\", self.file_path, self.file_size])\n", "\n", "        # get the audio properties using FFPROBEWrapper\n", "        try:\n", "            self.log(u\"Reading properties with FFPROBEWrapper...\")\n", "            properties = FFPROBEWrapper(\n", "                rconf=self.rconf,\n", "                logger=self.logger\n", "            ).read_properties(self.file_path)\n", "            self.log(u\"Reading properties with FFPROBEWrapper... done\")\n", "        except FFPROBEPathError:\n", "            self.log_exc(u\"Unable to call ffprobe executable\", None, True, AudioFileProbeError)\n", "        except (FFPROBEUnsupportedFormatError, FFPROBEParsingError):\n", "            self.log_exc(u\"Audio file format not supported by ffprobe\", None, True, AudioFileUnsupportedFormatError)\n", "\n", "        # save relevant properties in results inside the audiofile object\n", "        self.audio_length = TimeValue(properties[FFPROBEWrapper.STDOUT_DURATION])\n", "        self.audio_format = properties[FFPROBEWrapper.STDOUT_CODEC_NAME]\n", "        self.audio_sample_rate = gf.safe_int(properties[FFPROBEWrapper.STDOUT_SAMPLE_RATE])\n", "        self.audio_channels = gf.safe_int(properties[FFPROBEWrapper.STDOUT_CHANNELS])\n", "        self.log([u\"Stored audio_length: '%s'\", self.audio_length])\n", "        self.log([u\"Stored audio_format: '%s'\", self.audio_format])\n", "        self.log([u\"Stored audio_sample_rate: '%s'\", self.audio_sample_rate])\n", "        self.log([u\"Stored audio_channels: '%s'\", self.audio_channels])\n", "        self.log(u\"Reading properties... done\")\n"], "language": "python", "code": "def read_properties(self):\n    \"\"\"\"\"\"\n    self.log(u'Reading properties...')\n    if not gf.file_can_be_read(self.file_path):\n        self.log_exc(u\"File '%s' cannot be read\" % self.file_path, None, \n            True, OSError)\n    self.log([u\"Getting file size for '%s'\", self.file_path])\n    self.file_size = gf.file_size(self.file_path)\n    self.log([u\"File size for '%s' is '%d'\", self.file_path, self.file_size])\n    try:\n        self.log(u'Reading properties with FFPROBEWrapper...')\n        properties = FFPROBEWrapper(rconf=self.rconf, logger=self.logger\n            ).read_properties(self.file_path)\n        self.log(u'Reading properties with FFPROBEWrapper... done')\n    except FFPROBEPathError:\n        self.log_exc(u'Unable to call ffprobe executable', None, True,\n            AudioFileProbeError)\n    except (FFPROBEUnsupportedFormatError, FFPROBEParsingError):\n        self.log_exc(u'Audio file format not supported by ffprobe', None, \n            True, AudioFileUnsupportedFormatError)\n    self.audio_length = TimeValue(properties[FFPROBEWrapper.STDOUT_DURATION])\n    self.audio_format = properties[FFPROBEWrapper.STDOUT_CODEC_NAME]\n    self.audio_sample_rate = gf.safe_int(properties[FFPROBEWrapper.\n        STDOUT_SAMPLE_RATE])\n    self.audio_channels = gf.safe_int(properties[FFPROBEWrapper.\n        STDOUT_CHANNELS])\n    self.log([u\"Stored audio_length: '%s'\", self.audio_length])\n    self.log([u\"Stored audio_format: '%s'\", self.audio_format])\n    self.log([u\"Stored audio_sample_rate: '%s'\", self.audio_sample_rate])\n    self.log([u\"Stored audio_channels: '%s'\", self.audio_channels])\n    self.log(u'Reading properties... done')\n", "code_tokens": ["read", "properties", "self", "self", "log", "reading", "properties", "if", "not", "gf", "file", "can", "be", "read", "self", "file", "path", "self", "log", "exc", "file", "cannot", "be", "read", "self", "file", "path", "none", "true", "oserror", "self", "log", "getting", "file", "size", "for", "self", "file", "path", "self", "file", "size", "gf", "file", "size", "self", "file", "path", "self", "log", "file", "size", "for", "is", "self", "file", "path", "self", "file", "size", "try", "self", "log", "reading", "properties", "with", "ffprobewrapper", "properties", "ffprobewrapper", "rconf", "self", "rconf", "logger", "self", "logger", "read", "properties", "self", "file", "path", "self", "log", "reading", "properties", "with", "ffprobewrapper", "done", "except", "ffprobepatherror", "self", "log", "exc", "unable", "to", "call", "ffprobe", "executable", "none", "true", "audiofileprobeerror", "except", "ffprobeunsupportedformaterror", "ffprobeparsingerror", "self", "log", "exc", "audio", "file", "format", "not", "supported", "by", "ffprobe", "none", "true", "audiofileunsupportedformaterror", "self", "audio", "length", "timevalue", "properties", "ffprobewrapper", "stdout", "duration", "self", "audio", "format", "properties", "ffprobewrapper", "stdout", "codec", "name", "self", "audio", "sample", "rate", "gf", "safe", "int", "properties", "ffprobewrapper", "stdout", "sample", "rate", "self", "audio", "channels", "gf", "safe", "int", "properties", "ffprobewrapper", "stdout", "channels", "self", "log", "stored", "audio", "length", "self", "audio", "length", "self", "log", "stored", "audio", "format", "self", "audio", "format", "self", "log", "stored", "audio", "sample", "rate", "self", "audio", "sample", "rate", "self", "log", "stored", "audio", "channels", "self", "audio", "channels", "self", "log", "reading", "properties", "done"], "docstring": "read properties file", "docstring_tokens": ["read", "properties", "file"], "idx": 287}
{"url": "https://github.com/rcook/upload-haddocks/blob/a33826be1873da68ba073a42ec828c8ec150d576/setup.py#L16-L26", "repo": "upload-haddocks", "func_name": "_read_properties", "original_string": ["def _read_properties():\n", "    init_path = os.path.abspath(os.path.join(\"uploadhaddocks\", \"__init__.py\"))\n", "    regex = re.compile(\"^\\\\s*__(?P<key>.*)__\\\\s*=\\\\s*\\\"(?P<value>.*)\\\"\\\\s*$\")\n", "    with open(init_path, \"rt\") as f:\n", "        props = {}\n", "        for line in f.readlines():\n", "            m = regex.match(line)\n", "            if m is not None:\n", "                props[m.group(\"key\")] = m.group(\"value\")\n", "\n", "    return props\n"], "language": "python", "code": "def _read_properties():\n    init_path = os.path.abspath(os.path.join('uploadhaddocks', '__init__.py'))\n    regex = re.compile('^\\\\s*__(?P<key>.*)__\\\\s*=\\\\s*\"(?P<value>.*)\"\\\\s*$')\n    with open(init_path, 'rt') as f:\n        props = {}\n        for line in f.readlines():\n            m = regex.match(line)\n            if m is not None:\n                props[m.group('key')] = m.group('value')\n    return props\n", "code_tokens": ["read", "properties", "init", "path", "os", "path", "abspath", "os", "path", "join", "uploadhaddocks", "init", "py", "regex", "re", "compile", "key", "value", "with", "open", "init", "path", "rt", "as", "props", "for", "line", "in", "readlines", "regex", "match", "line", "if", "is", "not", "none", "props", "group", "key", "group", "value", "return", "props"], "docstring": "read properties file", "docstring_tokens": ["read", "properties", "file"], "idx": 288}
{"url": "https://github.com/oscarbranson/latools/blob/cd25a650cfee318152f234d992708511f7047fbe/latools/filtering/clustering.py#L35-L56", "repo": "latools", "func_name": "cluster_kmeans", "original_string": ["def cluster_kmeans(data, n_clusters, **kwargs):\n", "    \"\"\"\n", "    Identify clusters using K - Means algorithm.\n", "\n", "    Parameters\n", "    ----------\n", "    data : array_like\n", "        array of size [n_samples, n_features].\n", "    n_clusters : int\n", "        The number of clusters expected in the data.\n", "\n", "    Returns\n", "    -------\n", "    dict\n", "        boolean array for each identified cluster.\n", "    \"\"\"\n", "    km = cl.KMeans(n_clusters, **kwargs)\n", "    kmf = km.fit(data)\n", "\n", "    labels = kmf.labels_\n", "\n", "    return labels, [np.nan]\n"], "language": "python", "code": "def cluster_kmeans(data, n_clusters, **kwargs):\n    \"\"\"\"\"\"\n    km = cl.KMeans(n_clusters, **kwargs)\n    kmf = km.fit(data)\n    labels = kmf.labels_\n    return labels, [np.nan]\n", "code_tokens": ["cluster", "kmeans", "data", "clusters", "kwargs", "km", "cl", "kmeans", "clusters", "kwargs", "kmf", "km", "fit", "data", "labels", "kmf", "labels", "return", "labels", "np", "nan"], "docstring": "k means clustering", "docstring_tokens": ["k", "means", "clustering"], "idx": 289}
{"url": "https://github.com/insilicolife/micti/blob/f12f46724295b57c4859e6acf7eab580fc355eb1/build/lib/MICTI/GM.py#L71-L87", "repo": "micti", "func_name": "setMeans", "original_string": ["    def setMeans(self):\n", "        #np.random.seed(5)\n", "        num_clusters = self.k\n", "\n", "        # Use scikit-learn's k-means to simplify workflow\n", "        #kmeans_model = KMeans(n_clusters=num_clusters, n_init=5, max_iter=400, random_state=1, n_jobs=-1) # uncomment to use parallelism -- may break on your installation\n", "        if self.initial_kmean_Model == None:\n", "            kmeans_model = self.initial_cluster_assignment()\n", "            centroids, cluster_assignment = kmeans_model.kmeans(10)\n", "        else:\n", "            centroids=self.initial_kmean_Model.centroids\n", "            cluster_assignment=self.initial_kmean_Model.cluster_assignment\n", "            \n", "        means = [centroid for centroid in centroids]\n", "        self.means=means\n", "        self.cluster_assignment=cluster_assignment\n", "        return\n"], "language": "python", "code": "def setMeans(self):\n    num_clusters = self.k\n    if self.initial_kmean_Model == None:\n        kmeans_model = self.initial_cluster_assignment()\n        centroids, cluster_assignment = kmeans_model.kmeans(10)\n    else:\n        centroids = self.initial_kmean_Model.centroids\n        cluster_assignment = self.initial_kmean_Model.cluster_assignment\n    means = [centroid for centroid in centroids]\n    self.means = means\n    self.cluster_assignment = cluster_assignment\n    return\n", "code_tokens": ["setmeans", "self", "num", "clusters", "self", "if", "self", "initial", "kmean", "model", "none", "kmeans", "model", "self", "initial", "cluster", "assignment", "centroids", "cluster", "assignment", "kmeans", "model", "kmeans", "10", "else", "centroids", "self", "initial", "kmean", "model", "centroids", "cluster", "assignment", "self", "initial", "kmean", "model", "cluster", "assignment", "means", "centroid", "for", "centroid", "in", "centroids", "self", "means", "means", "self", "cluster", "assignment", "cluster", "assignment", "return"], "docstring": "k means clustering", "docstring_tokens": ["k", "means", "clustering"], "idx": 290}
{"url": "https://github.com/szairis/sakmapper/blob/ac462fd2674e6aa1aa3b209222d8ac4e9268a790/sakmapper/network.py#L111-L170", "repo": "sakmapper", "func_name": "optimal_clustering", "original_string": ["def optimal_clustering(df, patch, method='kmeans', statistic='gap', max_K=5):\n", "    if len(patch) == 1:\n", "        return [patch]\n", "\n", "    if statistic == 'db':\n", "        if method == 'kmeans':\n", "            if len(patch) <= 5:\n", "                K_max = 2\n", "            else:\n", "                K_max = min(len(patch) / 2, max_K)\n", "            clustering = {}\n", "            db_index = []\n", "            X = df.ix[patch, :]\n", "            for k in range(2, K_max + 1):\n", "                kmeans = cluster.KMeans(n_clusters=k).fit(X)\n", "                clustering[k] = pd.DataFrame(kmeans.predict(X), index=patch)\n", "                dist_mu = squareform(pdist(kmeans.cluster_centers_))\n", "                sigma = []\n", "                for i in range(k):\n", "                    points_in_cluster = clustering[k][clustering[k][0] == i].index\n", "                    sigma.append(sqrt(X.ix[points_in_cluster, :].var(axis=0).sum()))\n", "                db_index.append(davies_bouldin(dist_mu, np.array(sigma)))\n", "            db_index = np.array(db_index)\n", "            k_optimal = np.argmin(db_index) + 2\n", "            return [list(clustering[k_optimal][clustering[k_optimal][0] == i].index) for i in range(k_optimal)]\n", "\n", "        elif method == 'agglomerative':\n", "            if len(patch) <= 5:\n", "                K_max = 2\n", "            else:\n", "                K_max = min(len(patch) / 2, max_K)\n", "            clustering = {}\n", "            db_index = []\n", "            X = df.ix[patch, :]\n", "            for k in range(2, K_max + 1):\n", "                agglomerative = cluster.AgglomerativeClustering(n_clusters=k, linkage='average').fit(X)\n", "                clustering[k] = pd.DataFrame(agglomerative.fit_predict(X), index=patch)\n", "                tmp = [list(clustering[k][clustering[k][0] == i].index) for i in range(k)]\n", "                centers = np.array([np.mean(X.ix[c, :], axis=0) for c in tmp])\n", "                dist_mu = squareform(pdist(centers))\n", "                sigma = []\n", "                for i in range(k):\n", "                    points_in_cluster = clustering[k][clustering[k][0] == i].index\n", "                    sigma.append(sqrt(X.ix[points_in_cluster, :].var(axis=0).sum()))\n", "                db_index.append(davies_bouldin(dist_mu, np.array(sigma)))\n", "            db_index = np.array(db_index)\n", "            k_optimal = np.argmin(db_index) + 2\n", "            return [list(clustering[k_optimal][clustering[k_optimal][0] == i].index) for i in range(k_optimal)]\n", "\n", "    elif statistic == 'gap':\n", "        X = np.array(df.ix[patch, :])\n", "        if method == 'kmeans':\n", "            f = cluster.KMeans\n", "        gaps = gap(X, ks=range(1, min(max_K, len(patch))), method=f)\n", "        k_optimal = list(gaps).index(max(gaps))+1\n", "        clustering = pd.DataFrame(f(n_clusters=k_optimal).fit_predict(X), index=patch)\n", "        return [list(clustering[clustering[0] == i].index) for i in range(k_optimal)]\n", "\n", "    else:\n", "        raise 'error: only db and gat statistics are supported'\n"], "language": "python", "code": "def optimal_clustering(df, patch, method='kmeans', statistic='gap', max_K=5):\n    if len(patch) == 1:\n        return [patch]\n    if statistic == 'db':\n        if method == 'kmeans':\n            if len(patch) <= 5:\n                K_max = 2\n            else:\n                K_max = min(len(patch) / 2, max_K)\n            clustering = {}\n            db_index = []\n            X = df.ix[patch, :]\n            for k in range(2, K_max + 1):\n                kmeans = cluster.KMeans(n_clusters=k).fit(X)\n                clustering[k] = pd.DataFrame(kmeans.predict(X), index=patch)\n                dist_mu = squareform(pdist(kmeans.cluster_centers_))\n                sigma = []\n                for i in range(k):\n                    points_in_cluster = clustering[k][clustering[k][0] == i\n                        ].index\n                    sigma.append(sqrt(X.ix[points_in_cluster, :].var(axis=0\n                        ).sum()))\n                db_index.append(davies_bouldin(dist_mu, np.array(sigma)))\n            db_index = np.array(db_index)\n            k_optimal = np.argmin(db_index) + 2\n            return [list(clustering[k_optimal][clustering[k_optimal][0] ==\n                i].index) for i in range(k_optimal)]\n        elif method == 'agglomerative':\n            if len(patch) <= 5:\n                K_max = 2\n            else:\n                K_max = min(len(patch) / 2, max_K)\n            clustering = {}\n            db_index = []\n            X = df.ix[patch, :]\n            for k in range(2, K_max + 1):\n                agglomerative = cluster.AgglomerativeClustering(n_clusters=\n                    k, linkage='average').fit(X)\n                clustering[k] = pd.DataFrame(agglomerative.fit_predict(X),\n                    index=patch)\n                tmp = [list(clustering[k][clustering[k][0] == i].index) for\n                    i in range(k)]\n                centers = np.array([np.mean(X.ix[c, :], axis=0) for c in tmp])\n                dist_mu = squareform(pdist(centers))\n                sigma = []\n                for i in range(k):\n                    points_in_cluster = clustering[k][clustering[k][0] == i\n                        ].index\n                    sigma.append(sqrt(X.ix[points_in_cluster, :].var(axis=0\n                        ).sum()))\n                db_index.append(davies_bouldin(dist_mu, np.array(sigma)))\n            db_index = np.array(db_index)\n            k_optimal = np.argmin(db_index) + 2\n            return [list(clustering[k_optimal][clustering[k_optimal][0] ==\n                i].index) for i in range(k_optimal)]\n    elif statistic == 'gap':\n        X = np.array(df.ix[patch, :])\n        if method == 'kmeans':\n            f = cluster.KMeans\n        gaps = gap(X, ks=range(1, min(max_K, len(patch))), method=f)\n        k_optimal = list(gaps).index(max(gaps)) + 1\n        clustering = pd.DataFrame(f(n_clusters=k_optimal).fit_predict(X),\n            index=patch)\n        return [list(clustering[clustering[0] == i].index) for i in range(\n            k_optimal)]\n    else:\n        raise 'error: only db and gat statistics are supported'\n", "code_tokens": ["optimal", "clustering", "df", "patch", "method", "kmeans", "statistic", "gap", "max", "5", "if", "len", "patch", "1", "return", "patch", "if", "statistic", "db", "if", "method", "kmeans", "if", "len", "patch", "5", "max", "2", "else", "max", "min", "len", "patch", "2", "max", "clustering", "db", "index", "df", "ix", "patch", "for", "in", "range", "2", "max", "1", "kmeans", "cluster", "kmeans", "clusters", "fit", "clustering", "pd", "dataframe", "kmeans", "predict", "index", "patch", "dist", "mu", "squareform", "pdist", "kmeans", "cluster", "centers", "sigma", "for", "in", "range", "points", "in", "cluster", "clustering", "clustering", "0", "index", "sigma", "append", "sqrt", "ix", "points", "in", "cluster", "var", "axis", "0", "sum", "db", "index", "append", "davies", "bouldin", "dist", "mu", "np", "array", "sigma", "db", "index", "np", "array", "db", "index", "optimal", "np", "argmin", "db", "index", "2", "return", "list", "clustering", "optimal", "clustering", "optimal", "0", "index", "for", "in", "range", "optimal", "elif", "method", "agglomerative", "if", "len", "patch", "5", "max", "2", "else", "max", "min", "len", "patch", "2", "max", "clustering", "db", "index", "df", "ix", "patch", "for", "in", "range", "2", "max", "1", "agglomerative", "cluster", "agglomerativeclustering", "clusters", "linkage", "average", "fit", "clustering", "pd", "dataframe", "agglomerative", "fit", "predict", "index", "patch", "tmp", "list", "clustering", "clustering", "0", "index", "for", "in", "range", "centers", "np", "array", "np", "mean", "ix", "axis", "0", "for", "in", "tmp", "dist", "mu", "squareform", "pdist", "centers", "sigma", "for", "in", "range", "points", "in", "cluster", "clustering", "clustering", "0", "index", "sigma", "append", "sqrt", "ix", "points", "in", "cluster", "var", "axis", "0", "sum", "db", "index", "append", "davies", "bouldin", "dist", "mu", "np", "array", "sigma", "db", "index", "np", "array", "db", "index", "optimal", "np", "argmin", "db", "index", "2", "return", "list", "clustering", "optimal", "clustering", "optimal", "0", "index", "for", "in", "range", "optimal", "elif", "statistic", "gap", "np", "array", "df", "ix", "patch", "if", "method", "kmeans", "cluster", "kmeans", "gaps", "gap", "ks", "range", "1", "min", "max", "len", "patch", "method", "optimal", "list", "gaps", "index", "max", "gaps", "1", "clustering", "pd", "dataframe", "clusters", "optimal", "fit", "predict", "index", "patch", "return", "list", "clustering", "clustering", "0", "index", "for", "in", "range", "optimal", "else", "raise", "error", "only", "db", "and", "gat", "statistics", "are", "supported"], "docstring": "k means clustering", "docstring_tokens": ["k", "means", "clustering"], "idx": 291}
{"url": "https://github.com/DEIB-GECO/PyGMQL/blob/e58b2f9402a86056dcda484a32e3de0bb06ed991/gmql/ml/algorithms/clustering.py#L307-L350", "repo": "PyGMQL", "func_name": "elbow_method", "original_string": ["    def elbow_method(data, k_min, k_max, distance='euclidean'):\n", "        \"\"\"\n", "        Calculates and plots the plot of variance explained - number of clusters\n", "        Implementation reference: https://github.com/sarguido/k-means-clustering.rst\n", "\n", "        :param data: The dataset\n", "        :param k_min: lowerbound of the cluster range\n", "        :param k_max: upperbound of the cluster range\n", "        :param distance: the distance metric, 'euclidean' by default\n", "        :return:\n", "        \"\"\"\n", "        # Determine your k range\n", "        k_range = range(k_min, k_max)\n", "\n", "        # Fit the kmeans model for each n_clusters = k\n", "        k_means_var = [Clustering.kmeans(k).fit(data) for k in k_range]\n", "\n", "        # Pull out the cluster centers for each model\n", "        centroids = [X.model.cluster_centers_ for X in k_means_var]\n", "\n", "        # Calculate the Euclidean distance from\n", "        # each point to each cluster center\n", "        k_euclid = [cdist(data, cent, distance) for cent in centroids]\n", "        dist = [np.min(ke, axis=1) for ke in k_euclid]\n", "\n", "        # Total within-cluster sum of squares\n", "        wcss = [sum(d ** 2) for d in dist]\n", "\n", "        # The total sum of squares\n", "        tss = sum(pdist(data) ** 2) / data.shape[0]\n", "\n", "        # The between-cluster sum of squares\n", "        bss = tss - wcss\n", "\n", "        # elbow curve\n", "        fig = plt.figure()\n", "        ax = fig.add_subplot(111)\n", "        ax.plot(k_range, bss / tss * 100, 'b*-')\n", "        ax.set_ylim((0, 100))\n", "        plt.grid(True)\n", "        plt.xlabel('n_clusters')\n", "        plt.ylabel('Percentage of variance explained')\n", "        plt.title('Variance Explained vs. k')\n", "        plt.show()\n"], "language": "python", "code": "def elbow_method(data, k_min, k_max, distance='euclidean'):\n    \"\"\"\"\"\"\n    k_range = range(k_min, k_max)\n    k_means_var = [Clustering.kmeans(k).fit(data) for k in k_range]\n    centroids = [X.model.cluster_centers_ for X in k_means_var]\n    k_euclid = [cdist(data, cent, distance) for cent in centroids]\n    dist = [np.min(ke, axis=1) for ke in k_euclid]\n    wcss = [sum(d ** 2) for d in dist]\n    tss = sum(pdist(data) ** 2) / data.shape[0]\n    bss = tss - wcss\n    fig = plt.figure()\n    ax = fig.add_subplot(111)\n    ax.plot(k_range, bss / tss * 100, 'b*-')\n    ax.set_ylim((0, 100))\n    plt.grid(True)\n    plt.xlabel('n_clusters')\n    plt.ylabel('Percentage of variance explained')\n    plt.title('Variance Explained vs. k')\n    plt.show()\n", "code_tokens": ["elbow", "method", "data", "min", "max", "distance", "euclidean", "range", "range", "min", "max", "means", "var", "clustering", "kmeans", "fit", "data", "for", "in", "range", "centroids", "model", "cluster", "centers", "for", "in", "means", "var", "euclid", "cdist", "data", "cent", "distance", "for", "cent", "in", "centroids", "dist", "np", "min", "ke", "axis", "1", "for", "ke", "in", "euclid", "wcss", "sum", "2", "for", "in", "dist", "tss", "sum", "pdist", "data", "2", "data", "shape", "0", "bss", "tss", "wcss", "fig", "plt", "figure", "ax", "fig", "add", "subplot", "111", "ax", "plot", "range", "bss", "tss", "100", "ax", "set", "ylim", "0", "100", "plt", "grid", "true", "plt", "xlabel", "clusters", "plt", "ylabel", "percentage", "of", "variance", "explained", "plt", "title", "variance", "explained", "vs", "plt", "show"], "docstring": "k means clustering", "docstring_tokens": ["k", "means", "clustering"], "idx": 292}
{"url": "https://github.com/vmirly/pyclust/blob/bdb12be4649e70c6c90da2605bc5f4b314e2d07e/pyclust/_kmeans.py#L78-L101", "repo": "pyclust", "func_name": "_kmeans", "original_string": ["def _kmeans(X, n_clusters, max_iter, n_trials, tol):\n", "    \"\"\" Run multiple trials of k-means clustering,\n", "        and outputt he best centers, and cluster labels\n", "    \"\"\"\n", "    n_samples, n_features = X.shape[0], X.shape[1]\n", "\n", "    centers_best = np.empty(shape=(n_clusters,n_features), dtype=float)\n", "    labels_best  = np.empty(shape=n_samples, dtype=int)\n", "    for i in range(n_trials):\n", "        centers, labels, sse_tot, sse_arr, n_iter  = _kmeans_run(X, n_clusters, max_iter, tol)\n", "        if i==0:\n", "            sse_tot_best = sse_tot\n", "            sse_arr_best = sse_arr\n", "            n_iter_best = n_iter\n", "            centers_best = centers.copy()\n", "            labels_best  = labels.copy()\n", "        if sse_tot < sse_tot_best:\n", "            sse_tot_best = sse_tot\n", "            sse_arr_best = sse_arr\n", "            n_iter_best = n_iter\n", "            centers_best = centers.copy()\n", "            labels_best  = labels.copy()\n", "\n", "    return(centers_best, labels_best, sse_arr_best, n_iter_best)\n"], "language": "python", "code": "def _kmeans(X, n_clusters, max_iter, n_trials, tol):\n    \"\"\"\"\"\"\n    n_samples, n_features = X.shape[0], X.shape[1]\n    centers_best = np.empty(shape=(n_clusters, n_features), dtype=float)\n    labels_best = np.empty(shape=n_samples, dtype=int)\n    for i in range(n_trials):\n        centers, labels, sse_tot, sse_arr, n_iter = _kmeans_run(X,\n            n_clusters, max_iter, tol)\n        if i == 0:\n            sse_tot_best = sse_tot\n            sse_arr_best = sse_arr\n            n_iter_best = n_iter\n            centers_best = centers.copy()\n            labels_best = labels.copy()\n        if sse_tot < sse_tot_best:\n            sse_tot_best = sse_tot\n            sse_arr_best = sse_arr\n            n_iter_best = n_iter\n            centers_best = centers.copy()\n            labels_best = labels.copy()\n    return centers_best, labels_best, sse_arr_best, n_iter_best\n", "code_tokens": ["kmeans", "clusters", "max", "iter", "trials", "tol", "samples", "features", "shape", "0", "shape", "1", "centers", "best", "np", "empty", "shape", "clusters", "features", "dtype", "float", "labels", "best", "np", "empty", "shape", "samples", "dtype", "int", "for", "in", "range", "trials", "centers", "labels", "sse", "tot", "sse", "arr", "iter", "kmeans", "run", "clusters", "max", "iter", "tol", "if", "0", "sse", "tot", "best", "sse", "tot", "sse", "arr", "best", "sse", "arr", "iter", "best", "iter", "centers", "best", "centers", "copy", "labels", "best", "labels", "copy", "if", "sse", "tot", "sse", "tot", "best", "sse", "tot", "best", "sse", "tot", "sse", "arr", "best", "sse", "arr", "iter", "best", "iter", "centers", "best", "centers", "copy", "labels", "best", "labels", "copy", "return", "centers", "best", "labels", "best", "sse", "arr", "best", "iter", "best"], "docstring": "k means clustering", "docstring_tokens": ["k", "means", "clustering"], "idx": 293}
{"url": "https://github.com/VIVelev/PyDojoML/blob/773fdce6866aa6decd306a5a85f94129fed816eb/dojo/cluster/hierarchical.py#L43-L62", "repo": "PyDojoML", "func_name": "cluster", "original_string": ["    def cluster(self, X):\n", "        X = super().cluster(X)\n", "        self._distances = linkage(X, method=self.linkage)\n", "\n", "        if self.mode == \"n_clusters\":\n", "            return fcluster(\n", "                self._distances,\n", "                self.n_clusters,\n", "                criterion=\"maxclust\"\n", "            )\n", "\n", "        elif self.mode == \"max_distance\":\n", "            return fcluster(\n", "                self._distances,\n", "                self.max_distance,\n", "                criterion=\"distance\"\n", "            )\n", "\n", "        else:\n", "            raise ParameterError(f\"Unknown / unsupported clustering mode: \\\"{self.mode}\\\"\")\n"], "language": "python", "code": "def cluster(self, X):\n    X = super().cluster(X)\n    self._distances = linkage(X, method=self.linkage)\n    if self.mode == 'n_clusters':\n        return fcluster(self._distances, self.n_clusters, criterion='maxclust')\n    elif self.mode == 'max_distance':\n        return fcluster(self._distances, self.max_distance, criterion=\n            'distance')\n    else:\n        raise ParameterError(\n            f'Unknown / unsupported clustering mode: \"{self.mode}\"')\n", "code_tokens": ["cluster", "self", "super", "cluster", "self", "distances", "linkage", "method", "self", "linkage", "if", "self", "mode", "clusters", "return", "fcluster", "self", "distances", "self", "clusters", "criterion", "maxclust", "elif", "self", "mode", "max", "distance", "return", "fcluster", "self", "distances", "self", "max", "distance", "criterion", "distance", "else", "raise", "parametererror", "unknown", "unsupported", "clustering", "mode", "self", "mode"], "docstring": "k means clustering", "docstring_tokens": ["k", "means", "clustering"], "idx": 294}
{"url": "https://github.com/neuropsychology/NeuroKit.py/blob/c9589348fbbde0fa7e986048c48f38e6b488adfe/examples/UnderDev/eeg/eeg_microstates.py#L201-L253", "repo": "NeuroKit.py", "func_name": "eeg_microstates_clustering", "original_string": ["def eeg_microstates_clustering(data, n_microstates=4, clustering_method=\"kmeans\", n_jobs=1, n_init=25, occurence_rejection_treshold=0.05, max_refitting=5, verbose=True):\n", "    \"\"\"\n", "    Fit the clustering algorithm.\n", "    \"\"\"\n", "    # Create training set\n", "    training_set = data.copy()\n", "\n", "    if verbose is True:\n", "        print(\"- Initializing the clustering algorithm...\")\n", "    if clustering_method == \"kmeans\":\n", "        algorithm = sklearn.cluster.KMeans(init='k-means++', n_clusters=n_microstates, n_init=n_init, n_jobs=n_jobs)\n", "    elif clustering_method == \"spectral\":\n", "        algorithm = sklearn.cluster.SpectralClustering(n_clusters=n_microstates, n_init=n_init, n_jobs=n_jobs)\n", "    elif clustering_method == \"agglom\":\n", "        algorithm = sklearn.cluster.AgglomerativeClustering(n_clusters=n_microstates, linkage=\"complete\")\n", "    elif clustering_method == \"dbscan\":\n", "        algorithm = sklearn.cluster.DBSCAN(min_samples=100)\n", "    elif clustering_method == \"affinity\":\n", "        algorithm = sklearn.cluster.AffinityPropagation(damping=0.5)\n", "    else:\n", "        print(\"NeuroKit Error: eeg_microstates(): clustering_method must be 'kmeans', 'spectral', 'dbscan', 'affinity' or 'agglom'\")\n", "\n", "\n", "    refitting = 0  # Initialize the number of refittings\n", "    good_fit_achieved = False\n", "    while good_fit_achieved is False:\n", "        good_fit_achieved = True\n", "        if verbose is True:\n", "            print(\"- Fitting the classifier...\")\n", "        # Fit the algorithm\n", "        algorithm.fit(training_set)\n", "\n", "        if verbose is True:\n", "            print(\"- Clustering back the initial data...\")\n", "        # Predict the more likely cluster for each observation\n", "        predicted = algorithm.fit_predict(training_set)\n", "\n", "        if verbose is True:\n", "            print(\"- Check for abnormalities...\")\n", "        # Check for abnormalities and prune the training set until none found\n", "        occurences = dict(collections.Counter(predicted))\n", "        masks = [np.array([True]*len(training_set))]\n", "        for microstate in occurences:\n", "            # is the frequency of one microstate inferior to a treshold\n", "            if occurences[microstate] < len(data)*occurence_rejection_treshold:\n", "                good_fit_achieved = False\n", "                refitting += 1  # Increment the refitting\n", "                print(\"NeuroKit Warning: eeg_microstates(): detected some outliers: refitting the classifier (n=\" + str(refitting) + \").\")\n", "                masks.append(predicted!=microstate)\n", "        mask = np.all(masks, axis=0)\n", "        training_set = training_set[mask]\n", "\n", "    return(algorithm)\n"], "language": "python", "code": "def eeg_microstates_clustering(data, n_microstates=4, clustering_method=\n    'kmeans', n_jobs=1, n_init=25, occurence_rejection_treshold=0.05,\n    max_refitting=5, verbose=True):\n    \"\"\"\"\"\"\n    training_set = data.copy()\n    if verbose is True:\n        print('- Initializing the clustering algorithm...')\n    if clustering_method == 'kmeans':\n        algorithm = sklearn.cluster.KMeans(init='k-means++', n_clusters=\n            n_microstates, n_init=n_init, n_jobs=n_jobs)\n    elif clustering_method == 'spectral':\n        algorithm = sklearn.cluster.SpectralClustering(n_clusters=\n            n_microstates, n_init=n_init, n_jobs=n_jobs)\n    elif clustering_method == 'agglom':\n        algorithm = sklearn.cluster.AgglomerativeClustering(n_clusters=\n            n_microstates, linkage='complete')\n    elif clustering_method == 'dbscan':\n        algorithm = sklearn.cluster.DBSCAN(min_samples=100)\n    elif clustering_method == 'affinity':\n        algorithm = sklearn.cluster.AffinityPropagation(damping=0.5)\n    else:\n        print(\n            \"NeuroKit Error: eeg_microstates(): clustering_method must be 'kmeans', 'spectral', 'dbscan', 'affinity' or 'agglom'\"\n            )\n    refitting = 0\n    good_fit_achieved = False\n    while good_fit_achieved is False:\n        good_fit_achieved = True\n        if verbose is True:\n            print('- Fitting the classifier...')\n        algorithm.fit(training_set)\n        if verbose is True:\n            print('- Clustering back the initial data...')\n        predicted = algorithm.fit_predict(training_set)\n        if verbose is True:\n            print('- Check for abnormalities...')\n        occurences = dict(collections.Counter(predicted))\n        masks = [np.array([True] * len(training_set))]\n        for microstate in occurences:\n            if occurences[microstate] < len(data\n                ) * occurence_rejection_treshold:\n                good_fit_achieved = False\n                refitting += 1\n                print(\n                    'NeuroKit Warning: eeg_microstates(): detected some outliers: refitting the classifier (n='\n                     + str(refitting) + ').')\n                masks.append(predicted != microstate)\n        mask = np.all(masks, axis=0)\n        training_set = training_set[mask]\n    return algorithm\n", "code_tokens": ["eeg", "microstates", "clustering", "data", "microstates", "4", "clustering", "method", "kmeans", "jobs", "1", "init", "25", "occurence", "rejection", "treshold", "0", "05", "max", "refitting", "5", "verbose", "true", "training", "set", "data", "copy", "if", "verbose", "is", "true", "print", "initializing", "the", "clustering", "algorithm", "if", "clustering", "method", "kmeans", "algorithm", "sklearn", "cluster", "kmeans", "init", "means", "clusters", "microstates", "init", "init", "jobs", "jobs", "elif", "clustering", "method", "spectral", "algorithm", "sklearn", "cluster", "spectralclustering", "clusters", "microstates", "init", "init", "jobs", "jobs", "elif", "clustering", "method", "agglom", "algorithm", "sklearn", "cluster", "agglomerativeclustering", "clusters", "microstates", "linkage", "complete", "elif", "clustering", "method", "dbscan", "algorithm", "sklearn", "cluster", "dbscan", "min", "samples", "100", "elif", "clustering", "method", "affinity", "algorithm", "sklearn", "cluster", "affinitypropagation", "damping", "0", "5", "else", "print", "neurokit", "error", "eeg", "microstates", "clustering", "method", "must", "be", "kmeans", "spectral", "dbscan", "affinity", "or", "agglom", "refitting", "0", "good", "fit", "achieved", "false", "while", "good", "fit", "achieved", "is", "false", "good", "fit", "achieved", "true", "if", "verbose", "is", "true", "print", "fitting", "the", "classifier", "algorithm", "fit", "training", "set", "if", "verbose", "is", "true", "print", "clustering", "back", "the", "initial", "data", "predicted", "algorithm", "fit", "predict", "training", "set", "if", "verbose", "is", "true", "print", "check", "for", "abnormalities", "occurences", "dict", "collections", "counter", "predicted", "masks", "np", "array", "true", "len", "training", "set", "for", "microstate", "in", "occurences", "if", "occurences", "microstate", "len", "data", "occurence", "rejection", "treshold", "good", "fit", "achieved", "false", "refitting", "1", "print", "neurokit", "warning", "eeg", "microstates", "detected", "some", "outliers", "refitting", "the", "classifier", "str", "refitting", "masks", "append", "predicted", "microstate", "mask", "np", "all", "masks", "axis", "0", "training", "set", "training", "set", "mask", "return", "algorithm"], "docstring": "k means clustering", "docstring_tokens": ["k", "means", "clustering"], "idx": 295}
{"url": "https://github.com/joshburnett/scanf/blob/52f8911581c1590a3dcc6f17594eeb7b39716d42/scanf.py#L158-L184", "repo": "scanf", "func_name": "extractdata", "original_string": ["def extractdata(pattern, text=None, filepath=None):\n", "    \"\"\"\n", "    Read through an entire file or body of text one line at a time. Parse each line that matches the supplied\n", "    pattern string and ignore the rest.\n", "\n", "    If *text* is supplied, it will be parsed according to the *pattern* string.\n", "    If *text* is not supplied, the file at *filepath* will be opened and parsed.\n", "    \"\"\"\n", "    y = []\n", "    if text is None:\n", "        textsource = open(filepath, 'r')\n", "    else:\n", "        textsource = text.splitlines()\n", "\n", "    for line in textsource:\n", "        match = scanf(pattern, line)\n", "        if match:\n", "            if len(y) == 0:\n", "                y = [[s] for s in match]\n", "            else:\n", "                for i, ydata in enumerate(y):\n", "                    ydata.append(match[i])\n", "\n", "    if text is None:\n", "        textsource.close()\n", "\n", "    return y\n"], "language": "python", "code": "def extractdata(pattern, text=None, filepath=None):\n    \"\"\"\"\"\"\n    y = []\n    if text is None:\n        textsource = open(filepath, 'r')\n    else:\n        textsource = text.splitlines()\n    for line in textsource:\n        match = scanf(pattern, line)\n        if match:\n            if len(y) == 0:\n                y = [[s] for s in match]\n            else:\n                for i, ydata in enumerate(y):\n                    ydata.append(match[i])\n    if text is None:\n        textsource.close()\n    return y\n", "code_tokens": ["extractdata", "pattern", "text", "none", "filepath", "none", "if", "text", "is", "none", "textsource", "open", "filepath", "else", "textsource", "text", "splitlines", "for", "line", "in", "textsource", "match", "scanf", "pattern", "line", "if", "match", "if", "len", "0", "for", "in", "match", "else", "for", "ydata", "in", "enumerate", "ydata", "append", "match", "if", "text", "is", "none", "textsource", "close", "return"], "docstring": "extracting data from a text file", "docstring_tokens": ["extracting", "data", "from", "a", "text", "file"], "idx": 296}
{"url": "https://github.com/wtsi-hgi/python-common/blob/0376a6b574ff46e82e509e90b6cb3693a3dbb577/hgicommon/data_source/static_from_file.py#L72-L82", "repo": "python-common", "func_name": "no_error_extract_data_from_file", "original_string": ["    def no_error_extract_data_from_file(self, file_path: str) -> Iterable[DataSourceType]:\n", "        \"\"\"\n", "        Proxy for `extract_data_from_file` that suppresses any errors and instead just returning an empty list.\n", "        :param file_path: see `extract_data_from_file`\n", "        :return: see `extract_data_from_file`\n", "        \"\"\"\n", "        try:\n", "            return self.extract_data_from_file(file_path)\n", "        except Exception as e:\n", "            logging.warning(e)\n", "            return []\n"], "language": "python", "code": "def no_error_extract_data_from_file(self, file_path: str) ->Iterable[\n    DataSourceType]:\n    \"\"\"\"\"\"\n    try:\n        return self.extract_data_from_file(file_path)\n    except Exception as e:\n        logging.warning(e)\n        return []\n", "code_tokens": ["no", "error", "extract", "data", "from", "file", "self", "file", "path", "str", "iterable", "datasourcetype", "try", "return", "self", "extract", "data", "from", "file", "file", "path", "except", "exception", "as", "logging", "warning", "return"], "docstring": "extracting data from a text file", "docstring_tokens": ["extracting", "data", "from", "a", "text", "file"], "idx": 297}
{"url": "https://github.com/bakwc/JamSpell/blob/bfdfd889436df4dbcd9ec86b20d2baeb815068bd/evaluate/utils.py#L25-L28", "repo": "JamSpell", "func_name": "loadText", "original_string": ["def loadText(fname):\n", "    with codecs.open(fname, 'r', 'utf-8') as f:\n", "        data = f.read()\n", "        return normalize(data).split()\n"], "language": "python", "code": "def loadText(fname):\n    with codecs.open(fname, 'r', 'utf-8') as f:\n        data = f.read()\n        return normalize(data).split()\n", "code_tokens": ["loadtext", "fname", "with", "codecs", "open", "fname", "utf", "8", "as", "data", "read", "return", "normalize", "data", "split"], "docstring": "extracting data from a text file", "docstring_tokens": ["extracting", "data", "from", "a", "text", "file"], "idx": 298}
{"url": "https://github.com/limodou/uliweb/blob/34472f25e4bc0b954a35346672f94e84ef18b076/uliweb/core/template.py#L489-L524", "repo": "uliweb", "func_name": "reindent", "original_string": ["def reindent(text, filename):\n", "    new_lines=[]\n", "    k=0\n", "    c=0\n", "    for n, raw_line in enumerate(text.splitlines()):\n", "        line=raw_line.strip()\n", "        if not line or line[0]=='#':\n", "            new_lines.append(line)\n", "            continue\n", "\n", "        line3 = line[:3]\n", "        line4 = line[:4]\n", "        line5 = line[:5]\n", "        line6 = line[:6]\n", "        line7 = line[:7]\n", "        if line3=='if ' or line4 in ('def ', 'for ', 'try:') or\\\n", "            line6=='while ' or line6=='class ' or line5=='with ':\n", "            new_lines.append('    '*k+line)\n", "            k += 1\n", "            continue\n", "        elif line5=='elif ' or line5=='else:' or    \\\n", "            line7=='except:' or line7=='except ' or \\\n", "            line7=='finally:':\n", "                c = k-1\n", "                if c<0:\n", "                    # print (_format_code(text))\n", "                    raise ParseError(\"Extra pass founded on line %s:%d\" % (filename, n))\n", "                new_lines.append('    '*c+line)\n", "                continue\n", "        else:\n", "            new_lines.append('    '*k+line)\n", "        if line=='pass' or line5=='pass ':\n", "            k-=1\n", "        if k<0: k = 0\n", "    text='\\n'.join(new_lines)\n", "    return text\n"], "language": "python", "code": "def reindent(text, filename):\n    new_lines = []\n    k = 0\n    c = 0\n    for n, raw_line in enumerate(text.splitlines()):\n        line = raw_line.strip()\n        if not line or line[0] == '#':\n            new_lines.append(line)\n            continue\n        line3 = line[:3]\n        line4 = line[:4]\n        line5 = line[:5]\n        line6 = line[:6]\n        line7 = line[:7]\n        if line3 == 'if ' or line4 in ('def ', 'for ', 'try:'\n            ) or line6 == 'while ' or line6 == 'class ' or line5 == 'with ':\n            new_lines.append('    ' * k + line)\n            k += 1\n            continue\n        elif line5 == 'elif ' or line5 == 'else:' or line7 == 'except:' or line7 == 'except ' or line7 == 'finally:':\n            c = k - 1\n            if c < 0:\n                raise ParseError('Extra pass founded on line %s:%d' % (\n                    filename, n))\n            new_lines.append('    ' * c + line)\n            continue\n        else:\n            new_lines.append('    ' * k + line)\n        if line == 'pass' or line5 == 'pass ':\n            k -= 1\n        if k < 0:\n            k = 0\n    text = '\\n'.join(new_lines)\n    return text\n", "code_tokens": ["reindent", "text", "filename", "new", "lines", "0", "0", "for", "raw", "line", "in", "enumerate", "text", "splitlines", "line", "raw", "line", "strip", "if", "not", "line", "or", "line", "0", "new", "lines", "append", "line", "continue", "line", "3", "line", "4", "line", "5", "line", "6", "line", "7", "if", "if", "or", "in", "def", "for", "try", "or", "while", "or", "class", "or", "with", "new", "lines", "append", "line", "1", "continue", "elif", "elif", "or", "else", "or", "except", "or", "except", "or", "finally", "1", "if", "0", "raise", "parseerror", "extra", "pass", "founded", "on", "line", "filename", "new", "lines", "append", "line", "continue", "else", "new", "lines", "append", "line", "if", "line", "pass", "or", "pass", "1", "if", "0", "0", "text", "join", "new", "lines", "return", "text"], "docstring": "read text file line by line", "docstring_tokens": ["read", "text", "file", "line", "by", "line"], "idx": 299}
{"url": "https://github.com/molmod/molmod/blob/a7b5b4364ed514ad4c465856c05b5eda1cb561e0/molmod/io/cp2k.py#L204-L213", "repo": "molmod", "func_name": "readline", "original_string": ["    def readline(self, f):\n", "        \"\"\"A helper method that only reads uncommented lines\"\"\"\n", "        while True:\n", "            line = f.readline()\n", "            if len(line) == 0:\n", "                raise EOFError\n", "            line = line[:line.find('#')]\n", "            line = line.strip()\n", "            if len(line) > 0:\n", "                return line\n"], "language": "python", "code": "def readline(self, f):\n    \"\"\"\"\"\"\n    while True:\n        line = f.readline()\n        if len(line) == 0:\n            raise EOFError\n        line = line[:line.find('#')]\n        line = line.strip()\n        if len(line) > 0:\n            return line\n", "code_tokens": ["readline", "self", "while", "true", "line", "readline", "if", "len", "line", "0", "raise", "eoferror", "line", "line", "line", "find", "line", "line", "strip", "if", "len", "line", "0", "return", "line"], "docstring": "read text file line by line", "docstring_tokens": ["read", "text", "file", "line", "by", "line"], "idx": 300}
{"url": "https://github.com/niccokunzmann/ledtable/blob/a94d276f8a06e0f7f05f5cc704018c899e56bd9f/python/ledtable/SerialLEDTable.py#L54-L63", "repo": "ledtable", "func_name": "default_line", "original_string": ["    def default_line(self, line):\n", "        if line.endswith(b\"\\n\"):\n", "            line = line[:-1]\n", "        if line.endswith(b\"\\r\"):\n", "            line = line[:-1]\n", "        try:\n", "            line = line.decode(\"ASCII\")\n", "        except UnicodeDecodeError:\n", "            pass\n", "        print(line)\n"], "language": "python", "code": "def default_line(self, line):\n    if line.endswith(b'\\n'):\n        line = line[:-1]\n    if line.endswith(b'\\r'):\n        line = line[:-1]\n    try:\n        line = line.decode('ASCII')\n    except UnicodeDecodeError:\n        pass\n    print(line)\n", "code_tokens": ["default", "line", "self", "line", "if", "line", "endswith", "line", "line", "1", "if", "line", "endswith", "line", "line", "1", "try", "line", "line", "decode", "ascii", "except", "unicodedecodeerror", "pass", "print", "line"], "docstring": "read text file line by line", "docstring_tokens": ["read", "text", "file", "line", "by", "line"], "idx": 301}
{"url": "https://github.com/chrisjsewell/jsonextended/blob/c3a7a880cc09789b3c61204265dcbb127be76c8a/jsonextended/mockpath.py#L44-L54", "repo": "jsonextended", "func_name": "readline", "original_string": ["    def readline(self):\n", "        if self._current_line >= len(self._linelist):\n", "            line = ''\n", "        else:\n", "            line = self._linelist[self._current_line] + '\\n'\n", "        self._current_line += 1\n", "        self._current_indx += len(\n", "            '\\n'.join(self._linelist[0:self._current_line]))\n", "        if self._encoding is not None:\n", "            line = line.encode(self._encoding)\n", "        return line\n"], "language": "python", "code": "def readline(self):\n    if self._current_line >= len(self._linelist):\n        line = ''\n    else:\n        line = self._linelist[self._current_line] + '\\n'\n    self._current_line += 1\n    self._current_indx += len('\\n'.join(self._linelist[0:self._current_line]))\n    if self._encoding is not None:\n        line = line.encode(self._encoding)\n    return line\n", "code_tokens": ["readline", "self", "if", "self", "current", "line", "len", "self", "linelist", "line", "else", "line", "self", "linelist", "self", "current", "line", "self", "current", "line", "1", "self", "current", "indx", "len", "join", "self", "linelist", "0", "self", "current", "line", "if", "self", "encoding", "is", "not", "none", "line", "line", "encode", "self", "encoding", "return", "line"], "docstring": "read text file line by line", "docstring_tokens": ["read", "text", "file", "line", "by", "line"], "idx": 302}
{"url": "https://github.com/nutechsoftware/alarmdecoder/blob/b0c014089e24455228cb4402cf30ba98157578cd/alarmdecoder/devices/serial_device.py#L149-L172", "repo": "alarmdecoder", "func_name": "write", "original_string": ["    def write(self, data):\n", "        \"\"\"\n", "        Writes data to the device.\n", "\n", "        :param data: data to write\n", "        :type data: string\n", "\n", "        :raises: py:class:`~alarmdecoder.util.CommError`\n", "        \"\"\"\n", "        try:\n", "            # Hack to support unicode under Python 2.x\n", "            if isinstance(data, str) or (sys.version_info < (3,) and isinstance(data, unicode)):\n", "                data = data.encode('utf-8')\n", "\n", "            self._device.write(data)\n", "\n", "        except serial.SerialTimeoutException:\n", "            pass\n", "\n", "        except serial.SerialException as err:\n", "            raise CommError('Error writing to device.', err)\n", "\n", "        else:\n", "            self.on_write(data=data)\n"], "language": "python", "code": "def write(self, data):\n    \"\"\"\"\"\"\n    try:\n        if isinstance(data, str) or sys.version_info < (3,) and isinstance(data\n            , unicode):\n            data = data.encode('utf-8')\n        self._device.write(data)\n    except serial.SerialTimeoutException:\n        pass\n    except serial.SerialException as err:\n        raise CommError('Error writing to device.', err)\n    else:\n        self.on_write(data=data)\n", "code_tokens": ["write", "self", "data", "try", "if", "isinstance", "data", "str", "or", "sys", "version", "info", "3", "and", "isinstance", "data", "unicode", "data", "data", "encode", "utf", "8", "self", "device", "write", "data", "except", "serial", "serialtimeoutexception", "pass", "except", "serial", "serialexception", "as", "err", "raise", "commerror", "error", "writing", "to", "device", "err", "else", "self", "on", "write", "data", "data"], "docstring": "sending binary data over a serial connection", "docstring_tokens": ["sending", "binary", "data", "over", "a", "serial", "connection"], "idx": 303}
{"url": "https://github.com/ransford/sllurp/blob/d744b7e17d7ba64a24d9a31bde6cba65d91ad9b1/sllurp/epc/sgtin_96.py#L27-L71", "repo": "sllurp", "func_name": "parse_sgtin_96", "original_string": ["def parse_sgtin_96(sgtin_96):\n", "    '''Given a SGTIN-96 hex string, parse each segment.\n", "    Returns a dictionary of the segments.'''\n", "\n", "    if not sgtin_96:\n", "        raise Exception('Pass in a value.')\n", "\n", "    if not sgtin_96.startswith(\"30\"):\n", "        # not a sgtin, not handled\n", "        raise Exception('Not SGTIN-96.')\n", "\n", "    binary = \"{0:020b}\".format(int(sgtin_96, 16)).zfill(96)\n", "\n", "    header = int(binary[:8], 2)\n", "    tag_filter = int(binary[8:11], 2)\n", "\n", "    partition = binary[11:14]\n", "    partition_value = int(partition, 2)\n", "\n", "    m, l, n, k = SGTIN_96_PARTITION_MAP[partition_value]\n", "\n", "    company_start = 8 + 3 + 3\n", "    company_end = company_start + m\n", "    company_data = int(binary[company_start:company_end], 2)\n", "    if company_data > pow(10, l):\n", "        # can't be too large\n", "        raise Exception('Company value is too large')\n", "    company_prefix = str(company_data).zfill(l)\n", "\n", "    item_start = company_end\n", "    item_end = item_start + n\n", "    item_data = binary[item_start:item_end]\n", "    item_number = int(item_data, 2)\n", "    item_reference = str(item_number).zfill(k)\n", "\n", "    serial = int(binary[-38:], 2)\n", "\n", "    return {\n", "        \"header\": header,\n", "        \"filter\": tag_filter,\n", "        \"partition\": partition,\n", "        \"company_prefix\": company_prefix,\n", "        \"item_reference\": item_reference,\n", "        \"serial\": serial\n", "    }\n"], "language": "python", "code": "def parse_sgtin_96(sgtin_96):\n    \"\"\"\"\"\"\n    if not sgtin_96:\n        raise Exception('Pass in a value.')\n    if not sgtin_96.startswith('30'):\n        raise Exception('Not SGTIN-96.')\n    binary = '{0:020b}'.format(int(sgtin_96, 16)).zfill(96)\n    header = int(binary[:8], 2)\n    tag_filter = int(binary[8:11], 2)\n    partition = binary[11:14]\n    partition_value = int(partition, 2)\n    m, l, n, k = SGTIN_96_PARTITION_MAP[partition_value]\n    company_start = 8 + 3 + 3\n    company_end = company_start + m\n    company_data = int(binary[company_start:company_end], 2)\n    if company_data > pow(10, l):\n        raise Exception('Company value is too large')\n    company_prefix = str(company_data).zfill(l)\n    item_start = company_end\n    item_end = item_start + n\n    item_data = binary[item_start:item_end]\n    item_number = int(item_data, 2)\n    item_reference = str(item_number).zfill(k)\n    serial = int(binary[-38:], 2)\n    return {'header': header, 'filter': tag_filter, 'partition': partition,\n        'company_prefix': company_prefix, 'item_reference': item_reference,\n        'serial': serial}\n", "code_tokens": ["parse", "sgtin", "96", "sgtin", "96", "if", "not", "sgtin", "96", "raise", "exception", "pass", "in", "value", "if", "not", "sgtin", "96", "startswith", "30", "raise", "exception", "not", "sgtin", "96", "binary", "0", "format", "int", "sgtin", "96", "16", "zfill", "96", "header", "int", "binary", "8", "2", "tag", "filter", "int", "binary", "8", "11", "2", "partition", "binary", "11", "14", "partition", "value", "int", "partition", "2", "sgtin", "96", "partition", "map", "partition", "value", "company", "start", "8", "3", "3", "company", "end", "company", "start", "company", "data", "int", "binary", "company", "start", "company", "end", "2", "if", "company", "data", "pow", "10", "raise", "exception", "company", "value", "is", "too", "large", "company", "prefix", "str", "company", "data", "zfill", "item", "start", "company", "end", "item", "end", "item", "start", "item", "data", "binary", "item", "start", "item", "end", "item", "number", "int", "item", "data", "2", "item", "reference", "str", "item", "number", "zfill", "serial", "int", "binary", "38", "2", "return", "header", "header", "filter", "tag", "filter", "partition", "partition", "company", "prefix", "company", "prefix", "item", "reference", "item", "reference", "serial", "serial"], "docstring": "sending binary data over a serial connection", "docstring_tokens": ["sending", "binary", "data", "over", "a", "serial", "connection"], "idx": 304}
{"url": "https://github.com/securestate/termineter/blob/d657d25d97c7739e650b951c396404e857e56625/lib/c1218/connection.py#L210-L222", "repo": "termineter", "func_name": "read", "original_string": ["\tdef read(self, size):\n", "\t\t\"\"\"\n", "\t\tRead raw data from the serial connection. This function is not\n", "\t\tmeant to be called directly.\n", "\n", "\t\t:param int size: The number of bytes to read from the serial connection.\n", "\t\t\"\"\"\n", "\t\tdata = self.serial_h.read(size)\n", "\t\tself.logger.debug('read data, length: ' + str(len(data)) + ' data: ' + binascii.b2a_hex(data).decode('utf-8'))\n", "\t\tself.serial_h.write(ACK)\n", "\t\tif sys.version_info[0] == 2:\n", "\t\t\tdata = bytearray(data)\n", "\t\treturn data\n"], "language": "python", "code": "def read(self, size):\n    \"\"\"\"\"\"\n    data = self.serial_h.read(size)\n    self.logger.debug('read data, length: ' + str(len(data)) + ' data: ' +\n        binascii.b2a_hex(data).decode('utf-8'))\n    self.serial_h.write(ACK)\n    if sys.version_info[0] == 2:\n        data = bytearray(data)\n    return data\n", "code_tokens": ["read", "self", "size", "data", "self", "serial", "read", "size", "self", "logger", "debug", "read", "data", "length", "str", "len", "data", "data", "binascii", "hex", "data", "decode", "utf", "8", "self", "serial", "write", "ack", "if", "sys", "version", "info", "0", "2", "data", "bytearray", "data", "return", "data"], "docstring": "sending binary data over a serial connection", "docstring_tokens": ["sending", "binary", "data", "over", "a", "serial", "connection"], "idx": 305}
{"url": "https://github.com/FaradayRF/faradayio/blob/6cf3af88bb4a83e5d2036e5cbdfaf8f0f01500bb/faradayio/faraday.py#L32-L56", "repo": "faradayio", "func_name": "send", "original_string": ["    def send(self, msg):\n", "        \"\"\"Encodes data to slip protocol and then sends over serial port\n", "\n", "        Uses the SlipLib module to convert the message data into SLIP format.\n", "        The message is then sent over the serial port opened with the instance\n", "        of the Faraday class used when invoking send().\n", "\n", "        Args:\n", "            msg (bytes): Bytes format message to send over serial port.\n", "\n", "        Returns:\n", "            int: Number of bytes transmitted over the serial port.\n", "\n", "        \"\"\"\n", "        # Create a sliplib Driver\n", "        slipDriver = sliplib.Driver()\n", "\n", "        # Package data in slip format\n", "        slipData = slipDriver.send(msg)\n", "\n", "        # Send data over serial port\n", "        res = self._serialPort.write(slipData)\n", "\n", "        # Return number of bytes transmitted over serial port\n", "        return res\n"], "language": "python", "code": "def send(self, msg):\n    \"\"\"\"\"\"\n    slipDriver = sliplib.Driver()\n    slipData = slipDriver.send(msg)\n    res = self._serialPort.write(slipData)\n    return res\n", "code_tokens": ["send", "self", "msg", "slipdriver", "sliplib", "driver", "slipdata", "slipdriver", "send", "msg", "res", "self", "serialport", "write", "slipdata", "return", "res"], "docstring": "sending binary data over a serial connection", "docstring_tokens": ["sending", "binary", "data", "over", "a", "serial", "connection"], "idx": 306}
{"url": "https://github.com/Seeed-Studio/wio-cli/blob/ce83f4c2d30be7f72d1a128acd123dfc5effa563/wio/commands/cmd_setup.py#L217-L384", "repo": "wio-cli", "func_name": "serial_send", "original_string": ["def serial_send(msvr, msvr_ip, xsvr, xsvr_ip, node_sn, node_key, port):\n", "    ### check is configure mode?\n", "    thread = termui.waiting_echo(\"Getting device information...\")\n", "    thread.daemon = True\n", "    thread.start()\n", "\n", "    flag = False\n", "    try:\n", "        with serial.Serial(port, 115200, timeout=5) as ser:\n", "            cmd = 'Blank?\\r\\n'\n", "            ser.write(cmd.encode('utf-8'))\n", "            if 'Node' in ser.readline():\n", "                flag = True\n", "    except serial.SerialException as e:\n", "        thread.stop('')\n", "        thread.join()\n", "        click.secho('>> ', fg='red', nl=False)\n", "        click.echo(e)\n", "        if e.errno == 13:\n", "            click.echo(\"For more information, see https://github.com/Seeed-Studio/wio-cli#serial-port-permissions\")\n", "        return None\n", "\n", "    thread.stop('')\n", "    thread.join()\n", "\n", "    if flag:\n", "        click.secho('> ', fg='green', nl=False)\n", "        click.secho(\"Found Wio.\", fg='green', bold=True)\n", "        click.echo()\n", "    else:\n", "        click.secho('> ', fg='green', nl=False)\n", "        click.secho(\"No nearby Wio detected.\", fg='white', bold=True)\n", "        if click.confirm(click.style('? ', fg='green') +\n", "                click.style(\"Would you like to wait and monitor for Wio entering configure mode\", bold=True),\n", "                default=True):\n", "\n", "            thread = termui.waiting_echo(\"Waiting for a wild Wio to appear... (press ctrl + C to exit)\")\n", "            thread.daemon = True\n", "            thread.start()\n", "\n", "            flag = False\n", "            while 1:\n", "                with serial.Serial(port, 115200, timeout=5) as ser:\n", "                    cmd = 'Blank?\\r\\n'\n", "                    ser.write(cmd.encode('utf-8'))\n", "                    if 'Node' in ser.readline():\n", "                        flag = True\n", "                        break\n", "\n", "            thread.stop('')\n", "            thread.join()\n", "            click.secho('> ', fg='green', nl=False)\n", "            click.secho(\"Found Wio.\", fg='green', bold=True)\n", "            click.echo()\n", "        else:\n", "            click.secho('> ', fg='green', nl=False)\n", "            click.secho(\"\\nQuit wio setup!\", bg='white', bold=True)\n", "\n", "    while 1:\n", "        if not click.confirm(click.style('? ', fg='green') +\n", "                    click.style(\"Would you like to manually enter your Wi-Fi network configuration?\", bold=True),\n", "                    default=False):\n", "            thread = termui.waiting_echo(\"Asking the Wio to scan for nearby Wi-Fi networks...\")\n", "            thread.daemon = True\n", "            thread.start()\n", "\n", "            flag = False\n", "            with serial.Serial(port, 115200, timeout=3) as ser:\n", "                cmd = 'SCAN\\r\\n'\n", "                ser.write(cmd.encode('utf-8'))\n", "                ssid_list = []\n", "                while True:\n", "                    ssid = ser.readline()\n", "                    if ssid == '\\r\\n':\n", "                        flag = True\n", "                        break\n", "                    ssid = ssid.strip('\\r\\n')\n", "                    ssid_list.append(ssid)\n", "\n", "            if flag:\n", "                thread.stop('')\n", "                thread.join()\n", "            else:\n", "                thread.stop(\"\\rsearch failure...\\n\")\n", "                return None\n", "\n", "            while 1:\n", "                for x in range(len(ssid_list)):\n", "                    click.echo(\"%s.) %s\" %(x, ssid_list[x]))\n", "                click.secho('? ', fg='green', nl=False)\n", "                value = click.prompt(\n", "                            click.style('Please select the network to which your Wio should connect', bold=True),\n", "                            type=int)\n", "                if value >= 0 and value < len(ssid_list):\n", "                    ssid = ssid_list[value]\n", "                    break\n", "                else:\n", "                    click.echo(click.style('>> ', fg='red') + \"invalid input, range 0 to %s\" %(len(ssid_list)-1))\n", "\n", "            ap = ssid\n", "        else:\n", "            ap = click.prompt(click.style('> ', fg='green') +\n", "                click.style('Please enter the SSID of your Wi-Fi network', bold=True), type=str)\n", "\n", "        ap_pwd = click.prompt(click.style('> ', fg='green') +\n", "            click.style('Please enter your Wi-Fi network password (leave blank for none)', bold=True),\n", "            default='', show_default=False)\n", "        d_name = click.prompt(click.style('> ', fg='green') +\n", "        click.style('Please enter the name of a device will be created', bold=True), type=str)\n", "\n", "        click.echo(click.style('> ', fg='green') + \"Here's what we're going to send to the Wio:\")\n", "        click.echo()\n", "        click.echo(click.style('> ', fg='green') + \"Wi-Fi network: \" +\n", "            click.style(ap, fg='green', bold=True))\n", "        ap_pwd_p = ap_pwd\n", "        if ap_pwd_p == '':\n", "            ap_pwd_p = 'None'\n", "        click.echo(click.style('> ', fg='green') + \"Password: \" +\n", "            click.style(ap_pwd_p, fg='green', bold=True))\n", "        click.echo(click.style('> ', fg='green') + \"Device name: \" +\n", "            click.style(d_name, fg='green', bold=True))\n", "        click.echo()\n", "\n", "        if click.confirm(click.style('? ', fg='green') +\n", "            \"Would you like to continue with the information shown above?\", default=True):\n", "            break\n", "\n", "    click.echo()\n", "    #waiting ui\n", "    thread = termui.waiting_echo(\"Sending Wi-Fi information to device...\")\n", "    thread.daemon = True\n", "    thread.start()\n", "\n", "    # send serial command\n", "    ## get version\n", "    version = 1.1\n", "    with serial.Serial(port, 115200, timeout=10) as ser:\n", "        cmd = 'VERSION\\r\\n'\n", "        ser.write(cmd.encode('utf-8'))\n", "        res = ser.readline()\n", "        try:\n", "            version = float(re.match(r\"([0-9]+.[0-9]+)\", res).group(0))\n", "        except Exception as e:\n", "            version = 1.1\n", "\n", "    send_flag = False\n", "    while 1:\n", "        with serial.Serial(port, 115200, timeout=10) as ser:\n", "            if version <= 1.1:\n", "                cmd = \"APCFG: %s\\t%s\\t%s\\t%s\\t%s\\t%s\\t\\r\\n\" %(ap, ap_pwd, node_key, node_sn, xsvr_ip, msvr_ip)\n", "            elif version >= 1.2:\n", "                cmd = \"APCFG: %s\\t%s\\t%s\\t%s\\t%s\\t%s\\t\\r\\n\" %(ap, ap_pwd, node_key, node_sn, xsvr, msvr)\n", "            else:\n", "                cmd = \"APCFG: %s\\t%s\\t%s\\t%s\\t%s\\t%s\\t\\r\\n\" %(ap, ap_pwd, node_key, node_sn, xsvr, msvr)\n", "            # click.echo(cmd)\n", "            ser.write(cmd.encode('utf-8'))\n", "            if \"ok\" in ser.readline():\n", "                click.echo(click.style('\\r> ', fg='green') + \"Send Wi-Fi information to device success.\")\n", "                thread.stop('')\n", "                thread.join()\n", "                send_flag = True\n", "        if send_flag:\n", "            break\n", "\n", "    if send_flag:\n", "        return {'name': d_name}\n", "    else:\n", "        return None\n"], "language": "python", "code": "def serial_send(msvr, msvr_ip, xsvr, xsvr_ip, node_sn, node_key, port):\n    thread = termui.waiting_echo('Getting device information...')\n    thread.daemon = True\n    thread.start()\n    flag = False\n    try:\n        with serial.Serial(port, 115200, timeout=5) as ser:\n            cmd = 'Blank?\\r\\n'\n            ser.write(cmd.encode('utf-8'))\n            if 'Node' in ser.readline():\n                flag = True\n    except serial.SerialException as e:\n        thread.stop('')\n        thread.join()\n        click.secho('>> ', fg='red', nl=False)\n        click.echo(e)\n        if e.errno == 13:\n            click.echo(\n                'For more information, see https://github.com/Seeed-Studio/wio-cli#serial-port-permissions'\n                )\n        return None\n    thread.stop('')\n    thread.join()\n    if flag:\n        click.secho('> ', fg='green', nl=False)\n        click.secho('Found Wio.', fg='green', bold=True)\n        click.echo()\n    else:\n        click.secho('> ', fg='green', nl=False)\n        click.secho('No nearby Wio detected.', fg='white', bold=True)\n        if click.confirm(click.style('? ', fg='green') + click.style(\n            'Would you like to wait and monitor for Wio entering configure mode'\n            , bold=True), default=True):\n            thread = termui.waiting_echo(\n                'Waiting for a wild Wio to appear... (press ctrl + C to exit)')\n            thread.daemon = True\n            thread.start()\n            flag = False\n            while 1:\n                with serial.Serial(port, 115200, timeout=5) as ser:\n                    cmd = 'Blank?\\r\\n'\n                    ser.write(cmd.encode('utf-8'))\n                    if 'Node' in ser.readline():\n                        flag = True\n                        break\n            thread.stop('')\n            thread.join()\n            click.secho('> ', fg='green', nl=False)\n            click.secho('Found Wio.', fg='green', bold=True)\n            click.echo()\n        else:\n            click.secho('> ', fg='green', nl=False)\n            click.secho('\\nQuit wio setup!', bg='white', bold=True)\n    while 1:\n        if not click.confirm(click.style('? ', fg='green') + click.style(\n            'Would you like to manually enter your Wi-Fi network configuration?'\n            , bold=True), default=False):\n            thread = termui.waiting_echo(\n                'Asking the Wio to scan for nearby Wi-Fi networks...')\n            thread.daemon = True\n            thread.start()\n            flag = False\n            with serial.Serial(port, 115200, timeout=3) as ser:\n                cmd = 'SCAN\\r\\n'\n                ser.write(cmd.encode('utf-8'))\n                ssid_list = []\n                while True:\n                    ssid = ser.readline()\n                    if ssid == '\\r\\n':\n                        flag = True\n                        break\n                    ssid = ssid.strip('\\r\\n')\n                    ssid_list.append(ssid)\n            if flag:\n                thread.stop('')\n                thread.join()\n            else:\n                thread.stop('\\rsearch failure...\\n')\n                return None\n            while 1:\n                for x in range(len(ssid_list)):\n                    click.echo('%s.) %s' % (x, ssid_list[x]))\n                click.secho('? ', fg='green', nl=False)\n                value = click.prompt(click.style(\n                    'Please select the network to which your Wio should connect'\n                    , bold=True), type=int)\n                if value >= 0 and value < len(ssid_list):\n                    ssid = ssid_list[value]\n                    break\n                else:\n                    click.echo(click.style('>> ', fg='red') + \n                        'invalid input, range 0 to %s' % (len(ssid_list) - 1))\n            ap = ssid\n        else:\n            ap = click.prompt(click.style('> ', fg='green') + click.style(\n                'Please enter the SSID of your Wi-Fi network', bold=True),\n                type=str)\n        ap_pwd = click.prompt(click.style('> ', fg='green') + click.style(\n            'Please enter your Wi-Fi network password (leave blank for none)',\n            bold=True), default='', show_default=False)\n        d_name = click.prompt(click.style('> ', fg='green') + click.style(\n            'Please enter the name of a device will be created', bold=True),\n            type=str)\n        click.echo(click.style('> ', fg='green') +\n            \"Here's what we're going to send to the Wio:\")\n        click.echo()\n        click.echo(click.style('> ', fg='green') + 'Wi-Fi network: ' +\n            click.style(ap, fg='green', bold=True))\n        ap_pwd_p = ap_pwd\n        if ap_pwd_p == '':\n            ap_pwd_p = 'None'\n        click.echo(click.style('> ', fg='green') + 'Password: ' + click.\n            style(ap_pwd_p, fg='green', bold=True))\n        click.echo(click.style('> ', fg='green') + 'Device name: ' + click.\n            style(d_name, fg='green', bold=True))\n        click.echo()\n        if click.confirm(click.style('? ', fg='green') +\n            'Would you like to continue with the information shown above?',\n            default=True):\n            break\n    click.echo()\n    thread = termui.waiting_echo('Sending Wi-Fi information to device...')\n    thread.daemon = True\n    thread.start()\n    version = 1.1\n    with serial.Serial(port, 115200, timeout=10) as ser:\n        cmd = 'VERSION\\r\\n'\n        ser.write(cmd.encode('utf-8'))\n        res = ser.readline()\n        try:\n            version = float(re.match('([0-9]+.[0-9]+)', res).group(0))\n        except Exception as e:\n            version = 1.1\n    send_flag = False\n    while 1:\n        with serial.Serial(port, 115200, timeout=10) as ser:\n            if version <= 1.1:\n                cmd = 'APCFG: %s\\t%s\\t%s\\t%s\\t%s\\t%s\\t\\r\\n' % (ap, ap_pwd,\n                    node_key, node_sn, xsvr_ip, msvr_ip)\n            elif version >= 1.2:\n                cmd = 'APCFG: %s\\t%s\\t%s\\t%s\\t%s\\t%s\\t\\r\\n' % (ap, ap_pwd,\n                    node_key, node_sn, xsvr, msvr)\n            else:\n                cmd = 'APCFG: %s\\t%s\\t%s\\t%s\\t%s\\t%s\\t\\r\\n' % (ap, ap_pwd,\n                    node_key, node_sn, xsvr, msvr)\n            ser.write(cmd.encode('utf-8'))\n            if 'ok' in ser.readline():\n                click.echo(click.style('\\r> ', fg='green') +\n                    'Send Wi-Fi information to device success.')\n                thread.stop('')\n                thread.join()\n                send_flag = True\n        if send_flag:\n            break\n    if send_flag:\n        return {'name': d_name}\n    else:\n        return None\n", "code_tokens": ["serial", "send", "msvr", "msvr", "ip", "xsvr", "xsvr", "ip", "node", "sn", "node", "key", "port", "thread", "termui", "waiting", "echo", "getting", "device", "information", "thread", "daemon", "true", "thread", "start", "flag", "false", "try", "with", "serial", "serial", "port", "115200", "timeout", "5", "as", "ser", "cmd", "blank", "ser", "write", "cmd", "encode", "utf", "8", "if", "node", "in", "ser", "readline", "flag", "true", "except", "serial", "serialexception", "as", "thread", "stop", "thread", "join", "click", "secho", "fg", "red", "nl", "false", "click", "echo", "if", "errno", "13", "click", "echo", "for", "more", "information", "see", "https", "github", "com", "seeed", "studio", "wio", "cli", "serial", "port", "permissions", "return", "none", "thread", "stop", "thread", "join", "if", "flag", "click", "secho", "fg", "green", "nl", "false", "click", "secho", "found", "wio", "fg", "green", "bold", "true", "click", "echo", "else", "click", "secho", "fg", "green", "nl", "false", "click", "secho", "no", "nearby", "wio", "detected", "fg", "white", "bold", "true", "if", "click", "confirm", "click", "style", "fg", "green", "click", "style", "would", "you", "like", "to", "wait", "and", "monitor", "for", "wio", "entering", "configure", "mode", "bold", "true", "default", "true", "thread", "termui", "waiting", "echo", "waiting", "for", "wild", "wio", "to", "appear", "press", "ctrl", "to", "exit", "thread", "daemon", "true", "thread", "start", "flag", "false", "while", "1", "with", "serial", "serial", "port", "115200", "timeout", "5", "as", "ser", "cmd", "blank", "ser", "write", "cmd", "encode", "utf", "8", "if", "node", "in", "ser", "readline", "flag", "true", "break", "thread", "stop", "thread", "join", "click", "secho", "fg", "green", "nl", "false", "click", "secho", "found", "wio", "fg", "green", "bold", "true", "click", "echo", "else", "click", "secho", "fg", "green", "nl", "false", "click", "secho", "nquit", "wio", "setup", "bg", "white", "bold", "true", "while", "1", "if", "not", "click", "confirm", "click", "style", "fg", "green", "click", "style", "would", "you", "like", "to", "manually", "enter", "your", "wi", "fi", "network", "configuration", "bold", "true", "default", "false", "thread", "termui", "waiting", "echo", "asking", "the", "wio", "to", "scan", "for", "nearby", "wi", "fi", "networks", "thread", "daemon", "true", "thread", "start", "flag", "false", "with", "serial", "serial", "port", "115200", "timeout", "3", "as", "ser", "cmd", "scan", "ser", "write", "cmd", "encode", "utf", "8", "ssid", "list", "while", "true", "ssid", "ser", "readline", "if", "ssid", "flag", "true", "break", "ssid", "ssid", "strip", "ssid", "list", "append", "ssid", "if", "flag", "thread", "stop", "thread", "join", "else", "thread", "stop", "rsearch", "failure", "return", "none", "while", "1", "for", "in", "range", "len", "ssid", "list", "click", "echo", "ssid", "list", "click", "secho", "fg", "green", "nl", "false", "value", "click", "prompt", "click", "style", "please", "select", "the", "network", "to", "which", "your", "wio", "should", "connect", "bold", "true", "type", "int", "if", "value", "0", "and", "value", "len", "ssid", "list", "ssid", "ssid", "list", "value", "break", "else", "click", "echo", "click", "style", "fg", "red", "invalid", "input", "range", "0", "to", "len", "ssid", "list", "1", "ap", "ssid", "else", "ap", "click", "prompt", "click", "style", "fg", "green", "click", "style", "please", "enter", "the", "ssid", "of", "your", "wi", "fi", "network", "bold", "true", "type", "str", "ap", "pwd", "click", "prompt", "click", "style", "fg", "green", "click", "style", "please", "enter", "your", "wi", "fi", "network", "password", "leave", "blank", "for", "none", "bold", "true", "default", "show", "default", "false", "name", "click", "prompt", "click", "style", "fg", "green", "click", "style", "please", "enter", "the", "name", "of", "device", "will", "be", "created", "bold", "true", "type", "str", "click", "echo", "click", "style", "fg", "green", "here", "what", "we", "re", "going", "to", "send", "to", "the", "wio", "click", "echo", "click", "echo", "click", "style", "fg", "green", "wi", "fi", "network", "click", "style", "ap", "fg", "green", "bold", "true", "ap", "pwd", "ap", "pwd", "if", "ap", "pwd", "ap", "pwd", "none", "click", "echo", "click", "style", "fg", "green", "password", "click", "style", "ap", "pwd", "fg", "green", "bold", "true", "click", "echo", "click", "style", "fg", "green", "device", "name", "click", "style", "name", "fg", "green", "bold", "true", "click", "echo", "if", "click", "confirm", "click", "style", "fg", "green", "would", "you", "like", "to", "continue", "with", "the", "information", "shown", "above", "default", "true", "break", "click", "echo", "thread", "termui", "waiting", "echo", "sending", "wi", "fi", "information", "to", "device", "thread", "daemon", "true", "thread", "start", "version", "1", "1", "with", "serial", "serial", "port", "115200", "timeout", "10", "as", "ser", "cmd", "version", "ser", "write", "cmd", "encode", "utf", "8", "res", "ser", "readline", "try", "version", "float", "re", "match", "0", "9", "0", "9", "res", "group", "0", "except", "exception", "as", "version", "1", "1", "send", "flag", "false", "while", "1", "with", "serial", "serial", "port", "115200", "timeout", "10", "as", "ser", "if", "version", "1", "1", "cmd", "apcfg", "ap", "ap", "pwd", "node", "key", "node", "sn", "xsvr", "ip", "msvr", "ip", "elif", "version", "1", "2", "cmd", "apcfg", "ap", "ap", "pwd", "node", "key", "node", "sn", "xsvr", "msvr", "else", "cmd", "apcfg", "ap", "ap", "pwd", "node", "key", "node", "sn", "xsvr", "msvr", "ser", "write", "cmd", "encode", "utf", "8", "if", "ok", "in", "ser", "readline", "click", "echo", "click", "style", "fg", "green", "send", "wi", "fi", "information", "to", "device", "success", "thread", "stop", "thread", "join", "send", "flag", "true", "if", "send", "flag", "break", "if", "send", "flag", "return", "name", "name", "else", "return", "none"], "docstring": "sending binary data over a serial connection", "docstring_tokens": ["sending", "binary", "data", "over", "a", "serial", "connection"], "idx": 307}
{"url": "https://github.com/securestate/termineter/blob/d657d25d97c7739e650b951c396404e857e56625/lib/termineter/core.py#L363-L374", "repo": "termineter", "func_name": "serial_connect", "original_string": ["\tdef serial_connect(self):\n", "\t\t\"\"\"\n", "\t\tConnect to the serial device.\n", "\t\t\"\"\"\n", "\t\tself.serial_get()\n", "\t\ttry:\n", "\t\t\tself.serial_connection.start()\n", "\t\texcept c1218.errors.C1218IOError as error:\n", "\t\t\tself.logger.error('serial connection has been opened but the meter is unresponsive')\n", "\t\t\traise error\n", "\t\tself._serial_connected = True\n", "\t\treturn True\n"], "language": "python", "code": "def serial_connect(self):\n    \"\"\"\"\"\"\n    self.serial_get()\n    try:\n        self.serial_connection.start()\n    except c1218.errors.C1218IOError as error:\n        self.logger.error(\n            'serial connection has been opened but the meter is unresponsive')\n        raise error\n    self._serial_connected = True\n    return True\n", "code_tokens": ["serial", "connect", "self", "self", "serial", "get", "try", "self", "serial", "connection", "start", "except", "errors", "as", "error", "self", "logger", "error", "serial", "connection", "has", "been", "opened", "but", "the", "meter", "is", "unresponsive", "raise", "error", "self", "serial", "connected", "true", "return", "true"], "docstring": "sending binary data over a serial connection", "docstring_tokens": ["sending", "binary", "data", "over", "a", "serial", "connection"], "idx": 308}
{"url": "https://github.com/google/mobly/blob/38ba2cf7d29a20e6a2fca1718eecb337df38db26/mobly/controllers/android_device_lib/jsonrpc_shell_base.py#L44-L69", "repo": "mobly", "func_name": "load_device", "original_string": ["  def load_device(self, serial=None):\n", "    \"\"\"Creates an AndroidDevice for the given serial number.\n", "\n", "    If no serial is given, it will read from the ANDROID_SERIAL\n", "    environmental variable. If the environmental variable is not set, then\n", "    it will read from 'adb devices' if there is only one.\n", "    \"\"\"\n", "    serials = android_device.list_adb_devices()\n", "    if not serials:\n", "      raise Error('No adb device found!')\n", "    # No serial provided, try to pick up the device automatically.\n", "    if not serial:\n", "      env_serial = os.environ.get('ANDROID_SERIAL', None)\n", "      if env_serial is not None:\n", "        serial = env_serial\n", "      elif len(serials) == 1:\n", "        serial = serials[0]\n", "      else:\n", "        raise Error('Expected one phone, but %d found. Use the -s flag or '\n", "                    'specify ANDROID_SERIAL.' % len(serials))\n", "    if serial not in serials:\n", "      raise Error('Device \"%s\" is not found by adb.' % serial)\n", "    ads = android_device.get_instances([serial])\n", "    assert len(ads) == 1\n", "    self._ad = ads[0]\n", "\n"], "language": "python", "code": "def load_device(self, serial=None):\n    \"\"\"\"\"\"\n    serials = android_device.list_adb_devices()\n    if not serials:\n        raise Error('No adb device found!')\n    if not serial:\n        env_serial = os.environ.get('ANDROID_SERIAL', None)\n        if env_serial is not None:\n            serial = env_serial\n        elif len(serials) == 1:\n            serial = serials[0]\n        else:\n            raise Error(\n                'Expected one phone, but %d found. Use the -s flag or specify ANDROID_SERIAL.'\n                 % len(serials))\n    if serial not in serials:\n        raise Error('Device \"%s\" is not found by adb.' % serial)\n    ads = android_device.get_instances([serial])\n    assert len(ads) == 1\n    self._ad = ads[0]\n", "code_tokens": ["load", "device", "self", "serial", "none", "serials", "android", "device", "list", "adb", "devices", "if", "not", "serials", "raise", "error", "no", "adb", "device", "found", "if", "not", "serial", "env", "serial", "os", "environ", "get", "android", "serial", "none", "if", "env", "serial", "is", "not", "none", "serial", "env", "serial", "elif", "len", "serials", "1", "serial", "serials", "0", "else", "raise", "error", "expected", "one", "phone", "but", "found", "use", "the", "flag", "or", "specify", "android", "serial", "len", "serials", "if", "serial", "not", "in", "serials", "raise", "error", "device", "is", "not", "found", "by", "adb", "serial", "ads", "android", "device", "get", "instances", "serial", "assert", "len", "ads", "1", "self", "ad", "ads", "0"], "docstring": "sending binary data over a serial connection", "docstring_tokens": ["sending", "binary", "data", "over", "a", "serial", "connection"], "idx": 309}
{"url": "https://github.com/clach04/x10_any/blob/5b90a543b127ab9e6112fd547929b5ef4b8f0cbc/x10_any/cm17a.py#L106-L118", "repo": "x10_any", "func_name": "_sendBinaryData", "original_string": ["def _sendBinaryData(port, data):\n", "    \"\"\"Send a string of binary data to the FireCracker with proper timing.\n", "\n", "    See the diagram in the spec referenced above for timing information.\n", "    The module level variables leadInOutDelay and bitDelay represent how\n", "    long each type of delay should be in seconds. They may require tweaking\n", "    on some setups.\n", "    \"\"\"\n", "    _reset(port)\n", "    time.sleep(leadInOutDelay)\n", "    for digit in data:\n", "        _sendBit(port, digit)\n", "    time.sleep(leadInOutDelay)\n"], "language": "python", "code": "def _sendBinaryData(port, data):\n    \"\"\"\"\"\"\n    _reset(port)\n    time.sleep(leadInOutDelay)\n    for digit in data:\n        _sendBit(port, digit)\n    time.sleep(leadInOutDelay)\n", "code_tokens": ["sendbinarydata", "port", "data", "reset", "port", "time", "sleep", "leadinoutdelay", "for", "digit", "in", "data", "sendbit", "port", "digit", "time", "sleep", "leadinoutdelay"], "docstring": "sending binary data over a serial connection", "docstring_tokens": ["sending", "binary", "data", "over", "a", "serial", "connection"], "idx": 310}
{"url": "https://github.com/saschpe/rapport/blob/ccceb8f84bd7e8add88ab5e137cdab6424aa4683/rapport/util.py#L55-L69", "repo": "rapport", "func_name": "datetime_from_iso8601", "original_string": ["def datetime_from_iso8601(date):\n", "    \"\"\"Small helper that parses ISO-8601 date dates.\n", "\n", "        >>> datetime_from_iso8601(\"2013-04-10T12:52:39\")\n", "        datetime.datetime(2013, 4, 10, 12, 52, 39)\n", "        >>> datetime_from_iso8601(\"2013-01-07T12:55:19.257\")\n", "        datetime.datetime(2013, 1, 7, 12, 55, 19, 257000)\n", "    \"\"\"\n", "    format = ISO8610_FORMAT\n", "    if date.endswith(\"Z\"):\n", "        date = date[:-1]  # Date date is UTC\n", "    if re.match(\".*\\.\\d+\", date):\n", "        # Date includes microseconds\n", "        format = ISO8610_FORMAT_MICROSECONDS\n", "    return datetime.datetime.strptime(date, format)\n"], "language": "python", "code": "def datetime_from_iso8601(date):\n    \"\"\"\"\"\"\n    format = ISO8610_FORMAT\n    if date.endswith('Z'):\n        date = date[:-1]\n    if re.match('.*\\\\.\\\\d+', date):\n        format = ISO8610_FORMAT_MICROSECONDS\n    return datetime.datetime.strptime(date, format)\n", "code_tokens": ["datetime", "from", "date", "format", "format", "if", "date", "endswith", "date", "date", "1", "if", "re", "match", "date", "format", "format", "microseconds", "return", "datetime", "datetime", "strptime", "date", "format"], "docstring": "format date", "docstring_tokens": ["format", "date"], "idx": 311}
{"url": "https://github.com/stestagg/dateformat/blob/4743f5dabf1eaf66524247328c76cfd3a05d0daf/benchmark/benchmark_formatting.py#L29-L32", "repo": "dateformat", "func_name": "format_dateformat", "original_string": ["def format_dateformat(dates):\n", "    format = dateformat.DateFormat(\"YYYY-MM-DD hh:mm:ss\")\n", "    for date in dates:\n", "        assert isinstance(format.format(date), str)\n"], "language": "python", "code": "def format_dateformat(dates):\n    format = dateformat.DateFormat('YYYY-MM-DD hh:mm:ss')\n    for date in dates:\n        assert isinstance(format.format(date), str)\n", "code_tokens": ["format", "dateformat", "dates", "format", "dateformat", "dateformat", "yyyy", "mm", "dd", "hh", "mm", "ss", "for", "date", "in", "dates", "assert", "isinstance", "format", "format", "date", "str"], "docstring": "format date", "docstring_tokens": ["format", "date"], "idx": 312}
{"url": "https://github.com/stestagg/dateformat/blob/4743f5dabf1eaf66524247328c76cfd3a05d0daf/benchmark/benchmark_parsing.py#L22-L24", "repo": "dateformat", "func_name": "format_date_list", "original_string": ["def format_date_list(dates):\n", "    format = dateformat.DateFormat(\"YYYY-MM-DD hh:mm:ss\")\n", "    return [format.format(date) for date in dates]\n"], "language": "python", "code": "def format_date_list(dates):\n    format = dateformat.DateFormat('YYYY-MM-DD hh:mm:ss')\n    return [format.format(date) for date in dates]\n", "code_tokens": ["format", "date", "list", "dates", "format", "dateformat", "dateformat", "yyyy", "mm", "dd", "hh", "mm", "ss", "return", "format", "format", "date", "for", "date", "in", "dates"], "docstring": "format date", "docstring_tokens": ["format", "date"], "idx": 313}
{"url": "https://github.com/geex-arts/django-jet/blob/64d5379f8a2278408694ce7913bf25f26035b855/jet/dashboard/dashboard_modules/google_analytics.py#L267-L279", "repo": "django-jet", "func_name": "format_grouped_date", "original_string": ["    def format_grouped_date(self, data, group):\n", "        date = self.get_grouped_date(data, group)\n", "\n", "        if group == 'week':\n", "            date = u'%s \u2014 %s' % (\n", "                (date - datetime.timedelta(days=6)).strftime('%d.%m'),\n", "                date.strftime('%d.%m')\n", "            )\n", "        elif group == 'month':\n", "            date = date.strftime('%b, %Y')\n", "        else:\n", "            date = formats.date_format(date, 'DATE_FORMAT')\n", "        return date\n"], "language": "python", "code": "def format_grouped_date(self, data, group):\n    date = self.get_grouped_date(data, group)\n    if group == 'week':\n        date = u'%s \u2014 %s' % ((date - datetime.timedelta(days=6)).strftime(\n            '%d.%m'), date.strftime('%d.%m'))\n    elif group == 'month':\n        date = date.strftime('%b, %Y')\n    else:\n        date = formats.date_format(date, 'DATE_FORMAT')\n    return date\n", "code_tokens": ["format", "grouped", "date", "self", "data", "group", "date", "self", "get", "grouped", "date", "data", "group", "if", "group", "week", "date", "date", "datetime", "timedelta", "days", "6", "strftime", "date", "strftime", "elif", "group", "month", "date", "date", "strftime", "else", "date", "formats", "date", "format", "date", "date", "format", "return", "date"], "docstring": "format date", "docstring_tokens": ["format", "date"], "idx": 314}
{"url": "https://github.com/LandRegistry/lr-utils/blob/811c9e5c11678a04ee203fa55a7c75080f4f9d89/lrutils/templatefilters/template_filters.py#L10-L12", "repo": "lr-utils", "func_name": "dateformat", "original_string": ["def dateformat(value, format='%-d %B %Y'):\n", "    new_date = parser.parse(value, dayfirst=True)\n", "    return new_date.strftime(format)\n"], "language": "python", "code": "def dateformat(value, format='%-d %B %Y'):\n    new_date = parser.parse(value, dayfirst=True)\n    return new_date.strftime(format)\n", "code_tokens": ["dateformat", "value", "format", "new", "date", "parser", "parse", "value", "dayfirst", "true", "return", "new", "date", "strftime", "format"], "docstring": "format date", "docstring_tokens": ["format", "date"], "idx": 315}
{"url": "https://github.com/BernardFW/bernard/blob/9c55703e5ffe5717c9fa39793df59dbfa5b4c5ab/src/bernard/i18n/_formatter.py#L80-L85", "repo": "bernard", "func_name": "format_number", "original_string": ["\n", "    def format_number(self, value):\n", "        \"\"\"\n", "        Format the number using Babel\n", "        \"\"\"\n", "        return numbers.format_number(value, locale=self.lang)\n"], "language": "python", "code": "def format_number(self, value):\n    \"\"\"\"\"\"\n    return numbers.format_number(value, locale=self.lang)\n", "code_tokens": ["format", "number", "self", "value", "return", "numbers", "format", "number", "value", "locale", "self", "lang"], "docstring": "format date", "docstring_tokens": ["format", "date"], "idx": 316}
{"url": "https://github.com/binux/pyspider/blob/3fccfabe2b057b7a56d4a4c79dc0dd6cd2239fe9/pyspider/libs/utils.py#L72-L130", "repo": "pyspider", "func_name": "format_date", "original_string": ["def format_date(date, gmt_offset=0, relative=True, shorter=False, full_format=False):\n", "    \"\"\"Formats the given date (which should be GMT).\n", "\n", "    By default, we return a relative time (e.g., \"2 minutes ago\"). You\n", "    can return an absolute date string with ``relative=False``.\n", "\n", "    You can force a full format date (\"July 10, 1980\") with\n", "    ``full_format=True``.\n", "\n", "    This method is primarily intended for dates in the past.\n", "    For dates in the future, we fall back to full format.\n", "\n", "    From tornado\n", "    \"\"\"\n", "\n", "    if not date:\n", "        return '-'\n", "    if isinstance(date, float) or isinstance(date, int):\n", "        date = datetime.datetime.utcfromtimestamp(date)\n", "    now = datetime.datetime.utcnow()\n", "    if date > now:\n", "        if relative and (date - now).seconds < 60:\n", "            # Due to click skew, things are some things slightly\n", "            # in the future. Round timestamps in the immediate\n", "            # future down to now in relative mode.\n", "            date = now\n", "        else:\n", "            # Otherwise, future dates always use the full format.\n", "            full_format = True\n", "    local_date = date - datetime.timedelta(minutes=gmt_offset)\n", "    local_now = now - datetime.timedelta(minutes=gmt_offset)\n", "    local_yesterday = local_now - datetime.timedelta(hours=24)\n", "    difference = now - date\n", "    seconds = difference.seconds\n", "    days = difference.days\n", "\n", "    format = None\n", "    if not full_format:\n", "        ret_, fff_format = fix_full_format(days, seconds, relative, shorter, local_date, local_yesterday)\n", "        format = fff_format\n", "        if ret_:\n", "            return format\n", "        else:\n", "            format = format\n", "\n", "    if format is None:\n", "        format = \"%(month_name)s %(day)s, %(year)s\" if shorter else \\\n", "            \"%(month_name)s %(day)s, %(year)s at %(time)s\"\n", "\n", "    str_time = \"%d:%02d\" % (local_date.hour, local_date.minute)\n", "\n", "    return format % {\n", "        \"month_name\": local_date.strftime('%b'),\n", "        \"weekday\": local_date.strftime('%A'),\n", "        \"day\": str(local_date.day),\n", "        \"year\": str(local_date.year),\n", "        \"month\": local_date.month,\n", "        \"time\": str_time\n", "    }\n"], "language": "python", "code": "def format_date(date, gmt_offset=0, relative=True, shorter=False,\n    full_format=False):\n    \"\"\"\"\"\"\n    if not date:\n        return '-'\n    if isinstance(date, float) or isinstance(date, int):\n        date = datetime.datetime.utcfromtimestamp(date)\n    now = datetime.datetime.utcnow()\n    if date > now:\n        if relative and (date - now).seconds < 60:\n            date = now\n        else:\n            full_format = True\n    local_date = date - datetime.timedelta(minutes=gmt_offset)\n    local_now = now - datetime.timedelta(minutes=gmt_offset)\n    local_yesterday = local_now - datetime.timedelta(hours=24)\n    difference = now - date\n    seconds = difference.seconds\n    days = difference.days\n    format = None\n    if not full_format:\n        ret_, fff_format = fix_full_format(days, seconds, relative, shorter,\n            local_date, local_yesterday)\n        format = fff_format\n        if ret_:\n            return format\n        else:\n            format = format\n    if format is None:\n        format = ('%(month_name)s %(day)s, %(year)s' if shorter else\n            '%(month_name)s %(day)s, %(year)s at %(time)s')\n    str_time = '%d:%02d' % (local_date.hour, local_date.minute)\n    return format % {'month_name': local_date.strftime('%b'), 'weekday':\n        local_date.strftime('%A'), 'day': str(local_date.day), 'year': str(\n        local_date.year), 'month': local_date.month, 'time': str_time}\n", "code_tokens": ["format", "date", "date", "gmt", "offset", "0", "relative", "true", "shorter", "false", "full", "format", "false", "if", "not", "date", "return", "if", "isinstance", "date", "float", "or", "isinstance", "date", "int", "date", "datetime", "datetime", "utcfromtimestamp", "date", "now", "datetime", "datetime", "utcnow", "if", "date", "now", "if", "relative", "and", "date", "now", "seconds", "60", "date", "now", "else", "full", "format", "true", "local", "date", "date", "datetime", "timedelta", "minutes", "gmt", "offset", "local", "now", "now", "datetime", "timedelta", "minutes", "gmt", "offset", "local", "yesterday", "local", "now", "datetime", "timedelta", "hours", "24", "difference", "now", "date", "seconds", "difference", "seconds", "days", "difference", "days", "format", "none", "if", "not", "full", "format", "ret", "fff", "format", "fix", "full", "format", "days", "seconds", "relative", "shorter", "local", "date", "local", "yesterday", "format", "fff", "format", "if", "ret", "return", "format", "else", "format", "format", "if", "format", "is", "none", "format", "month", "name", "day", "year", "if", "shorter", "else", "month", "name", "day", "year", "at", "time", "str", "time", "local", "date", "hour", "local", "date", "minute", "return", "format", "month", "name", "local", "date", "strftime", "weekday", "local", "date", "strftime", "day", "str", "local", "date", "day", "year", "str", "local", "date", "year", "month", "local", "date", "month", "time", "str", "time"], "docstring": "format date", "docstring_tokens": ["format", "date"], "idx": 317}
{"url": "https://github.com/hubo1016/vlcp/blob/239055229ec93a99cc7e15208075724ccf543bd1/vlcp/utils/http.py#L175-L196", "repo": "vlcp", "func_name": "setcookie", "original_string": ["    def setcookie(self, key, value, max_age=None, expires=None, path='/', domain=None, secure=None, httponly=False):\n", "        \"\"\"\n", "        Add a new cookie\n", "        \"\"\"\n", "        newcookie = Morsel()\n", "        newcookie.key = key\n", "        newcookie.value = value\n", "        newcookie.coded_value = value\n", "        if max_age is not None:\n", "            newcookie['max-age'] = max_age\n", "        if expires is not None:\n", "            newcookie['expires'] = expires\n", "        if path is not None:\n", "            newcookie['path'] = path\n", "        if domain is not None:\n", "            newcookie['domain'] = domain\n", "        if secure:\n", "            newcookie['secure'] = secure\n", "        if httponly:\n", "            newcookie['httponly'] = httponly\n", "        self.sent_cookies = [c for c in self.sent_cookies if c.key != key]\n", "        self.sent_cookies.append(newcookie)\n"], "language": "python", "code": "def setcookie(self, key, value, max_age=None, expires=None, path='/',\n    domain=None, secure=None, httponly=False):\n    \"\"\"\"\"\"\n    newcookie = Morsel()\n    newcookie.key = key\n    newcookie.value = value\n    newcookie.coded_value = value\n    if max_age is not None:\n        newcookie['max-age'] = max_age\n    if expires is not None:\n        newcookie['expires'] = expires\n    if path is not None:\n        newcookie['path'] = path\n    if domain is not None:\n        newcookie['domain'] = domain\n    if secure:\n        newcookie['secure'] = secure\n    if httponly:\n        newcookie['httponly'] = httponly\n    self.sent_cookies = [c for c in self.sent_cookies if c.key != key]\n    self.sent_cookies.append(newcookie)\n", "code_tokens": ["setcookie", "self", "key", "value", "max", "age", "none", "expires", "none", "path", "domain", "none", "secure", "none", "httponly", "false", "newcookie", "morsel", "newcookie", "key", "key", "newcookie", "value", "value", "newcookie", "coded", "value", "value", "if", "max", "age", "is", "not", "none", "newcookie", "max", "age", "max", "age", "if", "expires", "is", "not", "none", "newcookie", "expires", "expires", "if", "path", "is", "not", "none", "newcookie", "path", "path", "if", "domain", "is", "not", "none", "newcookie", "domain", "domain", "if", "secure", "newcookie", "secure", "secure", "if", "httponly", "newcookie", "httponly", "httponly", "self", "sent", "cookies", "for", "in", "self", "sent", "cookies", "if", "key", "key", "self", "sent", "cookies", "append", "newcookie"], "docstring": "create cookie", "docstring_tokens": ["create", "cookie"], "idx": 318}
{"url": "https://github.com/pyGrowler/Growler/blob/90c923ff204f28b86a01d741224987a22f69540f/growler/middleware/cookieparser.py#L34-L56", "repo": "Growler", "func_name": "__call__", "original_string": ["\n", "    def __call__(self, req, res):\n", "        \"\"\"\n", "        Parses cookies of the header request (using the 'cookie' header key)\n", "        and adds a callback to the 'on_headerstrings' response event.\n", "        \"\"\"\n", "        # Do not clobber cookies\n", "        if hasattr(req, 'cookies'):\n", "            return\n", "\n", "        # Create an empty cookie state\n", "        req.cookies, res.cookies = SimpleCookie(), SimpleCookie()\n", "\n", "        # If the request had a cookie, load it!\n", "        req.cookies.load(req.headers.get('COOKIE', ''))\n", "\n", "        def _gen_cookie():\n", "            if res.cookies:\n", "                cookie_string = res.cookies.output(header='', sep=res.EOL)\n", "                return cookie_string\n", "\n", "        res.headers['Set-Cookie'] = _gen_cookie\n"], "language": "python", "code": "def __call__(self, req, res):\n    \"\"\"\"\"\"\n    if hasattr(req, 'cookies'):\n        return\n    req.cookies, res.cookies = SimpleCookie(), SimpleCookie()\n    req.cookies.load(req.headers.get('COOKIE', ''))\n\n    def _gen_cookie():\n        if res.cookies:\n            cookie_string = res.cookies.output(header='', sep=res.EOL)\n            return cookie_string\n    res.headers['Set-Cookie'] = _gen_cookie\n", "code_tokens": ["call", "self", "req", "res", "if", "hasattr", "req", "cookies", "return", "req", "cookies", "res", "cookies", "simplecookie", "simplecookie", "req", "cookies", "load", "req", "headers", "get", "cookie", "def", "gen", "cookie", "if", "res", "cookies", "cookie", "string", "res", "cookies", "output", "header", "sep", "res", "eol", "return", "cookie", "string", "res", "headers", "set", "cookie", "gen", "cookie"], "docstring": "create cookie", "docstring_tokens": ["create", "cookie"], "idx": 319}
{"url": "https://github.com/wecatch/app-turbo/blob/75faf97371a9a138c53f92168d0a486636cb8a9c/turbo/session.py#L185-L193", "repo": "app-turbo", "func_name": "_set_cookie", "original_string": ["    def _set_cookie(self, name, value):\n", "        cookie_domain = self._config.cookie_domain\n", "        cookie_path = self._config.cookie_path\n", "        cookie_expires = self._config.cookie_expires\n", "        if self._config.secure:\n", "            return self.handler.set_secure_cookie(\n", "                name, value, expires_days=cookie_expires / (3600 * 24), domain=cookie_domain, path=cookie_path)\n", "        else:\n", "            return self.handler.set_cookie(name, value, expires=cookie_expires, domain=cookie_domain, path=cookie_path)\n"], "language": "python", "code": "def _set_cookie(self, name, value):\n    cookie_domain = self._config.cookie_domain\n    cookie_path = self._config.cookie_path\n    cookie_expires = self._config.cookie_expires\n    if self._config.secure:\n        return self.handler.set_secure_cookie(name, value, expires_days=\n            cookie_expires / (3600 * 24), domain=cookie_domain, path=\n            cookie_path)\n    else:\n        return self.handler.set_cookie(name, value, expires=cookie_expires,\n            domain=cookie_domain, path=cookie_path)\n", "code_tokens": ["set", "cookie", "self", "name", "value", "cookie", "domain", "self", "config", "cookie", "domain", "cookie", "path", "self", "config", "cookie", "path", "cookie", "expires", "self", "config", "cookie", "expires", "if", "self", "config", "secure", "return", "self", "handler", "set", "secure", "cookie", "name", "value", "expires", "days", "cookie", "expires", "3600", "24", "domain", "cookie", "domain", "path", "cookie", "path", "else", "return", "self", "handler", "set", "cookie", "name", "value", "expires", "cookie", "expires", "domain", "cookie", "domain", "path", "cookie", "path"], "docstring": "create cookie", "docstring_tokens": ["create", "cookie"], "idx": 320}
{"url": "https://github.com/mitsei/dlkit/blob/445f968a175d61c8d92c0f617a3c17dc1dc7c584/dlkit/json_/osid/markers.py#L309-L318", "repo": "dlkit", "func_name": "is_effective", "original_string": ["    def is_effective(self):\n", "        \"\"\"Tests if the current date is within the start end end dates inclusive.\n", "\n", "        return: (boolean) - ``true`` if this is effective, ``false``\n", "                otherwise\n", "        *compliance: mandatory -- This method must be implemented.*\n", "\n", "        \"\"\"\n", "        now = DateTime.utcnow()\n", "        return self.get_start_date() <= now and self.get_end_date() >= now\n"], "language": "python", "code": "def is_effective(self):\n    \"\"\"\"\"\"\n    now = DateTime.utcnow()\n    return self.get_start_date() <= now and self.get_end_date() >= now\n", "code_tokens": ["is", "effective", "self", "now", "datetime", "utcnow", "return", "self", "get", "start", "date", "now", "and", "self", "get", "end", "date", "now"], "docstring": "how to get current date", "docstring_tokens": ["how", "to", "get", "current", "date"], "idx": 321}
{"url": "https://github.com/alexhayes/django-toolkit/blob/b64106392fad596defc915b8235fe6e1d0013b5b/django_toolkit/views.py#L343-L353", "repo": "django-toolkit", "func_name": "get_date", "original_string": ["    def get_date(self):\n", "        \"\"\"\n", "        Return (date_list, items, extra_context) for this request.\n", "        \"\"\"\n", "        year = self.get_year()\n", "        month = self.get_month()\n", "        day = self.get_day()\n", "\n", "        return _date_from_string(year, self.get_year_format(),\n", "                                 month, self.get_month_format(),\n", "                                 day, self.get_day_format())\n"], "language": "python", "code": "def get_date(self):\n    \"\"\"\"\"\"\n    year = self.get_year()\n    month = self.get_month()\n    day = self.get_day()\n    return _date_from_string(year, self.get_year_format(), month, self.\n        get_month_format(), day, self.get_day_format())\n", "code_tokens": ["get", "date", "self", "year", "self", "get", "year", "month", "self", "get", "month", "day", "self", "get", "day", "return", "date", "from", "string", "year", "self", "get", "year", "format", "month", "self", "get", "month", "format", "day", "self", "get", "day", "format"], "docstring": "how to get current date", "docstring_tokens": ["how", "to", "get", "current", "date"], "idx": 322}
{"url": "https://github.com/ricobl/django-importer/blob/6967adfa7a286be7aaf59d3f33c6637270bd9df6/sample_project/tasks/importers.py#L64-L88", "repo": "django-importer", "func_name": "parse_date", "original_string": ["    def parse_date(self, item, field_name, source_name):\n", "        \"\"\"\n", "        Converts the date in the format: Thu 03.\n", "\n", "        As only the day is provided, tries to find the best match\n", "        based on the current date, considering that dates are on\n", "        the past.\n", "        \"\"\"\n", "        # Get the current date\n", "        now = datetime.now().date()\n", "        # Get the date from the source\n", "        val = self.get_value(item, source_name)\n", "        week_day, day = val.split()\n", "        day = int(day)\n", "        # If the current date is minor than the item date\n", "        # go back one month\n", "        if now.day < day:\n", "            if now.month == 1:\n", "                now = now.replace(month=12, year=now.year-1)\n", "            else:\n", "                now = now.replace(month=now.month-1)\n", "        # Finally, replace the source day in the current date\n", "        # and return\n", "        now = now.replace(day=day)\n", "        return now\n"], "language": "python", "code": "def parse_date(self, item, field_name, source_name):\n    \"\"\"\"\"\"\n    now = datetime.now().date()\n    val = self.get_value(item, source_name)\n    week_day, day = val.split()\n    day = int(day)\n    if now.day < day:\n        if now.month == 1:\n            now = now.replace(month=12, year=now.year - 1)\n        else:\n            now = now.replace(month=now.month - 1)\n    now = now.replace(day=day)\n    return now\n", "code_tokens": ["parse", "date", "self", "item", "field", "name", "source", "name", "now", "datetime", "now", "date", "val", "self", "get", "value", "item", "source", "name", "week", "day", "day", "val", "split", "day", "int", "day", "if", "now", "day", "day", "if", "now", "month", "1", "now", "now", "replace", "month", "12", "year", "now", "year", "1", "else", "now", "now", "replace", "month", "now", "month", "1", "now", "now", "replace", "day", "day", "return", "now"], "docstring": "how to get current date", "docstring_tokens": ["how", "to", "get", "current", "date"], "idx": 323}
{"url": "https://github.com/crazy-canux/arguspy/blob/e9486b5df61978a990d56bf43de35f3a4cdefcc3/scripts/check_wmi_sh.py#L226-L238", "repo": "arguspy", "func_name": "__get_current_datetime", "original_string": ["    def __get_current_datetime(self):\n", "        \"\"\"Get current datetime for every file.\"\"\"\n", "        self.wql_time = \"SELECT LocalDateTime FROM Win32_OperatingSystem\"\n", "        self.current_time = self.query(self.wql_time)\n", "        # [{'LocalDateTime': '20160824161431.977000+480'}]'\n", "        self.current_time_string = str(\n", "            self.current_time[0].get('LocalDateTime').split('.')[0])\n", "        # '20160824161431'\n", "        self.current_time_format = datetime.datetime.strptime(\n", "            self.current_time_string, '%Y%m%d%H%M%S')\n", "        # param: datetime.datetime(2016, 8, 24, 16, 14, 31) -> type:\n", "        # datetime.datetime\n", "        return self.current_time_format\n"], "language": "python", "code": "def __get_current_datetime(self):\n    \"\"\"\"\"\"\n    self.wql_time = 'SELECT LocalDateTime FROM Win32_OperatingSystem'\n    self.current_time = self.query(self.wql_time)\n    self.current_time_string = str(self.current_time[0].get('LocalDateTime'\n        ).split('.')[0])\n    self.current_time_format = datetime.datetime.strptime(self.\n        current_time_string, '%Y%m%d%H%M%S')\n    return self.current_time_format\n", "code_tokens": ["get", "current", "datetime", "self", "self", "wql", "time", "select", "localdatetime", "from", "operatingsystem", "self", "current", "time", "self", "query", "self", "wql", "time", "self", "current", "time", "string", "str", "self", "current", "time", "0", "get", "localdatetime", "split", "0", "self", "current", "time", "format", "datetime", "datetime", "strptime", "self", "current", "time", "string", "return", "self", "current", "time", "format"], "docstring": "how to get current date", "docstring_tokens": ["how", "to", "get", "current", "date"], "idx": 324}
{"url": "https://github.com/caktus/django-timepiece/blob/52515dec027664890efbc535429e1ba1ee152f40/timepiece/entries/views.py#L44-L55", "repo": "django-timepiece", "func_name": "get_dates", "original_string": ["    def get_dates(self):\n", "        today = datetime.date.today()\n", "        day = today\n", "        if 'week_start' in self.request.GET:\n", "            param = self.request.GET.get('week_start')\n", "            try:\n", "                day = datetime.datetime.strptime(param, '%Y-%m-%d').date()\n", "            except:\n", "                pass\n", "        week_start = utils.get_week_start(day)\n", "        week_end = week_start + relativedelta(days=6)\n", "        return today, week_start, week_end\n"], "language": "python", "code": "def get_dates(self):\n    today = datetime.date.today()\n    day = today\n    if 'week_start' in self.request.GET:\n        param = self.request.GET.get('week_start')\n        try:\n            day = datetime.datetime.strptime(param, '%Y-%m-%d').date()\n        except:\n            pass\n    week_start = utils.get_week_start(day)\n    week_end = week_start + relativedelta(days=6)\n    return today, week_start, week_end\n", "code_tokens": ["get", "dates", "self", "today", "datetime", "date", "today", "day", "today", "if", "week", "start", "in", "self", "request", "get", "param", "self", "request", "get", "get", "week", "start", "try", "day", "datetime", "datetime", "strptime", "param", "date", "except", "pass", "week", "start", "utils", "get", "week", "start", "day", "week", "end", "week", "start", "relativedelta", "days", "6", "return", "today", "week", "start", "week", "end"], "docstring": "how to get current date", "docstring_tokens": ["how", "to", "get", "current", "date"], "idx": 325}
{"url": "https://github.com/iloob/python-periods/blob/8988373522907d72c0ee5896c2ffbb573a8500d9/periods/week.py#L21-L30", "repo": "python-periods", "func_name": "__repr__", "original_string": ["    def __repr__(self):\n", "        if self.get_start_date().year != self.get_end_date().year:\n", "            if self.week == 1:\n", "                return \"%sW1\" % self.get_end_date().year\n", "            elif self.week > 51:\n", "                return \"%sW%s\" % (self.get_start_date().year,\n", "                                  self.get_start_date().isocalendar()[1])\n", "\n", "        return \"%sW%s\" % (self.get_start_date().year,\n", "                          self.get_start_date().isocalendar()[1])\n"], "language": "python", "code": "def __repr__(self):\n    if self.get_start_date().year != self.get_end_date().year:\n        if self.week == 1:\n            return '%sW1' % self.get_end_date().year\n        elif self.week > 51:\n            return '%sW%s' % (self.get_start_date().year, self.\n                get_start_date().isocalendar()[1])\n    return '%sW%s' % (self.get_start_date().year, self.get_start_date().\n        isocalendar()[1])\n", "code_tokens": ["repr", "self", "if", "self", "get", "start", "date", "year", "self", "get", "end", "date", "year", "if", "self", "week", "1", "return", "self", "get", "end", "date", "year", "elif", "self", "week", "51", "return", "sw", "self", "get", "start", "date", "year", "self", "get", "start", "date", "isocalendar", "1", "return", "sw", "self", "get", "start", "date", "year", "self", "get", "start", "date", "isocalendar", "1"], "docstring": "how to get current date", "docstring_tokens": ["how", "to", "get", "current", "date"], "idx": 326}
{"url": "https://github.com/jmbeach/KEP.py/blob/68cda64ab649640a486534867c81274c41e39446/src/keppy/tag.py#L31-L33", "repo": "KEP.py", "func_name": "name_replace", "original_string": ["    def name_replace(self, to_replace, replacement):\n", "        \"\"\"Replaces part of tag name with new value\"\"\"\n", "        self.name = self.name.replace(to_replace, replacement)\n"], "language": "python", "code": "def name_replace(self, to_replace, replacement):\n    \"\"\"\"\"\"\n    self.name = self.name.replace(to_replace, replacement)\n", "code_tokens": ["name", "replace", "self", "to", "replace", "replacement", "self", "name", "self", "name", "replace", "to", "replace", "replacement"], "docstring": "replace in file", "docstring_tokens": ["replace", "in", "file"], "idx": 327}
{"url": "https://github.com/char16t/wa/blob/ee28bf47665ea57f3a03a08dfc0a5daaa33d8121/wa/api.py#L152-L174", "repo": "wa", "func_name": "replace_f", "original_string": ["    def replace_f(self, path, arg_name=None):\n", "        \"\"\"Replace files\"\"\"\n", "        root, file = os.path.split(path)\n", "\n", "        pattern = re.compile(r'(\\<\\<\\<)([A-Za-z_]+)(\\>\\>\\>)')\n", "        file_path = path\n", "        fh, abs_path = mkstemp()\n", "        with open(abs_path, 'w') as new_file:\n", "            with open(file_path) as old_file:\n", "                for line in old_file:\n", "                    for (o, var_name, c) in re.findall(pattern, line):\n", "                        line = self.handle_args(line, var_name, arg_name)\n", "                    new_file.write(line)\n", "        os.close(fh)\n", "        # Remove original file\n", "        os.remove(file_path)\n", "        # Move new file\n", "        shutil.move(abs_path, file_path)\n", "\n", "        pattern = re.compile(r'(\\[\\[)([A-Za-z_]+)(\\]\\])')\n", "        for (o, var_name, c) in re.findall(pattern, file):\n", "            file = self.handle_args(file, var_name, isfilename=True)\n", "        os.rename(path, os.path.join(root, file))\n"], "language": "python", "code": "def replace_f(self, path, arg_name=None):\n    \"\"\"\"\"\"\n    root, file = os.path.split(path)\n    pattern = re.compile('(\\\\<\\\\<\\\\<)([A-Za-z_]+)(\\\\>\\\\>\\\\>)')\n    file_path = path\n    fh, abs_path = mkstemp()\n    with open(abs_path, 'w') as new_file:\n        with open(file_path) as old_file:\n            for line in old_file:\n                for o, var_name, c in re.findall(pattern, line):\n                    line = self.handle_args(line, var_name, arg_name)\n                new_file.write(line)\n    os.close(fh)\n    os.remove(file_path)\n    shutil.move(abs_path, file_path)\n    pattern = re.compile('(\\\\[\\\\[)([A-Za-z_]+)(\\\\]\\\\])')\n    for o, var_name, c in re.findall(pattern, file):\n        file = self.handle_args(file, var_name, isfilename=True)\n    os.rename(path, os.path.join(root, file))\n", "code_tokens": ["replace", "self", "path", "arg", "name", "none", "root", "file", "os", "path", "split", "path", "pattern", "re", "compile", "za", "file", "path", "path", "fh", "abs", "path", "mkstemp", "with", "open", "abs", "path", "as", "new", "file", "with", "open", "file", "path", "as", "old", "file", "for", "line", "in", "old", "file", "for", "var", "name", "in", "re", "findall", "pattern", "line", "line", "self", "handle", "args", "line", "var", "name", "arg", "name", "new", "file", "write", "line", "os", "close", "fh", "os", "remove", "file", "path", "shutil", "move", "abs", "path", "file", "path", "pattern", "re", "compile", "za", "for", "var", "name", "in", "re", "findall", "pattern", "file", "file", "self", "handle", "args", "file", "var", "name", "isfilename", "true", "os", "rename", "path", "os", "path", "join", "root", "file"], "docstring": "replace in file", "docstring_tokens": ["replace", "in", "file"], "idx": 328}
{"url": "https://github.com/NoviceLive/intellicoder/blob/6cac5ebfce65c370dbebe47756a1789b120ef982/intellicoder/transformers.py#L59-L66", "repo": "intellicoder", "func_name": "replace_source", "original_string": ["    def replace_source(self, body, name):\n", "        logging.debug(_('Processing function body: %s'), name)\n", "        replaced = re.sub(\n", "            self.FUNC_NAME_RE, self._func_replacer, body)\n", "        replaced = re.sub(\n", "            self.STR_LITERAL_RE, self._string_replacer, replaced)\n", "        return self._build_strings() + replaced\n", "        return replaced\n"], "language": "python", "code": "def replace_source(self, body, name):\n    logging.debug(_('Processing function body: %s'), name)\n    replaced = re.sub(self.FUNC_NAME_RE, self._func_replacer, body)\n    replaced = re.sub(self.STR_LITERAL_RE, self._string_replacer, replaced)\n    return self._build_strings() + replaced\n    return replaced\n", "code_tokens": ["replace", "source", "self", "body", "name", "logging", "debug", "processing", "function", "body", "name", "replaced", "re", "sub", "self", "func", "name", "re", "self", "func", "replacer", "body", "replaced", "re", "sub", "self", "str", "literal", "re", "self", "string", "replacer", "replaced", "return", "self", "build", "strings", "replaced", "return", "replaced"], "docstring": "replace in file", "docstring_tokens": ["replace", "in", "file"], "idx": 329}
{"url": "https://github.com/riptano/ccm/blob/275699f79d102b5039b79cc17fa6305dccf18412/ccmlib/common.py#L208-L219", "repo": "ccm", "func_name": "__init__", "original_string": ["\n", "\n", "class UnavailableSocketError(CCMError):\n", "    pass\n", "\n", "\n", "class TimeoutError(Exception):\n", "\n", "    def __init__(self, data):\n", "        Exception.__init__(self, str(data))\n", "\n", "\n"], "language": "python", "code": "def __init__(self, data):\n    Exception.__init__(self, str(data))\n", "code_tokens": ["init", "self", "data", "exception", "init", "self", "str", "data"], "docstring": "replace in file", "docstring_tokens": ["replace", "in", "file"], "idx": 330}
{"url": "https://github.com/JesseAldridge/clipmon/blob/5e16a31cf38f64db2d9b7a490968f922aca57512/clipmon.py#L19-L38", "repo": "clipmon", "func_name": "test_replacements", "original_string": ["def test_replacements(clip_str, path_exists):\n", "  replaced_str = clip_str\n", "  for find_regex, replace_str in conf.find_replace_map:\n", "    replaced_str = re.sub(find_regex, replace_str, replaced_str)\n", "\n", "  match = re.search(\n", "    #                 file extension\n", "    #    path               |\n", "    #     |                 |\n", "    r'((?:~|/)[^@^:^\\\\^\\(]+\\.[a-z]{2,3}).*(?:line.*?|\\()([0-9]+)', replaced_str)\n", "  if match and path_exists(os.path.expanduser(match.group(1))):\n", "    return ':'.join([match.group(1), match.group(2)])\n", "\n", "  match = re.search(\n", "    #                file extension\n", "    #   path              |\n", "    #    |                |\n", "    r'((?:~|/)[^@^:^\\\\^\\(]+\\.[a-z]{2,3}):([0-9]+)', replaced_str)\n", "  if match and path_exists(os.path.expanduser(match.group(1))):\n", "    return ':'.join([match.group(1), match.group(2)])\n"], "language": "python", "code": "def test_replacements(clip_str, path_exists):\n    replaced_str = clip_str\n    for find_regex, replace_str in conf.find_replace_map:\n        replaced_str = re.sub(find_regex, replace_str, replaced_str)\n    match = re.search(\n        '((?:~|/)[^@^:^\\\\\\\\^\\\\(]+\\\\.[a-z]{2,3}).*(?:line.*?|\\\\()([0-9]+)',\n        replaced_str)\n    if match and path_exists(os.path.expanduser(match.group(1))):\n        return ':'.join([match.group(1), match.group(2)])\n    match = re.search('((?:~|/)[^@^:^\\\\\\\\^\\\\(]+\\\\.[a-z]{2,3}):([0-9]+)',\n        replaced_str)\n    if match and path_exists(os.path.expanduser(match.group(1))):\n        return ':'.join([match.group(1), match.group(2)])\n", "code_tokens": ["test", "replacements", "clip", "str", "path", "exists", "replaced", "str", "clip", "str", "for", "find", "regex", "replace", "str", "in", "conf", "find", "replace", "map", "replaced", "str", "re", "sub", "find", "regex", "replace", "str", "replaced", "str", "match", "re", "search", "2", "3", "line", "0", "9", "replaced", "str", "if", "match", "and", "path", "exists", "os", "path", "expanduser", "match", "group", "1", "return", "join", "match", "group", "1", "match", "group", "2", "match", "re", "search", "2", "3", "0", "9", "replaced", "str", "if", "match", "and", "path", "exists", "os", "path", "expanduser", "match", "group", "1", "return", "join", "match", "group", "1", "match", "group", "2"], "docstring": "replace in file", "docstring_tokens": ["replace", "in", "file"], "idx": 331}
{"url": "https://github.com/TheRobotCarlson/DocxMerge/blob/df8d2add5ff4f33e31d36a6bd3d4495140334538/DocxMerge/DocxMerge.py#L274-L293", "repo": "DocxMerge", "func_name": "replace_doc_text", "original_string": ["def replace_doc_text(file, replacements):\n", "    document = Document(file)\n", "    paragraphs = document.paragraphs\n", "\n", "    changes = False\n", "\n", "    for paragraph in paragraphs:\n", "        text = paragraph.text\n", "        for original, replace in replacements.items():\n", "            if original in replace and replace in text:\n", "                continue\n", "            if original in text:\n", "                changes = True\n", "                text = text.replace(original, replace)\n", "                paragraph.text = text\n", "\n", "    if changes:\n", "        print(\"changing {}\".format(file))\n", "\n", "    document.save(file)\n"], "language": "python", "code": "def replace_doc_text(file, replacements):\n    document = Document(file)\n    paragraphs = document.paragraphs\n    changes = False\n    for paragraph in paragraphs:\n        text = paragraph.text\n        for original, replace in replacements.items():\n            if original in replace and replace in text:\n                continue\n            if original in text:\n                changes = True\n                text = text.replace(original, replace)\n                paragraph.text = text\n    if changes:\n        print('changing {}'.format(file))\n    document.save(file)\n", "code_tokens": ["replace", "doc", "text", "file", "replacements", "document", "document", "file", "paragraphs", "document", "paragraphs", "changes", "false", "for", "paragraph", "in", "paragraphs", "text", "paragraph", "text", "for", "original", "replace", "in", "replacements", "items", "if", "original", "in", "replace", "and", "replace", "in", "text", "continue", "if", "original", "in", "text", "changes", "true", "text", "text", "replace", "original", "replace", "paragraph", "text", "text", "if", "changes", "print", "changing", "format", "file", "document", "save", "file"], "docstring": "replace in file", "docstring_tokens": ["replace", "in", "file"], "idx": 332}
{"url": "https://github.com/etingof/pysnmp/blob/cde062dd42f67dfd2d7686286a322d40e9c3a4b7/pysnmp/proto/secmod/rfc3414/priv/des.py#L108-L130", "repo": "pysnmp", "func_name": "encryptData", "original_string": ["    def encryptData(self, encryptKey, privParameters, dataToEncrypt):\n", "        snmpEngineBoots, snmpEngineTime, salt = privParameters\n", "\n", "        # 8.3.1.1\n", "        desKey, salt, iv = self._getEncryptionKey(encryptKey, snmpEngineBoots)\n", "\n", "        # 8.3.1.2\n", "        privParameters = univ.OctetString(salt)\n", "\n", "        # 8.1.1.2\n", "        plaintext = dataToEncrypt\n", "        plaintext += univ.OctetString(\n", "            (0,) * (8 - len(dataToEncrypt) % 8)).asOctets()\n", "\n", "        try:\n", "            ciphertext = des.encrypt(plaintext, desKey, iv)\n", "\n", "        except PysnmpCryptoError:\n", "            raise error.StatusInformation(\n", "                errorIndication=errind.unsupportedPrivProtocol)\n", "\n", "        # 8.3.1.3 & 4\n", "        return univ.OctetString(ciphertext), privParameters\n"], "language": "python", "code": "def encryptData(self, encryptKey, privParameters, dataToEncrypt):\n    snmpEngineBoots, snmpEngineTime, salt = privParameters\n    desKey, salt, iv = self._getEncryptionKey(encryptKey, snmpEngineBoots)\n    privParameters = univ.OctetString(salt)\n    plaintext = dataToEncrypt\n    plaintext += univ.OctetString((0,) * (8 - len(dataToEncrypt) % 8)\n        ).asOctets()\n    try:\n        ciphertext = des.encrypt(plaintext, desKey, iv)\n    except PysnmpCryptoError:\n        raise error.StatusInformation(errorIndication=errind.\n            unsupportedPrivProtocol)\n    return univ.OctetString(ciphertext), privParameters\n", "code_tokens": ["encryptdata", "self", "encryptkey", "privparameters", "datatoencrypt", "snmpengineboots", "snmpenginetime", "salt", "privparameters", "deskey", "salt", "iv", "self", "getencryptionkey", "encryptkey", "snmpengineboots", "privparameters", "univ", "octetstring", "salt", "plaintext", "datatoencrypt", "plaintext", "univ", "octetstring", "0", "8", "len", "datatoencrypt", "8", "asoctets", "try", "ciphertext", "des", "encrypt", "plaintext", "deskey", "iv", "except", "pysnmpcryptoerror", "raise", "error", "statusinformation", "errorindication", "errind", "unsupportedprivprotocol", "return", "univ", "octetstring", "ciphertext", "privparameters"], "docstring": "aes encryption", "docstring_tokens": ["aes", "encryption"], "idx": 333}
{"url": "https://github.com/kalefranz/auxlib/blob/6ff2d6b57d128d0b9ed8f01ad83572e938da064f/auxlib/crypt.py#L77-L97", "repo": "auxlib", "func_name": "aes_encrypt", "original_string": ["def aes_encrypt(base64_encryption_key, data):\n", "    \"\"\"Encrypt data with AES-CBC and sign it with HMAC-SHA256\n", "\n", "    Arguments:\n", "        base64_encryption_key (str): a base64-encoded string containing an AES encryption key\n", "            and HMAC signing key as generated by generate_encryption_key()\n", "        data (str): a byte string containing the data to be encrypted\n", "\n", "    Returns:\n", "        str: the encrypted data as a byte string with the HMAC signature appended to the end\n", "\n", "    \"\"\"\n", "    if isinstance(data, text_type):\n", "        data = data.encode(\"UTF-8\")\n", "    aes_key_bytes, hmac_key_bytes = _extract_keys(base64_encryption_key)\n", "    data = _pad(data)\n", "    iv_bytes = os.urandom(AES_BLOCK_SIZE)\n", "    cipher = AES.new(aes_key_bytes, mode=AES.MODE_CBC, IV=iv_bytes)\n", "    data = iv_bytes + cipher.encrypt(data)  # prepend init vector\n", "    hmac_signature = hmac.new(hmac_key_bytes, data, hashlib.sha256).digest()\n", "    return as_base64(data + hmac_signature)\n"], "language": "python", "code": "def aes_encrypt(base64_encryption_key, data):\n    \"\"\"\"\"\"\n    if isinstance(data, text_type):\n        data = data.encode('UTF-8')\n    aes_key_bytes, hmac_key_bytes = _extract_keys(base64_encryption_key)\n    data = _pad(data)\n    iv_bytes = os.urandom(AES_BLOCK_SIZE)\n    cipher = AES.new(aes_key_bytes, mode=AES.MODE_CBC, IV=iv_bytes)\n    data = iv_bytes + cipher.encrypt(data)\n    hmac_signature = hmac.new(hmac_key_bytes, data, hashlib.sha256).digest()\n    return as_base64(data + hmac_signature)\n", "code_tokens": ["aes", "encrypt", "encryption", "key", "data", "if", "isinstance", "data", "text", "type", "data", "data", "encode", "utf", "8", "aes", "key", "bytes", "hmac", "key", "bytes", "extract", "keys", "encryption", "key", "data", "pad", "data", "iv", "bytes", "os", "urandom", "aes", "block", "size", "cipher", "aes", "new", "aes", "key", "bytes", "mode", "aes", "mode", "cbc", "iv", "iv", "bytes", "data", "iv", "bytes", "cipher", "encrypt", "data", "hmac", "signature", "hmac", "new", "hmac", "key", "bytes", "data", "hashlib", "digest", "return", "as", "data", "hmac", "signature"], "docstring": "aes encryption", "docstring_tokens": ["aes", "encryption"], "idx": 334}
{"url": "https://github.com/raymontag/kppy/blob/a43f1fff7d49da1da4b3d8628a1b3ebbaf47f43a/kppy/database.py#L861-L871", "repo": "kppy", "func_name": "_cbc_encrypt", "original_string": ["    def _cbc_encrypt(self, content, final_key):\n", "        \"\"\"This method encrypts the content.\"\"\"\n", "\n", "        aes = AES.new(final_key, AES.MODE_CBC, self._enc_iv)\n", "        padding = (16 - len(content) % AES.block_size)\n", "\n", "        for _ in range(padding):\n", "            content += chr(padding).encode()\n", "\n", "        temp = bytes(content)\n", "        return aes.encrypt(temp)\n"], "language": "python", "code": "def _cbc_encrypt(self, content, final_key):\n    \"\"\"\"\"\"\n    aes = AES.new(final_key, AES.MODE_CBC, self._enc_iv)\n    padding = 16 - len(content) % AES.block_size\n    for _ in range(padding):\n        content += chr(padding).encode()\n    temp = bytes(content)\n    return aes.encrypt(temp)\n", "code_tokens": ["cbc", "encrypt", "self", "content", "final", "key", "aes", "aes", "new", "final", "key", "aes", "mode", "cbc", "self", "enc", "iv", "padding", "16", "len", "content", "aes", "block", "size", "for", "in", "range", "padding", "content", "chr", "padding", "encode", "temp", "bytes", "content", "return", "aes", "encrypt", "temp"], "docstring": "aes encryption", "docstring_tokens": ["aes", "encryption"], "idx": 335}
{"url": "https://github.com/jaredLunde/vital-tools/blob/ea924c9bbb6ec22aa66f8095f018b1ee0099ac04/vital/security/__init__.py#L79-L98", "repo": "vital-tools", "func_name": "aes_encrypt", "original_string": ["def aes_encrypt(value, secret, block_size=AES.block_size):\n", "    \"\"\" AES encrypt @value with @secret using the |CFB| mode of AES\n", "        with a cryptographically secure initialization vector.\n", "\n", "        -> (#bytes) AES encrypted @value\n", "\n", "        ..\n", "            from vital.security import aes_encrypt, aes_decrypt\n", "            aes_encrypt(\"Hello, world\",\n", "                        \"aLWEFlwgwlreWELFNWEFWLEgwklgbweLKWEBGW\")\n", "            # -> 'zYgVYMbeOuiHR50aMFinY9JsfyMQCvpzI+LNqNcmZhw='\n", "            aes_decrypt(\n", "                \"zYgVYMbeOuiHR50aMFinY9JsfyMQCvpzI+LNqNcmZhw=\",\n", "                \"aLWEFlwgwlreWELFNWEFWLEgwklgbweLKWEBGW\")\n", "            # -> 'Hello, world'\n", "        ..f\n", "    \"\"\"\n", "    iv = os.urandom(block_size * 2)\n", "    cipher = AES.new(secret[:32], AES.MODE_CFB, iv[:block_size])\n", "    return b'%s%s' % (iv, cipher.encrypt(value))\n"], "language": "python", "code": "def aes_encrypt(value, secret, block_size=AES.block_size):\n    \"\"\"\"\"\"\n    iv = os.urandom(block_size * 2)\n    cipher = AES.new(secret[:32], AES.MODE_CFB, iv[:block_size])\n    return b'%s%s' % (iv, cipher.encrypt(value))\n", "code_tokens": ["aes", "encrypt", "value", "secret", "block", "size", "aes", "block", "size", "iv", "os", "urandom", "block", "size", "2", "cipher", "aes", "new", "secret", "32", "aes", "mode", "cfb", "iv", "block", "size", "return", "iv", "cipher", "encrypt", "value"], "docstring": "aes encryption", "docstring_tokens": ["aes", "encryption"], "idx": 336}
{"url": "https://github.com/pedroburon/tbk/blob/ecd6741e0bae06269eb4ac885c3ffcb7902ee40e/tbk/webpay/encryption.py#L36-L42", "repo": "tbk", "func_name": "encrypt_message", "original_string": ["    def encrypt_message(self, signed_message, message, key, iv):\n", "        raw = signed_message + message\n", "        block_size = AES.block_size\n", "        pad = lambda s: s + (block_size - len(s) % block_size) * chr(block_size - len(s) % block_size).encode('utf-8')\n", "        message_to_encrypt = pad(raw)\n", "        cipher = AES.new(key, AES.MODE_CBC, iv)\n", "        return cipher.encrypt(message_to_encrypt)\n"], "language": "python", "code": "def encrypt_message(self, signed_message, message, key, iv):\n    raw = signed_message + message\n    block_size = AES.block_size\n    pad = lambda s: s + (block_size - len(s) % block_size) * chr(block_size -\n        len(s) % block_size).encode('utf-8')\n    message_to_encrypt = pad(raw)\n    cipher = AES.new(key, AES.MODE_CBC, iv)\n    return cipher.encrypt(message_to_encrypt)\n", "code_tokens": ["encrypt", "message", "self", "signed", "message", "message", "key", "iv", "raw", "signed", "message", "message", "block", "size", "aes", "block", "size", "pad", "lambda", "block", "size", "len", "block", "size", "chr", "block", "size", "len", "block", "size", "encode", "utf", "8", "message", "to", "encrypt", "pad", "raw", "cipher", "aes", "new", "key", "aes", "mode", "cbc", "iv", "return", "cipher", "encrypt", "message", "to", "encrypt"], "docstring": "aes encryption", "docstring_tokens": ["aes", "encryption"], "idx": 337}
{"url": "https://github.com/konomae/lastpass-python/blob/5063911b789868a1fd9db9922db82cdf156b938a/lastpass/parser.py#L269-L284", "repo": "lastpass-python", "func_name": "decode_aes256", "original_string": ["def decode_aes256(cipher, iv, data, encryption_key):\n", "    \"\"\"\n", "    Decrypt AES-256 bytes.\n", "    Allowed ciphers are: :ecb, :cbc.\n", "    If for :ecb iv is not used and should be set to \"\".\n", "    \"\"\"\n", "    if cipher == 'cbc':\n", "        aes = AES.new(encryption_key, AES.MODE_CBC, iv)\n", "    elif cipher == 'ecb':\n", "        aes = AES.new(encryption_key, AES.MODE_ECB)\n", "    else:\n", "        raise ValueError('Unknown AES mode')\n", "    d = aes.decrypt(data)\n", "    # http://passingcuriosity.com/2009/aes-encryption-in-python-with-m2crypto/\n", "    unpad = lambda s: s[0:-ord(d[-1:])]\n", "    return unpad(d)\n"], "language": "python", "code": "def decode_aes256(cipher, iv, data, encryption_key):\n    \"\"\"\"\"\"\n    if cipher == 'cbc':\n        aes = AES.new(encryption_key, AES.MODE_CBC, iv)\n    elif cipher == 'ecb':\n        aes = AES.new(encryption_key, AES.MODE_ECB)\n    else:\n        raise ValueError('Unknown AES mode')\n    d = aes.decrypt(data)\n    unpad = lambda s: s[0:-ord(d[-1:])]\n    return unpad(d)\n", "code_tokens": ["decode", "cipher", "iv", "data", "encryption", "key", "if", "cipher", "cbc", "aes", "aes", "new", "encryption", "key", "aes", "mode", "cbc", "iv", "elif", "cipher", "ecb", "aes", "aes", "new", "encryption", "key", "aes", "mode", "ecb", "else", "raise", "valueerror", "unknown", "aes", "mode", "aes", "decrypt", "data", "unpad", "lambda", "0", "ord", "1", "return", "unpad"], "docstring": "aes encryption", "docstring_tokens": ["aes", "encryption"], "idx": 338}
{"url": "https://github.com/frispete/keyrings.cryptfile/blob/cfa80d4848a5c3c0aeee41a954b2b120c80e69b2/keyrings/cryptfile/cryptfile.py#L132-L162", "repo": "keyrings.cryptfile", "func_name": "_check_scheme", "original_string": ["    def _check_scheme(self, config):\n", "        \"\"\"\n", "        check for a valid scheme\n", "\n", "        raise AttributeError if missing\n", "        raise ValueError if not valid\n", "        \"\"\"\n", "        try:\n", "            scheme = config.get(\n", "                escape_for_ini('keyring-setting'),\n", "                escape_for_ini('scheme'),\n", "            )\n", "        except (configparser.NoSectionError, configparser.NoOptionError):\n", "            raise AttributeError(\"Encryption scheme missing\")\n", "\n", "        # extract AES mode\n", "        aesmode = scheme[-3:]\n", "        if aesmode not in self._get_mode():\n", "            raise ValueError(\"Encryption scheme invalid: %s\" % (aesmode))\n", "\n", "        # setup AES mode\n", "        self.aesmode = aesmode\n", "\n", "        # remove pointless crypto module name\n", "        if scheme.startswith('PyCryptodome '):\n", "            scheme = scheme[13:]\n", "\n", "        # check other scheme properties\n", "        if scheme != self.scheme:\n", "            raise ValueError(\"Encryption scheme mismatch \"\n", "                             \"(exp.: %s, found: %s)\" % (self.scheme, scheme))\n"], "language": "python", "code": "def _check_scheme(self, config):\n    \"\"\"\"\"\"\n    try:\n        scheme = config.get(escape_for_ini('keyring-setting'),\n            escape_for_ini('scheme'))\n    except (configparser.NoSectionError, configparser.NoOptionError):\n        raise AttributeError('Encryption scheme missing')\n    aesmode = scheme[-3:]\n    if aesmode not in self._get_mode():\n        raise ValueError('Encryption scheme invalid: %s' % aesmode)\n    self.aesmode = aesmode\n    if scheme.startswith('PyCryptodome '):\n        scheme = scheme[13:]\n    if scheme != self.scheme:\n        raise ValueError('Encryption scheme mismatch (exp.: %s, found: %s)' %\n            (self.scheme, scheme))\n", "code_tokens": ["check", "scheme", "self", "config", "try", "scheme", "config", "get", "escape", "for", "ini", "keyring", "setting", "escape", "for", "ini", "scheme", "except", "configparser", "nosectionerror", "configparser", "nooptionerror", "raise", "attributeerror", "encryption", "scheme", "missing", "aesmode", "scheme", "3", "if", "aesmode", "not", "in", "self", "get", "mode", "raise", "valueerror", "encryption", "scheme", "invalid", "aesmode", "self", "aesmode", "aesmode", "if", "scheme", "startswith", "pycryptodome", "scheme", "scheme", "13", "if", "scheme", "self", "scheme", "raise", "valueerror", "encryption", "scheme", "mismatch", "exp", "found", "self", "scheme", "scheme"], "docstring": "aes encryption", "docstring_tokens": ["aes", "encryption"], "idx": 339}
{"url": "https://github.com/hackedd/gw2api/blob/5543a78e6e3ed0573b7e84c142c44004b4779eac/gw2api/map.py#L50-L105", "repo": "gw2api", "func_name": "maps", "original_string": ["def maps(map_id=None, lang=\"en\"):\n", "    \"\"\"This resource returns details about maps in the game, including details\n", "    about floor and translation data on how to translate between world\n", "    coordinates and map coordinates.\n", "\n", "    :param map_id: Only list this map.\n", "    :param lang: Show localized texts in the specified language.\n", "\n", "    The response is a dictionary where the key is the map id and the value is\n", "    a dictionary containing the following properties:\n", "\n", "    map_name (string)\n", "        The map name.\n", "\n", "    min_level (number)\n", "        The minimal level of this map.\n", "\n", "    max_level (number)\n", "        The maximum level of this map.\n", "\n", "    default_floor (number)\n", "        The default floor of this map.\n", "\n", "    floors (list)\n", "        A list of available floors for this map.\n", "\n", "    region_id (number)\n", "        The id of the region this map belongs to.\n", "\n", "    region_name (string)\n", "        The name of the region this map belongs to.\n", "\n", "    continent_id (number)\n", "        The id of the continent this map belongs to.\n", "\n", "    continent_name (string)\n", "        The name of the continent this map belongs to.\n", "\n", "    map_rect (rect)\n", "        The dimensions of the map.\n", "\n", "    continent_rect (rect)\n", "        The dimensions of the map within the continent coordinate system.\n", "\n", "    If a map_id is given, only the values for that map are returned.\n", "\n", "    \"\"\"\n", "    if map_id:\n", "        cache_name = \"maps.%s.%s.json\" % (map_id, lang)\n", "        params = {\"map_id\": map_id, \"lang\": lang}\n", "    else:\n", "        cache_name = \"maps.%s.json\" % lang\n", "        params = {\"lang\": lang}\n", "\n", "    data = get_cached(\"maps.json\", cache_name, params=params).get(\"maps\")\n", "    return data.get(str(map_id)) if map_id else data\n"], "language": "python", "code": "def maps(map_id=None, lang='en'):\n    \"\"\"\"\"\"\n    if map_id:\n        cache_name = 'maps.%s.%s.json' % (map_id, lang)\n        params = {'map_id': map_id, 'lang': lang}\n    else:\n        cache_name = 'maps.%s.json' % lang\n        params = {'lang': lang}\n    data = get_cached('maps.json', cache_name, params=params).get('maps')\n    return data.get(str(map_id)) if map_id else data\n", "code_tokens": ["maps", "map", "id", "none", "lang", "en", "if", "map", "id", "cache", "name", "maps", "json", "map", "id", "lang", "params", "map", "id", "map", "id", "lang", "lang", "else", "cache", "name", "maps", "json", "lang", "params", "lang", "lang", "data", "get", "cached", "maps", "json", "cache", "name", "params", "params", "get", "maps", "return", "data", "get", "str", "map", "id", "if", "map", "id", "else", "data"], "docstring": "map to json", "docstring_tokens": ["map", "to", "json"], "idx": 340}
{"url": "https://github.com/mapbox/mapboxgl-jupyter/blob/f6e403c13eaa910e70659c7d179e8e32ce95ae34/mapboxgl/viz.py#L20-L37", "repo": "mapboxgl-jupyter", "func_name": "generate_vector_color_map", "original_string": ["    def generate_vector_color_map(self):\n", "        \"\"\"Generate color stops array for use with match expression in mapbox template\"\"\"\n", "        vector_stops = []\n", "\n", "        # if join data specified as filename or URL, parse JSON to list of Python dicts\n", "        if type(self.data) == str:\n", "            self.data = geojson_to_dict_list(self.data)\n", "\n", "        # loop through features in self.data to create join-data map\n", "        for row in self.data:\n", "            \n", "            # map color to JSON feature using color_property\n", "            color = color_map(row[self.color_property], self.color_stops, self.color_default)\n", "\n", "            # link to vector feature using data_join_property (from JSON object)\n", "            vector_stops.append([row[self.data_join_property], color])\n", "\n", "        return vector_stops\n"], "language": "python", "code": "def generate_vector_color_map(self):\n    \"\"\"\"\"\"\n    vector_stops = []\n    if type(self.data) == str:\n        self.data = geojson_to_dict_list(self.data)\n    for row in self.data:\n        color = color_map(row[self.color_property], self.color_stops, self.\n            color_default)\n        vector_stops.append([row[self.data_join_property], color])\n    return vector_stops\n", "code_tokens": ["generate", "vector", "color", "map", "self", "vector", "stops", "if", "type", "self", "data", "str", "self", "data", "geojson", "to", "dict", "list", "self", "data", "for", "row", "in", "self", "data", "color", "color", "map", "row", "self", "color", "property", "self", "color", "stops", "self", "color", "default", "vector", "stops", "append", "row", "self", "data", "join", "property", "color", "return", "vector", "stops"], "docstring": "map to json", "docstring_tokens": ["map", "to", "json"], "idx": 341}
{"url": "https://github.com/kyper-data/python-highcharts/blob/a4c488ae5c2e125616efad5a722f3dfd8a9bc450/highcharts/highmaps/highmaps.py#L292-L315", "repo": "python-highcharts", "func_name": "set_map_source", "original_string": ["    def set_map_source(self, map_src, jsonp_map = False):\n", "        \"\"\"set map data \n", "        use if the mapData is loaded directly from a https source\n", "        the map_src is the https link for the mapData\n", "        geojson (from jsonp) or .js formates are acceptable\n", "        default is js script from highcharts' map collection: https://code.highcharts.com/mapdata/\n", "        \"\"\"\n", "\n", "        if not map_src:\n", "            raise OptionTypeError(\"No map source input, please refer to: https://code.highcharts.com/mapdata/\")\n", "        \n", "        if  jsonp_map:\n", "            self.jsonp_map_flag = True\n", "            self.map = 'geojson'\n", "            self.jsonp_map_url = json.dumps(map_src)\n", "        else:\n", "            self.add_JSsource(map_src)\n", "            map_name = self._get_jsmap_name(map_src)\n", "            self.map = 'geojson'\n", "            self.jsmap = self.map + ' = Highcharts.geojson(' + map_name + ');'\n", "            self.add_JSscript('var ' + self.jsmap, 'head')\n", "\n", "        if self.data_temp:\n", "            self.data_temp[0].__options__().update({'mapData': MapObject(self.map)})\n"], "language": "python", "code": "def set_map_source(self, map_src, jsonp_map=False):\n    \"\"\"\"\"\"\n    if not map_src:\n        raise OptionTypeError(\n            'No map source input, please refer to: https://code.highcharts.com/mapdata/'\n            )\n    if jsonp_map:\n        self.jsonp_map_flag = True\n        self.map = 'geojson'\n        self.jsonp_map_url = json.dumps(map_src)\n    else:\n        self.add_JSsource(map_src)\n        map_name = self._get_jsmap_name(map_src)\n        self.map = 'geojson'\n        self.jsmap = self.map + ' = Highcharts.geojson(' + map_name + ');'\n        self.add_JSscript('var ' + self.jsmap, 'head')\n    if self.data_temp:\n        self.data_temp[0].__options__().update({'mapData': MapObject(self.map)}\n            )\n", "code_tokens": ["set", "map", "source", "self", "map", "src", "jsonp", "map", "false", "if", "not", "map", "src", "raise", "optiontypeerror", "no", "map", "source", "input", "please", "refer", "to", "https", "code", "highcharts", "com", "mapdata", "if", "jsonp", "map", "self", "jsonp", "map", "flag", "true", "self", "map", "geojson", "self", "jsonp", "map", "url", "json", "dumps", "map", "src", "else", "self", "add", "jssource", "map", "src", "map", "name", "self", "get", "jsmap", "name", "map", "src", "self", "map", "geojson", "self", "jsmap", "self", "map", "highcharts", "geojson", "map", "name", "self", "add", "jsscript", "var", "self", "jsmap", "head", "if", "self", "data", "temp", "self", "data", "temp", "0", "options", "update", "mapdata", "mapobject", "self", "map"], "docstring": "map to json", "docstring_tokens": ["map", "to", "json"], "idx": 342}
{"url": "https://github.com/tensorflow/tensor2tensor/blob/272500b6efe353aeb638d2745ed56e519462ca31/tensor2tensor/utils/hparam.py#L558-L572", "repo": "tensor2tensor", "func_name": "parse_json", "original_string": ["  def parse_json(self, values_json):\n", "    \"\"\"Override existing hyperparameter values, parsing new values from a json object.\n", "\n", "    Args:\n", "      values_json: String containing a json object of name:value pairs.\n", "\n", "    Returns:\n", "      The `HParams` instance.\n", "\n", "    Raises:\n", "      KeyError: If a hyperparameter in `values_json` doesn't exist.\n", "      ValueError: If `values_json` cannot be parsed.\n", "    \"\"\"\n", "    values_map = json.loads(values_json)\n", "    return self.override_from_dict(values_map)\n"], "language": "python", "code": "def parse_json(self, values_json):\n    \"\"\"\"\"\"\n    values_map = json.loads(values_json)\n    return self.override_from_dict(values_map)\n", "code_tokens": ["parse", "json", "self", "values", "json", "values", "map", "json", "loads", "values", "json", "return", "self", "override", "from", "dict", "values", "map"], "docstring": "map to json", "docstring_tokens": ["map", "to", "json"], "idx": 343}
{"url": "https://github.com/grst/geos/blob/ea15abcc5d8f86c9051df55e489b7d941b51a638/geos/server.py#L25-L53", "repo": "geos", "func_name": "maps_json", "original_string": ["def maps_json():\n", "    \"\"\"\n", "    Generates a json object which serves as bridge between\n", "    the web interface and the map source collection.\n", "\n", "    All attributes relevant for openlayers are converted into\n", "    JSON and served through this route.\n", "\n", "    Returns:\n", "        Response: All map sources as JSON object.\n", "    \"\"\"\n", "    map_sources = {\n", "        id: {\n", "                \"id\": map_source.id,\n", "                \"name\": map_source.name,\n", "                \"folder\": map_source.folder,\n", "                \"min_zoom\": map_source.min_zoom,\n", "                \"max_zoom\": map_source.max_zoom,\n", "                \"layers\": [\n", "                    {\n", "                        \"min_zoom\": layer.min_zoom,\n", "                        \"max_zoom\": layer.max_zoom,\n", "                        \"tile_url\": layer.get_tile_urls,\n", "                    } for layer in map_source.layers\n", "                    ]\n", "\n", "            } for id, map_source in app.config[\"mapsources\"].items()\n", "        }\n", "    return jsonify(map_sources)\n"], "language": "python", "code": "def maps_json():\n    \"\"\"\"\"\"\n    map_sources = {id: {'id': map_source.id, 'name': map_source.name,\n        'folder': map_source.folder, 'min_zoom': map_source.min_zoom,\n        'max_zoom': map_source.max_zoom, 'layers': [{'min_zoom': layer.\n        min_zoom, 'max_zoom': layer.max_zoom, 'tile_url': layer.\n        get_tile_urls} for layer in map_source.layers]} for id, map_source in\n        app.config['mapsources'].items()}\n    return jsonify(map_sources)\n", "code_tokens": ["maps", "json", "map", "sources", "id", "id", "map", "source", "id", "name", "map", "source", "name", "folder", "map", "source", "folder", "min", "zoom", "map", "source", "min", "zoom", "max", "zoom", "map", "source", "max", "zoom", "layers", "min", "zoom", "layer", "min", "zoom", "max", "zoom", "layer", "max", "zoom", "tile", "url", "layer", "get", "tile", "urls", "for", "layer", "in", "map", "source", "layers", "for", "id", "map", "source", "in", "app", "config", "mapsources", "items", "return", "jsonify", "map", "sources"], "docstring": "map to json", "docstring_tokens": ["map", "to", "json"], "idx": 344}
{"url": "https://github.com/mattjj/pybasicbayes/blob/76aef00f011415cc5c858cd1a101f3aab971a62d/pybasicbayes/distributions/gaussian.py#L1017-L1019", "repo": "pybasicbayes", "func_name": "_empty_stats", "original_string": ["    def _empty_stats(self):\n", "        return np.array([0.,np.zeros_like(self.mu_0),np.zeros_like(self.mu_0)],\n", "                dtype=np.object)\n"], "language": "python", "code": "def _empty_stats(self):\n    return np.array([0.0, np.zeros_like(self.mu_0), np.zeros_like(self.mu_0\n        )], dtype=np.object)\n", "code_tokens": ["empty", "stats", "self", "return", "np", "array", "0", "0", "np", "zeros", "like", "self", "mu", "0", "np", "zeros", "like", "self", "mu", "0", "dtype", "np", "object"], "docstring": "how to empty array", "docstring_tokens": ["how", "to", "empty", "array"], "idx": 345}
{"url": "https://github.com/supercoderz/pyeurofx/blob/3f579bb6e4836dadb187df8c74a9d186ae7e39e7/eurofx/common.py#L88-L98", "repo": "pyeurofx", "func_name": "get_date", "original_string": ["def get_date(date_string):\n", "    d=None\n", "    try:\n", "        d=datetime.datetime.strptime(date_string, \"%d %B %Y\").date()\n", "    except:\n", "        d=datetime.datetime.strptime(date_string, \"%Y-%m-%d\").date()\n", "\n", "    if d:\n", "        return d.strftime(\"%Y-%m-%d\")\n", "    else:\n", "        return date_string\n"], "language": "python", "code": "def get_date(date_string):\n    d = None\n    try:\n        d = datetime.datetime.strptime(date_string, '%d %B %Y').date()\n    except:\n        d = datetime.datetime.strptime(date_string, '%Y-%m-%d').date()\n    if d:\n        return d.strftime('%Y-%m-%d')\n    else:\n        return date_string\n", "code_tokens": ["get", "date", "date", "string", "none", "try", "datetime", "datetime", "strptime", "date", "string", "date", "except", "datetime", "datetime", "strptime", "date", "string", "date", "if", "return", "strftime", "else", "return", "date", "string"], "docstring": "string to date", "docstring_tokens": ["string", "to", "date"], "idx": 346}
{"url": "https://github.com/robgolding/tasklib/blob/0ad882377639865283021041f19add5aeb10126a/tasklib/serializing.py#L71-L79", "repo": "tasklib", "func_name": "timestamp_serializer", "original_string": ["    def timestamp_serializer(self, date):\n", "        if not date:\n", "            return ''\n", "\n", "        # Any serialized timestamp should be localized, we need to\n", "        # convert to UTC before converting to string (DATE_FORMAT uses UTC)\n", "        date = date.astimezone(ZoneInfo('UTC'))\n", "\n", "        return date.strftime(DATE_FORMAT)\n"], "language": "python", "code": "def timestamp_serializer(self, date):\n    if not date:\n        return ''\n    date = date.astimezone(ZoneInfo('UTC'))\n    return date.strftime(DATE_FORMAT)\n", "code_tokens": ["timestamp", "serializer", "self", "date", "if", "not", "date", "return", "date", "date", "astimezone", "zoneinfo", "utc", "return", "date", "strftime", "date", "format"], "docstring": "string to date", "docstring_tokens": ["string", "to", "date"], "idx": 347}
{"url": "https://github.com/eliangcs/pystock-crawler/blob/8b803c8944f36af46daf04c6767a74132e37a101/pystock_crawler/spiders/yahoo.py#L12-L16", "repo": "pystock-crawler", "func_name": "parse_date", "original_string": ["def parse_date(date_str):\n", "    if date_str:\n", "        date = datetime.strptime(date_str, '%Y%m%d')\n", "        return date.year, date.month - 1, date.day\n", "    return '', '', ''\n"], "language": "python", "code": "def parse_date(date_str):\n    if date_str:\n        date = datetime.strptime(date_str, '%Y%m%d')\n        return date.year, date.month - 1, date.day\n    return '', '', ''\n", "code_tokens": ["parse", "date", "date", "str", "if", "date", "str", "date", "datetime", "strptime", "date", "str", "return", "date", "year", "date", "month", "1", "date", "day", "return"], "docstring": "string to date", "docstring_tokens": ["string", "to", "date"], "idx": 348}
{"url": "https://github.com/EdwinvO/pyutillib/blob/6d773c31d1f27cc5256d47feb8afb5c3ae5f0db5/pyutillib/date_utils.py#L35-L84", "repo": "pyutillib", "func_name": "datestr2date", "original_string": ["def datestr2date(date_str):\n", "    '''\n", "    Turns a string into a datetime.date object. This will only work if the \n", "    format can be \"guessed\", so the string must have one of the formats from\n", "    VALID_DATE_FORMATS_TEXT.\n", "\n", "    Args:\n", "        date_str (str) a string that represents a date\n", "    Returns:\n", "        datetime.date object\n", "    Raises:\n", "        ValueError if the input string does not have a valid format.\n", "    '''\n", "    if any(c not in '0123456789-/' for c in date_str):\n", "        raise ValueError('Illegal character in date string')\n", "    if '/' in date_str:\n", "        try:\n", "            m, d, y = date_str.split('/')\n", "        except:\n", "            raise ValueError('Date {} must have no or exactly 2 slashes. {}'.\n", "                    format(date_str, VALID_DATE_FORMATS_TEXT))\n", "    elif '-' in date_str:\n", "        try:\n", "            d, m, y = date_str.split('-')\n", "        except:\n", "            raise ValueError('Date {} must have no or exactly 2 dashes. {}'.\n", "                    format(date_str, VALID_DATE_FORMATS_TEXT))\n", "    elif len(date_str) == 8 or len(date_str) == 6:\n", "        d = date_str[-2:]\n", "        m = date_str[-4:-2]\n", "        y = date_str[:-4]\n", "    else:\n", "        raise ValueError('Date format not recognised. {}'.format(\n", "                VALID_DATE_FORMATS_TEXT))\n", "    if len(y) == 2:\n", "        year = 2000 + int(y)\n", "    elif len(y) == 4:\n", "        year = int(y)\n", "    else:\n", "        raise ValueError('year must be 2 or 4 digits')\n", "    for s in (m, d):\n", "        if 1 <= len(s) <= 2:\n", "            month, day = int(m), int(d)\n", "        else:\n", "            raise ValueError('m and d must be 1 or 2 digits')\n", "    try:\n", "        return datetime.date(year, month, day)\n", "    except ValueError:\n", "        raise ValueError('Invalid date {}. {}'.format(date_str, \n", "                VALID_DATE_FORMATS_TEXT))\n"], "language": "python", "code": "def datestr2date(date_str):\n    \"\"\"\"\"\"\n    if any(c not in '0123456789-/' for c in date_str):\n        raise ValueError('Illegal character in date string')\n    if '/' in date_str:\n        try:\n            m, d, y = date_str.split('/')\n        except:\n            raise ValueError('Date {} must have no or exactly 2 slashes. {}'\n                .format(date_str, VALID_DATE_FORMATS_TEXT))\n    elif '-' in date_str:\n        try:\n            d, m, y = date_str.split('-')\n        except:\n            raise ValueError('Date {} must have no or exactly 2 dashes. {}'\n                .format(date_str, VALID_DATE_FORMATS_TEXT))\n    elif len(date_str) == 8 or len(date_str) == 6:\n        d = date_str[-2:]\n        m = date_str[-4:-2]\n        y = date_str[:-4]\n    else:\n        raise ValueError('Date format not recognised. {}'.format(\n            VALID_DATE_FORMATS_TEXT))\n    if len(y) == 2:\n        year = 2000 + int(y)\n    elif len(y) == 4:\n        year = int(y)\n    else:\n        raise ValueError('year must be 2 or 4 digits')\n    for s in (m, d):\n        if 1 <= len(s) <= 2:\n            month, day = int(m), int(d)\n        else:\n            raise ValueError('m and d must be 1 or 2 digits')\n    try:\n        return datetime.date(year, month, day)\n    except ValueError:\n        raise ValueError('Invalid date {}. {}'.format(date_str,\n            VALID_DATE_FORMATS_TEXT))\n", "code_tokens": ["date", "str", "if", "any", "not", "in", "0123456789", "for", "in", "date", "str", "raise", "valueerror", "illegal", "character", "in", "date", "string", "if", "in", "date", "str", "try", "date", "str", "split", "except", "raise", "valueerror", "date", "must", "have", "no", "or", "exactly", "2", "slashes", "format", "date", "str", "valid", "date", "formats", "text", "elif", "in", "date", "str", "try", "date", "str", "split", "except", "raise", "valueerror", "date", "must", "have", "no", "or", "exactly", "2", "dashes", "format", "date", "str", "valid", "date", "formats", "text", "elif", "len", "date", "str", "8", "or", "len", "date", "str", "6", "date", "str", "2", "date", "str", "4", "2", "date", "str", "4", "else", "raise", "valueerror", "date", "format", "not", "recognised", "format", "valid", "date", "formats", "text", "if", "len", "2", "year", "2000", "int", "elif", "len", "4", "year", "int", "else", "raise", "valueerror", "year", "must", "be", "2", "or", "4", "digits", "for", "in", "if", "1", "len", "2", "month", "day", "int", "int", "else", "raise", "valueerror", "and", "must", "be", "1", "or", "2", "digits", "try", "return", "datetime", "date", "year", "month", "day", "except", "valueerror", "raise", "valueerror", "invalid", "date", "format", "date", "str", "valid", "date", "formats", "text"], "docstring": "string to date", "docstring_tokens": ["string", "to", "date"], "idx": 349}
{"url": "https://github.com/lrq3000/pyFileFixity/blob/fd5ef23bb13835faf1e3baa773619b86a1cc9bdf/pyFileFixity/lib/profilers/pyinstrument/profiler.py#L159-L185", "repo": "pyFileFixity", "func_name": "output_html", "original_string": ["    def output_html(self, root=False):\n", "        resources_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), 'resources/')\n", "\n", "        with open(os.path.join(resources_dir, 'style.css')) as f:\n", "            css = f.read()\n", "\n", "        with open(os.path.join(resources_dir, 'profile.js')) as f:\n", "            js = f.read()\n", "\n", "        with open(os.path.join(resources_dir, 'jquery-1.11.0.min.js')) as f:\n", "            jquery_js = f.read()\n", "\n", "        body = self.starting_frame(root).as_html()\n", "\n", "        page = '''\n", "            <html>\n", "            <head>\n", "                <style>{css}</style>\n", "                <script>{jquery_js}</script>\n", "            </head>\n", "            <body>\n", "                {body}\n", "                <script>{js}</script>\n", "            </body>\n", "            </html>'''.format(css=css, js=js, jquery_js=jquery_js, body=body)\n", "\n", "        return page\n"], "language": "python", "code": "def output_html(self, root=False):\n    resources_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)),\n        'resources/')\n    with open(os.path.join(resources_dir, 'style.css')) as f:\n        css = f.read()\n    with open(os.path.join(resources_dir, 'profile.js')) as f:\n        js = f.read()\n    with open(os.path.join(resources_dir, 'jquery-1.11.0.min.js')) as f:\n        jquery_js = f.read()\n    body = self.starting_frame(root).as_html()\n    page = (\n        \"\"\"\n            <html>\n            <head>\n                <style>{css}</style>\n                <script>{jquery_js}</script>\n            </head>\n            <body>\n                {body}\n                <script>{js}</script>\n            </body>\n            </html>\"\"\"\n        .format(css=css, js=js, jquery_js=jquery_js, body=body))\n    return page\n", "code_tokens": ["output", "html", "self", "root", "false", "resources", "dir", "os", "path", "join", "os", "path", "dirname", "os", "path", "abspath", "file", "resources", "with", "open", "os", "path", "join", "resources", "dir", "style", "css", "as", "css", "read", "with", "open", "os", "path", "join", "resources", "dir", "profile", "js", "as", "js", "read", "with", "open", "os", "path", "join", "resources", "dir", "jquery", "1", "11", "0", "min", "js", "as", "jquery", "js", "read", "body", "self", "starting", "frame", "root", "as", "html", "page", "html", "head", "style", "css", "style", "script", "jquery", "js", "script", "head", "body", "body", "script", "js", "script", "body", "html", "format", "css", "css", "js", "js", "jquery", "js", "jquery", "js", "body", "body", "return", "page"], "docstring": "output to html file", "docstring_tokens": ["output", "to", "html", "file"], "idx": 350}
{"url": "https://github.com/swharden/SWHLab/blob/a86c3c65323cec809a4bd4f81919644927094bf5/swhlab/indexing/indexing.py#L239-L256", "repo": "SWHLab", "func_name": "html_index", "original_string": ["    def html_index(self,launch=True):\n", "        html=\"<h1>MENU</h1>\"\n", "        htmlFiles=[x for x in self.files2 if x.endswith(\".html\")]\n", "        for htmlFile in cm.abfSort(htmlFiles):\n", "            if not htmlFile.endswith('_basic.html'):\n", "                continue\n", "            name=htmlFile.split(\"_\")[0]\n", "            if name in self.groups.keys():\n", "                html+='<a href=\"%s\" target=\"content\">%s</a> '%(htmlFile,name)\n", "                html+='<span style=\"color: #CCC;\">'\n", "                html+='[<a href=\"%s\" target=\"content\">int</a>]'%(name+\"_plot.html\")\n", "                html+='</span>'\n", "                html+='<br>'\n", "        style.save(html,os.path.abspath(self.folder2+\"/index_menu.html\"))\n", "        html=\"<h1>SPLASH</h1>\"\n", "        style.save(html,os.path.abspath(self.folder2+\"/index_splash.html\"))\n", "        style.frames(os.path.abspath(self.folder2+\"/index.html\"),launch=launch)\n", "        return\n"], "language": "python", "code": "def html_index(self, launch=True):\n    html = '<h1>MENU</h1>'\n    htmlFiles = [x for x in self.files2 if x.endswith('.html')]\n    for htmlFile in cm.abfSort(htmlFiles):\n        if not htmlFile.endswith('_basic.html'):\n            continue\n        name = htmlFile.split('_')[0]\n        if name in self.groups.keys():\n            html += '<a href=\"%s\" target=\"content\">%s</a> ' % (htmlFile, name)\n            html += '<span style=\"color: #CCC;\">'\n            html += '[<a href=\"%s\" target=\"content\">int</a>]' % (name +\n                '_plot.html')\n            html += '</span>'\n            html += '<br>'\n    style.save(html, os.path.abspath(self.folder2 + '/index_menu.html'))\n    html = '<h1>SPLASH</h1>'\n    style.save(html, os.path.abspath(self.folder2 + '/index_splash.html'))\n    style.frames(os.path.abspath(self.folder2 + '/index.html'), launch=launch)\n    return\n", "code_tokens": ["html", "index", "self", "launch", "true", "html", "menu", "htmlfiles", "for", "in", "self", "if", "endswith", "html", "for", "htmlfile", "in", "cm", "abfsort", "htmlfiles", "if", "not", "htmlfile", "endswith", "basic", "html", "continue", "name", "htmlfile", "split", "0", "if", "name", "in", "self", "groups", "keys", "html", "href", "target", "content", "htmlfile", "name", "html", "span", "style", "color", "ccc", "html", "href", "target", "content", "int", "name", "plot", "html", "html", "span", "html", "br", "style", "save", "html", "os", "path", "abspath", "self", "index", "menu", "html", "html", "splash", "style", "save", "html", "os", "path", "abspath", "self", "index", "splash", "html", "style", "frames", "os", "path", "abspath", "self", "index", "html", "launch", "launch", "return"], "docstring": "output to html file", "docstring_tokens": ["output", "to", "html", "file"], "idx": 351}
{"url": "https://github.com/openvax/topiary/blob/04f0077bc4bf1ad350a0e78c26fa48c55fe7813b/topiary/cli/outputs.py#L23-L61", "repo": "topiary", "func_name": "add_output_args", "original_string": ["def add_output_args(arg_parser):\n", "    output_group = arg_parser.add_argument_group(\n", "        title=\"Output\",\n", "        description=\"How and where to write results\")\n", "\n", "    output_group.add_argument(\n", "        \"--output-csv\",\n", "        default=None,\n", "        help=\"Path to output CSV file\")\n", "\n", "    output_group.add_argument(\n", "        \"--output-html\",\n", "        default=None,\n", "        help=\"Path to output HTML file\")\n", "\n", "    output_group.add_argument(\n", "        \"--output-csv-sep\",\n", "        default=\",\",\n", "        help=\"Separator for CSV file\")\n", "\n", "    output_group.add_argument(\n", "        \"--subset-output-columns\",\n", "        nargs=\"*\")\n", "\n", "    output_group.add_argument(\n", "        \"--rename-output-column\",\n", "        nargs=2,\n", "        action=\"append\",\n", "        help=(\n", "            \"Rename original column (first parameter) to new\"\n", "            \" name (second parameter)\"))\n", "\n", "    output_group.add_argument(\n", "        \"--print-columns\",\n", "        default=False,\n", "        action=\"store_true\",\n", "        help=\"Print columns before writing data to file(s)\")\n", "\n", "    return output_group\n"], "language": "python", "code": "def add_output_args(arg_parser):\n    output_group = arg_parser.add_argument_group(title='Output',\n        description='How and where to write results')\n    output_group.add_argument('--output-csv', default=None, help=\n        'Path to output CSV file')\n    output_group.add_argument('--output-html', default=None, help=\n        'Path to output HTML file')\n    output_group.add_argument('--output-csv-sep', default=',', help=\n        'Separator for CSV file')\n    output_group.add_argument('--subset-output-columns', nargs='*')\n    output_group.add_argument('--rename-output-column', nargs=2, action=\n        'append', help=\n        'Rename original column (first parameter) to new name (second parameter)'\n        )\n    output_group.add_argument('--print-columns', default=False, action=\n        'store_true', help='Print columns before writing data to file(s)')\n    return output_group\n", "code_tokens": ["add", "output", "args", "arg", "parser", "output", "group", "arg", "parser", "add", "argument", "group", "title", "output", "description", "how", "and", "where", "to", "write", "results", "output", "group", "add", "argument", "output", "csv", "default", "none", "help", "path", "to", "output", "csv", "file", "output", "group", "add", "argument", "output", "html", "default", "none", "help", "path", "to", "output", "html", "file", "output", "group", "add", "argument", "output", "csv", "sep", "default", "help", "separator", "for", "csv", "file", "output", "group", "add", "argument", "subset", "output", "columns", "nargs", "output", "group", "add", "argument", "rename", "output", "column", "nargs", "2", "action", "append", "help", "rename", "original", "column", "first", "parameter", "to", "new", "name", "second", "parameter", "output", "group", "add", "argument", "print", "columns", "default", "false", "action", "store", "true", "help", "print", "columns", "before", "writing", "data", "to", "file", "return", "output", "group"], "docstring": "output to html file", "docstring_tokens": ["output", "to", "html", "file"], "idx": 352}
{"url": "https://github.com/readbeyond/aeneas/blob/9d95535ad63eef4a98530cfdff033b8c35315ee1/aeneas/syncmap/__init__.py#L309-L368", "repo": "aeneas", "func_name": "output_html_for_tuning", "original_string": ["    def output_html_for_tuning(\n", "            self,\n", "            audio_file_path,\n", "            output_file_path,\n", "            parameters=None\n", "    ):\n", "        \"\"\"\n", "        Output an HTML file for fine tuning the sync map manually.\n", "\n", "        :param string audio_file_path: the path to the associated audio file\n", "        :param string output_file_path: the path to the output file to write\n", "        :param dict parameters: additional parameters\n", "\n", "        .. versionadded:: 1.3.1\n", "        \"\"\"\n", "        if not gf.file_can_be_written(output_file_path):\n", "            self.log_exc(u\"Cannot output HTML file '%s'. Wrong permissions?\" % (output_file_path), None, True, OSError)\n", "        if parameters is None:\n", "            parameters = {}\n", "        audio_file_path_absolute = gf.fix_slash(os.path.abspath(audio_file_path))\n", "        template_path_absolute = gf.absolute_path(self.FINETUNEAS_PATH, __file__)\n", "        with io.open(template_path_absolute, \"r\", encoding=\"utf-8\") as file_obj:\n", "            template = file_obj.read()\n", "        for repl in self.FINETUNEAS_REPLACEMENTS:\n", "            template = template.replace(repl[0], repl[1])\n", "        template = template.replace(\n", "            self.FINETUNEAS_REPLACE_AUDIOFILEPATH,\n", "            u\"audioFilePath = \\\"file://%s\\\";\" % audio_file_path_absolute\n", "        )\n", "        template = template.replace(\n", "            self.FINETUNEAS_REPLACE_FRAGMENTS,\n", "            u\"fragments = (%s).fragments;\" % self.json_string\n", "        )\n", "        if gc.PPN_TASK_OS_FILE_FORMAT in parameters:\n", "            output_format = parameters[gc.PPN_TASK_OS_FILE_FORMAT]\n", "            if output_format in self.FINETUNEAS_ALLOWED_FORMATS:\n", "                template = template.replace(\n", "                    self.FINETUNEAS_REPLACE_OUTPUT_FORMAT,\n", "                    u\"outputFormat = \\\"%s\\\";\" % output_format\n", "                )\n", "                if output_format == \"smil\":\n", "                    for key, placeholder, replacement in [\n", "                            (\n", "                                gc.PPN_TASK_OS_FILE_SMIL_AUDIO_REF,\n", "                                self.FINETUNEAS_REPLACE_SMIL_AUDIOREF,\n", "                                \"audioref = \\\"%s\\\";\"\n", "                            ),\n", "                            (\n", "                                gc.PPN_TASK_OS_FILE_SMIL_PAGE_REF,\n", "                                self.FINETUNEAS_REPLACE_SMIL_PAGEREF,\n", "                                \"pageref = \\\"%s\\\";\"\n", "                            ),\n", "                    ]:\n", "                        if key in parameters:\n", "                            template = template.replace(\n", "                                placeholder,\n", "                                replacement % parameters[key]\n", "                            )\n", "        with io.open(output_file_path, \"w\", encoding=\"utf-8\") as file_obj:\n", "            file_obj.write(template)\n"], "language": "python", "code": "def output_html_for_tuning(self, audio_file_path, output_file_path,\n    parameters=None):\n    \"\"\"\"\"\"\n    if not gf.file_can_be_written(output_file_path):\n        self.log_exc(u\"Cannot output HTML file '%s'. Wrong permissions?\" %\n            output_file_path, None, True, OSError)\n    if parameters is None:\n        parameters = {}\n    audio_file_path_absolute = gf.fix_slash(os.path.abspath(audio_file_path))\n    template_path_absolute = gf.absolute_path(self.FINETUNEAS_PATH, __file__)\n    with io.open(template_path_absolute, 'r', encoding='utf-8') as file_obj:\n        template = file_obj.read()\n    for repl in self.FINETUNEAS_REPLACEMENTS:\n        template = template.replace(repl[0], repl[1])\n    template = template.replace(self.FINETUNEAS_REPLACE_AUDIOFILEPATH, \n        u'audioFilePath = \"file://%s\";' % audio_file_path_absolute)\n    template = template.replace(self.FINETUNEAS_REPLACE_FRAGMENTS, \n        u'fragments = (%s).fragments;' % self.json_string)\n    if gc.PPN_TASK_OS_FILE_FORMAT in parameters:\n        output_format = parameters[gc.PPN_TASK_OS_FILE_FORMAT]\n        if output_format in self.FINETUNEAS_ALLOWED_FORMATS:\n            template = template.replace(self.\n                FINETUNEAS_REPLACE_OUTPUT_FORMAT, u'outputFormat = \"%s\";' %\n                output_format)\n            if output_format == 'smil':\n                for key, placeholder, replacement in [(gc.\n                    PPN_TASK_OS_FILE_SMIL_AUDIO_REF, self.\n                    FINETUNEAS_REPLACE_SMIL_AUDIOREF, 'audioref = \"%s\";'),\n                    (gc.PPN_TASK_OS_FILE_SMIL_PAGE_REF, self.\n                    FINETUNEAS_REPLACE_SMIL_PAGEREF, 'pageref = \"%s\";')]:\n                    if key in parameters:\n                        template = template.replace(placeholder, \n                            replacement % parameters[key])\n    with io.open(output_file_path, 'w', encoding='utf-8') as file_obj:\n        file_obj.write(template)\n", "code_tokens": ["output", "html", "for", "tuning", "self", "audio", "file", "path", "output", "file", "path", "parameters", "none", "if", "not", "gf", "file", "can", "be", "written", "output", "file", "path", "self", "log", "exc", "cannot", "output", "html", "file", "wrong", "permissions", "output", "file", "path", "none", "true", "oserror", "if", "parameters", "is", "none", "parameters", "audio", "file", "path", "absolute", "gf", "fix", "slash", "os", "path", "abspath", "audio", "file", "path", "template", "path", "absolute", "gf", "absolute", "path", "self", "finetuneas", "path", "file", "with", "io", "open", "template", "path", "absolute", "encoding", "utf", "8", "as", "file", "obj", "template", "file", "obj", "read", "for", "repl", "in", "self", "finetuneas", "replacements", "template", "template", "replace", "repl", "0", "repl", "1", "template", "template", "replace", "self", "finetuneas", "replace", "audiofilepath", "audiofilepath", "file", "audio", "file", "path", "absolute", "template", "template", "replace", "self", "finetuneas", "replace", "fragments", "fragments", "fragments", "self", "json", "string", "if", "gc", "ppn", "task", "os", "file", "format", "in", "parameters", "output", "format", "parameters", "gc", "ppn", "task", "os", "file", "format", "if", "output", "format", "in", "self", "finetuneas", "allowed", "formats", "template", "template", "replace", "self", "finetuneas", "replace", "output", "format", "outputformat", "output", "format", "if", "output", "format", "smil", "for", "key", "placeholder", "replacement", "in", "gc", "ppn", "task", "os", "file", "smil", "audio", "ref", "self", "finetuneas", "replace", "smil", "audioref", "audioref", "gc", "ppn", "task", "os", "file", "smil", "page", "ref", "self", "finetuneas", "replace", "smil", "pageref", "pageref", "if", "key", "in", "parameters", "template", "template", "replace", "placeholder", "replacement", "parameters", "key", "with", "io", "open", "output", "file", "path", "encoding", "utf", "8", "as", "file", "obj", "file", "obj", "write", "template"], "docstring": "output to html file", "docstring_tokens": ["output", "to", "html", "file"], "idx": 353}
{"url": "https://github.com/nigma/django-easy-pdf/blob/327605b91a445b453d8969b341ef74b12ab00a83/easy_pdf/rendering.py#L51-L78", "repo": "django-easy-pdf", "func_name": "html_to_pdf", "original_string": ["def html_to_pdf(content, encoding=\"utf-8\",\n", "                link_callback=fetch_resources, **kwargs):\n", "    \"\"\"\n", "    Converts html ``content`` into PDF document.\n", "\n", "    :param unicode content: html content\n", "    :returns: PDF content\n", "    :rtype: :class:`bytes`\n", "    :raises: :exc:`~easy_pdf.exceptions.PDFRenderingError`\n", "    \"\"\"\n", "    src = BytesIO(content.encode(encoding))\n", "    dest = BytesIO()\n", "\n", "    pdf = pisa.pisaDocument(src, dest, encoding=encoding,\n", "                            link_callback=link_callback, **kwargs)\n", "    if pdf.err:\n", "        logger.error(\"Error rendering PDF document\")\n", "        for entry in pdf.log:\n", "            if entry[0] == xhtml2pdf.default.PML_ERROR:\n", "                logger_x2p.error(\"line %s, msg: %s, fragment: %s\", entry[1], entry[2], entry[3])\n", "        raise PDFRenderingError(\"Errors rendering PDF\", content=content, log=pdf.log)\n", "\n", "    if pdf.warn:\n", "        for entry in pdf.log:\n", "            if entry[0] == xhtml2pdf.default.PML_WARNING:\n", "                logger_x2p.warning(\"line %s, msg: %s, fragment: %s\", entry[1], entry[2], entry[3])\n", "\n", "    return dest.getvalue()\n"], "language": "python", "code": "def html_to_pdf(content, encoding='utf-8', link_callback=fetch_resources,\n    **kwargs):\n    \"\"\"\"\"\"\n    src = BytesIO(content.encode(encoding))\n    dest = BytesIO()\n    pdf = pisa.pisaDocument(src, dest, encoding=encoding, link_callback=\n        link_callback, **kwargs)\n    if pdf.err:\n        logger.error('Error rendering PDF document')\n        for entry in pdf.log:\n            if entry[0] == xhtml2pdf.default.PML_ERROR:\n                logger_x2p.error('line %s, msg: %s, fragment: %s', entry[1],\n                    entry[2], entry[3])\n        raise PDFRenderingError('Errors rendering PDF', content=content,\n            log=pdf.log)\n    if pdf.warn:\n        for entry in pdf.log:\n            if entry[0] == xhtml2pdf.default.PML_WARNING:\n                logger_x2p.warning('line %s, msg: %s, fragment: %s', entry[\n                    1], entry[2], entry[3])\n    return dest.getvalue()\n", "code_tokens": ["html", "to", "pdf", "content", "encoding", "utf", "8", "link", "callback", "fetch", "resources", "kwargs", "src", "bytesio", "content", "encode", "encoding", "dest", "bytesio", "pdf", "pisa", "pisadocument", "src", "dest", "encoding", "encoding", "link", "callback", "link", "callback", "kwargs", "if", "pdf", "err", "logger", "error", "error", "rendering", "pdf", "document", "for", "entry", "in", "pdf", "log", "if", "entry", "0", "default", "pml", "error", "logger", "error", "line", "msg", "fragment", "entry", "1", "entry", "2", "entry", "3", "raise", "pdfrenderingerror", "errors", "rendering", "pdf", "content", "content", "log", "pdf", "log", "if", "pdf", "warn", "for", "entry", "in", "pdf", "log", "if", "entry", "0", "default", "pml", "warning", "logger", "warning", "line", "msg", "fragment", "entry", "1", "entry", "2", "entry", "3", "return", "dest", "getvalue"], "docstring": "convert html to pdf", "docstring_tokens": ["convert", "html", "to", "pdf"], "idx": 354}
{"url": "https://github.com/alexhayes/django-pdfkit/blob/02774ae2cb67d05dd5e4cb50661c56464ebb2413/django_pdfkit/views.py#L55-L74", "repo": "django-pdfkit", "func_name": "render_pdf", "original_string": ["    def render_pdf(self, *args, **kwargs):\n", "        \"\"\"\n", "        Render the PDF and returns as bytes.\n", "\n", "        :rtype: bytes\n", "        \"\"\"\n", "        html = self.render_html(*args, **kwargs)\n", "\n", "        options = self.get_pdfkit_options()\n", "        if 'debug' in self.request.GET and settings.DEBUG:\n", "            options['debug-javascript'] = 1\n", "\n", "        kwargs = {}\n", "        wkhtmltopdf_bin = os.environ.get('WKHTMLTOPDF_BIN')\n", "        if wkhtmltopdf_bin:\n", "            kwargs['configuration'] = pdfkit.configuration(wkhtmltopdf=wkhtmltopdf_bin)\n", "\n", "        pdf = pdfkit.from_string(html, False, options, **kwargs)\n", "\n", "        return pdf\n"], "language": "python", "code": "def render_pdf(self, *args, **kwargs):\n    \"\"\"\"\"\"\n    html = self.render_html(*args, **kwargs)\n    options = self.get_pdfkit_options()\n    if 'debug' in self.request.GET and settings.DEBUG:\n        options['debug-javascript'] = 1\n    kwargs = {}\n    wkhtmltopdf_bin = os.environ.get('WKHTMLTOPDF_BIN')\n    if wkhtmltopdf_bin:\n        kwargs['configuration'] = pdfkit.configuration(wkhtmltopdf=\n            wkhtmltopdf_bin)\n    pdf = pdfkit.from_string(html, False, options, **kwargs)\n    return pdf\n", "code_tokens": ["render", "pdf", "self", "args", "kwargs", "html", "self", "render", "html", "args", "kwargs", "options", "self", "get", "pdfkit", "options", "if", "debug", "in", "self", "request", "get", "and", "settings", "debug", "options", "debug", "javascript", "1", "kwargs", "wkhtmltopdf", "bin", "os", "environ", "get", "wkhtmltopdf", "bin", "if", "wkhtmltopdf", "bin", "kwargs", "configuration", "pdfkit", "configuration", "wkhtmltopdf", "wkhtmltopdf", "bin", "pdf", "pdfkit", "from", "string", "html", "false", "options", "kwargs", "return", "pdf"], "docstring": "convert html to pdf", "docstring_tokens": ["convert", "html", "to", "pdf"], "idx": 355}
